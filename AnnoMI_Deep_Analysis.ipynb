{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnnoMI Deep Analysis - Filtered & Enhanced\n",
    "\n",
    "This notebook performs advanced analysis on the AnnoMI cognitive action predictions, with improvements:\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ Filter out short utterances (< 10 characters) to remove noise (\"hmm\", \"uh-huh\", etc.)\n",
    "- ‚úÖ Enhanced cognitive pattern analysis\n",
    "- ‚úÖ Co-occurrence analysis (which cognitive actions appear together)\n",
    "- ‚úÖ Sequential pattern analysis (what follows what in conversations)\n",
    "- ‚úÖ MI quality correlation (high vs low quality sessions)\n",
    "- ‚úÖ Therapist behavior breakdown by cognitive actions\n",
    "- ‚úÖ Advanced visualizations\n",
    "\n",
    "**Requirements:**\n",
    "- Pre-computed predictions from `all_predictions.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load predictions\n",
    "predictions_path = 'output/analysis_AnnoMI/all_predictions.json'\n",
    "\n",
    "print(\"üì• Loading predictions...\")\n",
    "with open(predictions_path, 'r') as f:\n",
    "    all_predictions = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(all_predictions)} predictions\")\n",
    "\n",
    "# Create DataFrame for easier analysis\n",
    "df = pd.DataFrame(all_predictions)\n",
    "\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Total utterances: {len(df)}\")\n",
    "print(f\"   Transcripts: {df['transcript_id'].nunique()}\")\n",
    "print(f\"   Topics: {df['topic'].nunique()}\")\n",
    "print(f\"   Therapist utterances: {len(df[df['interlocutor'] == 'therapist'])}\")\n",
    "print(f\"   Client utterances: {len(df[df['interlocutor'] == 'client'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-filter-header",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£.1 Filter High-Confidence Cognitive Actions\n\n",
    "Filter predictions to keep only cognitive actions that:\n",
    "- Appear on **more than 2 layers**, OR\n",
    "- Have **100% confidence score**\n\n",
    "This removes noise and focuses on robust cognitive action detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-filter-code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üîç Filtering cognitive actions for high confidence...\")\nprint(f\"   Total predictions before filtering: {len(all_predictions)}\")\n\n# Filter each prediction to keep only high-confidence actions\nfiltered_predictions_list = []\n\nfor pred_data in all_predictions:\n    action_layer_details = pred_data.get('action_layer_details', {})\n    predictions = pred_data['predictions']\n    \n    # Create filtered predictions dict\n    filtered_predictions = {}\n    \n    for action_name, action_info in predictions.items():\n        if not action_info.get('is_active', False):\n            continue\n        \n        # Check filtering criteria\n        num_layers = len(action_layer_details.get(action_name, []))\n        max_confidence = max([layer_info['confidence'] for layer_info in action_layer_details.get(action_name, [])], default=0)\n        \n        # Keep if: appears on >2 layers OR has 100% confidence\n        if num_layers > 2 or max_confidence >= 1.0:\n            filtered_predictions[action_name] = action_info\n    \n    # Create filtered prediction entry\n    filtered_pred_data = pred_data.copy()\n    filtered_pred_data['predictions'] = filtered_predictions\n    filtered_predictions_list.append(filtered_pred_data)\n\n# Count statistics\ntotal_actions_before = sum(len([a for a, d in pred['predictions'].items() if d.get('is_active', False)]) \n                          for pred in all_predictions)\ntotal_actions_after = sum(len([a for a, d in pred['predictions'].items() if d.get('is_active', False)]) \n                         for pred in filtered_predictions_list)\n\nprint(f\"   Total predictions after filtering: {len(filtered_predictions_list)}\")\nprint(f\"   Active actions before: {total_actions_before}\")\nprint(f\"   Active actions after: {total_actions_after}\")\nprint(f\"   Removed: {total_actions_before - total_actions_after} actions ({(total_actions_before - total_actions_after)/total_actions_before*100:.1f}%)\")\n\n# Replace all_predictions with filtered version\nall_predictions = filtered_predictions_list\n\n# Recreate the DataFrame with filtered data\ndf = pd.DataFrame(all_predictions)\n\nprint(f\"‚úÖ Filtering complete! Using {len(all_predictions)} filtered predictions for all analyses.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Data Cleaning - Filter Short Utterances"
   ],
   "id": "cell-6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate utterance lengths\n",
    "df['utterance_length'] = df['utterance_text'].str.len()\n",
    "\n",
    "print(\"üìè Utterance Length Distribution:\")\n",
    "print(f\"   Mean length: {df['utterance_length'].mean():.1f} characters\")\n",
    "print(f\"   Median length: {df['utterance_length'].median():.1f} characters\")\n",
    "print(f\"   Min length: {df['utterance_length'].min()} characters\")\n",
    "print(f\"   Max length: {df['utterance_length'].max()} characters\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nüìä Length distribution:\")\n",
    "print(f\"   < 10 chars: {len(df[df['utterance_length'] < 10])} ({len(df[df['utterance_length'] < 10])/len(df)*100:.1f}%)\")\n",
    "print(f\"   10-50 chars: {len(df[(df['utterance_length'] >= 10) & (df['utterance_length'] < 50)])} ({len(df[(df['utterance_length'] >= 10) & (df['utterance_length'] < 50)])/len(df)*100:.1f}%)\")\n",
    "print(f\"   50-100 chars: {len(df[(df['utterance_length'] >= 50) & (df['utterance_length'] < 100)])} ({len(df[(df['utterance_length'] >= 50) & (df['utterance_length'] < 100)])/len(df)*100:.1f}%)\")\n",
    "print(f\"   100+ chars: {len(df[df['utterance_length'] >= 100])} ({len(df[df['utterance_length'] >= 100])/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Show some examples of short utterances\n",
    "print(\"\\nüîç Examples of short utterances being filtered:\")\n",
    "short_examples = df[df['utterance_length'] < 10]['utterance_text'].head(20)\n",
    "for i, text in enumerate(short_examples, 1):\n",
    "    print(f\"   {i}. \\\"{text}\\\" ({len(text)} chars)\")"
   ],
   "id": "cell-7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out short utterances\n",
    "MIN_LENGTH = 10\n",
    "\n",
    "df_filtered = df[df['utterance_length'] >= MIN_LENGTH].copy()\n",
    "\n",
    "print(f\"üîÑ Filtering utterances < {MIN_LENGTH} characters...\")\n",
    "print(f\"   Original: {len(df)} utterances\")\n",
    "print(f\"   Filtered: {len(df_filtered)} utterances\")\n",
    "print(f\"   Removed: {len(df) - len(df_filtered)} utterances ({(len(df) - len(df_filtered))/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Filtered Dataset:\")\n",
    "print(f\"   Transcripts: {df_filtered['transcript_id'].nunique()}\")\n",
    "print(f\"   Topics: {df_filtered['topic'].nunique()}\")\n",
    "print(f\"   Therapist utterances: {len(df_filtered[df_filtered['interlocutor'] == 'therapist'])}\")\n",
    "print(f\"   Client utterances: {len(df_filtered[df_filtered['interlocutor'] == 'client'])}\")"
   ],
   "id": "cell-8"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Extract Active Cognitive Actions"
   ],
   "id": "cell-9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract active cognitive actions for each utterance\n",
    "def extract_active_actions(predictions_dict):\n",
    "    \"\"\"Extract list of active cognitive actions and their confidences\"\"\"\n",
    "    active = []\n",
    "    for action, data in predictions_dict.items():\n",
    "        if data.get('is_active', False):\n",
    "            active.append({\n",
    "                'action': action,\n",
    "                'confidence': data['aggregate'],\n",
    "                'best_layer': data['best_layer']\n",
    "            })\n",
    "    return active\n",
    "\n",
    "df_filtered['active_actions'] = df_filtered['predictions'].apply(extract_active_actions)\n",
    "df_filtered['num_active_actions'] = df_filtered['active_actions'].apply(len)\n",
    "df_filtered['action_names'] = df_filtered['active_actions'].apply(\n",
    "    lambda x: [a['action'] for a in x]\n",
    ")\n",
    "\n",
    "print(\"üìä Active Actions Statistics:\")\n",
    "print(f\"   Mean actions per utterance: {df_filtered['num_active_actions'].mean():.2f}\")\n",
    "print(f\"   Median actions per utterance: {df_filtered['num_active_actions'].median():.1f}\")\n",
    "print(f\"   Max actions per utterance: {df_filtered['num_active_actions'].max()}\")\n",
    "print(f\"   Utterances with 0 actions: {len(df_filtered[df_filtered['num_active_actions'] == 0])}\")\n",
    "print(f\"   Utterances with 1+ actions: {len(df_filtered[df_filtered['num_active_actions'] > 0])}\")"
   ],
   "id": "cell-10"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Analysis 1: Cognitive Action Frequency (Filtered)"
   ],
   "id": "cell-11"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count cognitive actions by interlocutor\n",
    "therapist_actions = Counter()\n",
    "client_actions = Counter()\n",
    "\n",
    "for _, row in df_filtered.iterrows():\n",
    "    for action_data in row['active_actions']:\n",
    "        action = action_data['action']\n",
    "        confidence = action_data['confidence']\n",
    "        \n",
    "        if row['interlocutor'] == 'therapist':\n",
    "            therapist_actions[action] += confidence\n",
    "        else:\n",
    "            client_actions[action] += confidence\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TOP 20 COGNITIVE ACTIONS (FILTERED DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THERAPIST\")\n",
    "print(\"=\"*80)\n",
    "for action, score in therapist_actions.most_common(20):\n",
    "    bar = \"‚ñà\" * int(score / 10)\n",
    "    print(f\"{action:35s} {score:7.2f} {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLIENT\")\n",
    "print(\"=\"*80)\n",
    "for action, score in client_actions.most_common(20):\n",
    "    bar = \"‚ñà\" * int(score / 10)\n",
    "    print(f\"{action:35s} {score:7.2f} {bar}\")"
   ],
   "id": "cell-12"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Visualization: Filtered Cognitive Actions Comparison"
   ],
   "id": "cell-13"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top actions\n",
    "top_therapist = dict(therapist_actions.most_common(15))\n",
    "top_client = dict(client_actions.most_common(15))\n",
    "\n",
    "# Combine and get unique actions\n",
    "all_actions = sorted(\n",
    "    set(list(top_therapist.keys()) + list(top_client.keys())),\n",
    "    key=lambda x: max(top_therapist.get(x, 0), top_client.get(x, 0)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "therapist_scores = [top_therapist.get(a, 0) for a in all_actions]\n",
    "client_scores = [top_client.get(a, 0) for a in all_actions]\n",
    "\n",
    "# Create figure\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot 1: Comparison bar chart\n",
    "x = np.arange(len(all_actions))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].barh(x - width/2, therapist_scores, width, label='Therapist', color='steelblue', alpha=0.8)\n",
    "axes[0].barh(x + width/2, client_scores, width, label='Client', color='coral', alpha=0.8)\n",
    "axes[0].set_yticks(x)\n",
    "axes[0].set_yticklabels(all_actions, fontsize=10)\n",
    "axes[0].set_xlabel('Aggregate Confidence Score', fontsize=12)\n",
    "axes[0].set_title('Cognitive Action Comparison (Filtered Data):\\nTherapist vs Client', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(fontsize=11)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Plot 2: Ratio comparison\n",
    "ratios = []\n",
    "for action in all_actions:\n",
    "    t_score = top_therapist.get(action, 0.1)\n",
    "    c_score = top_client.get(action, 0.1)\n",
    "    ratio = np.log2(t_score / c_score) if c_score > 0 else 0\n",
    "    ratios.append(ratio)\n",
    "\n",
    "colors = ['steelblue' if r > 0 else 'coral' for r in ratios]\n",
    "axes[1].barh(range(len(all_actions)), ratios, color=colors, alpha=0.8)\n",
    "axes[1].set_yticks(range(len(all_actions)))\n",
    "axes[1].set_yticklabels(all_actions, fontsize=10)\n",
    "axes[1].set_xlabel('Log2(Therapist/Client)', fontsize=12)\n",
    "axes[1].set_title('Cognitive Action Bias (Filtered Data):\\nTherapist-Dominant (blue) vs Client-Dominant (red)', fontsize=14, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/filtered_viz_1_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/filtered_viz_1_comparison.png\")"
   ],
   "id": "cell-14"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Analysis 2: Cognitive Action Co-occurrence\n",
    "\n",
    "Which cognitive actions tend to appear together in the same utterance?"
   ],
   "id": "cell-15"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate co-occurrence matrix\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Get all unique actions\n",
    "all_action_names = sorted(set(\n",
    "    action for actions in df_filtered['action_names'] for action in actions\n",
    "))\n",
    "action_to_idx = {action: i for i, action in enumerate(all_action_names)}\n",
    "n_actions = len(all_action_names)\n",
    "\n",
    "print(f\"üìä Computing co-occurrence matrix for {n_actions} cognitive actions...\")\n",
    "\n",
    "# Build co-occurrence matrix\n",
    "cooccurrence = np.zeros((n_actions, n_actions))\n",
    "\n",
    "for actions in df_filtered['action_names']:\n",
    "    if len(actions) > 1:\n",
    "        for action1, action2 in combinations(actions, 2):\n",
    "            idx1 = action_to_idx[action1]\n",
    "            idx2 = action_to_idx[action2]\n",
    "            cooccurrence[idx1, idx2] += 1\n",
    "            cooccurrence[idx2, idx1] += 1\n",
    "\n",
    "# Find top co-occurrences\n",
    "cooccurrence_pairs = []\n",
    "for i in range(n_actions):\n",
    "    for j in range(i+1, n_actions):\n",
    "        if cooccurrence[i, j] > 0:\n",
    "            cooccurrence_pairs.append((\n",
    "                all_action_names[i],\n",
    "                all_action_names[j],\n",
    "                cooccurrence[i, j]\n",
    "            ))\n",
    "\n",
    "cooccurrence_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 30 COGNITIVE ACTION CO-OCCURRENCES\")\n",
    "print(\"=\"*80)\n",
    "for i, (action1, action2, count) in enumerate(cooccurrence_pairs[:30], 1):\n",
    "    print(f\"{i:2d}. {action1:25s} <-> {action2:25s} ({int(count):4d} times)\")\n",
    "\n",
    "# Save for later use\n",
    "cooccurrence_df = pd.DataFrame(cooccurrence_pairs, columns=['action1', 'action2', 'count'])\n",
    "cooccurrence_df.to_csv('output/analysis_AnnoMI/cooccurrence_matrix.csv', index=False)\n",
    "print(\"\\n‚úÖ Co-occurrence matrix saved to: output/analysis_AnnoMI/cooccurrence_matrix.csv\")"
   ],
   "id": "cell-16"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Visualization: Co-occurrence Network"
   ],
   "id": "cell-17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top co-occurrences as heatmap\n",
    "# Select top 20 most frequent actions\n",
    "combined_actions = therapist_actions + client_actions\n",
    "top_20_actions = [action for action, _ in combined_actions.most_common(20)]\n",
    "\n",
    "# Create submatrix\n",
    "top_20_idx = [action_to_idx[action] for action in top_20_actions]\n",
    "cooccurrence_top20 = cooccurrence[np.ix_(top_20_idx, top_20_idx)]\n",
    "\n",
    "# Plot heatmap\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "im = ax.imshow(cooccurrence_top20, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "# Set ticks\n",
    "ax.set_xticks(range(len(top_20_actions)))\n",
    "ax.set_yticks(range(len(top_20_actions)))\n",
    "ax.set_xticklabels(top_20_actions, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(top_20_actions, fontsize=10)\n",
    "\n",
    "# Add colorbar\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('Co-occurrence Count', fontsize=12)\n",
    "\n",
    "ax.set_title('Cognitive Action Co-occurrence Heatmap\\n(Top 20 Actions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/filtered_viz_2_cooccurrence.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/filtered_viz_2_cooccurrence.png\")"
   ],
   "id": "cell-18"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ Analysis 3: Sequential Patterns\n",
    "\n",
    "What cognitive actions tend to follow each other in conversation?"
   ],
   "id": "cell-19"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze sequential patterns (action A -> action B)\n",
    "print(\"üîÑ Analyzing sequential patterns...\")\n",
    "\n",
    "# Group by transcript\n",
    "transitions = Counter()\n",
    "\n",
    "for transcript_id in df_filtered['transcript_id'].unique():\n",
    "    transcript_df = df_filtered[df_filtered['transcript_id'] == transcript_id].sort_values('utterance_id')\n",
    "    \n",
    "    for i in range(len(transcript_df) - 1):\n",
    "        current_actions = transcript_df.iloc[i]['action_names']\n",
    "        next_actions = transcript_df.iloc[i+1]['action_names']\n",
    "        \n",
    "        # Count transitions\n",
    "        for curr_action in current_actions:\n",
    "            for next_action in next_actions:\n",
    "                transitions[(curr_action, next_action)] += 1\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 30 SEQUENTIAL PATTERNS (Action A -> Action B)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, ((action1, action2), count) in enumerate(transitions.most_common(30), 1):\n",
    "    print(f\"{i:2d}. {action1:25s} -> {action2:25s} ({count:4d} times)\")\n",
    "\n",
    "# Save transitions\n",
    "transitions_df = pd.DataFrame(\n",
    "    [(a1, a2, count) for (a1, a2), count in transitions.items()],\n",
    "    columns=['action_from', 'action_to', 'count']\n",
    ")\n",
    "transitions_df.to_csv('output/analysis_AnnoMI/sequential_transitions.csv', index=False)\n",
    "print(\"\\n‚úÖ Sequential transitions saved to: output/analysis_AnnoMI/sequential_transitions.csv\")"
   ],
   "id": "cell-20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Analysis 4: MI Quality Correlation\n",
    "\n",
    "How do cognitive actions differ between high and low quality MI sessions?"
   ],
   "id": "cell-21"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate by MI quality\n",
    "print(\"üéØ Analyzing cognitive actions by MI quality...\")\n",
    "\n",
    "high_quality = df_filtered[df_filtered['mi_quality'] == 'high']\n",
    "low_quality = df_filtered[df_filtered['mi_quality'] == 'low']\n",
    "\n",
    "print(f\"\\nüìä Dataset split:\")\n",
    "print(f\"   High quality: {len(high_quality)} utterances ({len(high_quality)/len(df_filtered)*100:.1f}%)\")\n",
    "print(f\"   Low quality: {len(low_quality)} utterances ({len(low_quality)/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "# Count actions by quality\n",
    "high_quality_actions = Counter()\n",
    "low_quality_actions = Counter()\n",
    "\n",
    "for _, row in high_quality.iterrows():\n",
    "    for action_data in row['active_actions']:\n",
    "        high_quality_actions[action_data['action']] += action_data['confidence']\n",
    "\n",
    "for _, row in low_quality.iterrows():\n",
    "    for action_data in row['active_actions']:\n",
    "        low_quality_actions[action_data['action']] += action_data['confidence']\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HIGH QUALITY MI SESSIONS - TOP 15 ACTIONS\")\n",
    "print(\"=\"*80)\n",
    "for action, score in high_quality_actions.most_common(15):\n",
    "    bar = \"‚ñà\" * int(score / 10)\n",
    "    print(f\"{action:35s} {score:7.2f} {bar}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOW QUALITY MI SESSIONS - TOP 15 ACTIONS\")\n",
    "print(\"=\"*80)\n",
    "for action, score in low_quality_actions.most_common(15):\n",
    "    bar = \"‚ñà\" * int(score / 10)\n",
    "    print(f\"{action:35s} {score:7.2f} {bar}\")"
   ],
   "id": "cell-22"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Visualization: MI Quality Comparison"
   ],
   "id": "cell-23"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize MI quality differences\n",
    "top_high = dict(high_quality_actions.most_common(15))\n",
    "top_low = dict(low_quality_actions.most_common(15))\n",
    "\n",
    "# Combine and get unique actions\n",
    "all_actions_quality = sorted(\n",
    "    set(list(top_high.keys()) + list(top_low.keys())),\n",
    "    key=lambda x: max(top_high.get(x, 0), top_low.get(x, 0)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "high_scores = [top_high.get(a, 0) for a in all_actions_quality]\n",
    "low_scores = [top_low.get(a, 0) for a in all_actions_quality]\n",
    "\n",
    "# Create figure\n",
    "fig, ax = plt.subplots(figsize=(16, 10))\n",
    "\n",
    "x = np.arange(len(all_actions_quality))\n",
    "width = 0.35\n",
    "\n",
    "ax.barh(x - width/2, high_scores, width, label='High Quality MI', color='green', alpha=0.7)\n",
    "ax.barh(x + width/2, low_scores, width, label='Low Quality MI', color='red', alpha=0.7)\n",
    "ax.set_yticks(x)\n",
    "ax.set_yticklabels(all_actions_quality, fontsize=10)\n",
    "ax.set_xlabel('Aggregate Confidence Score', fontsize=12)\n",
    "ax.set_title('Cognitive Actions by MI Quality\\n(High vs Low Quality Sessions)', fontsize=14, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/filtered_viz_3_mi_quality.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/filtered_viz_3_mi_quality.png\")"
   ],
   "id": "cell-24"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Analysis 5: Therapist Behavior Breakdown\n",
    "\n",
    "Analyze cognitive actions by therapist behavior type"
   ],
   "id": "cell-25"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter therapist utterances only\n",
    "therapist_df = df_filtered[df_filtered['interlocutor'] == 'therapist'].copy()\n",
    "\n",
    "# Get behavior types\n",
    "behavior_types = therapist_df['main_therapist_behaviour'].dropna().unique()\n",
    "\n",
    "print(\"üë®‚Äç‚öïÔ∏è Therapist Behavior Types:\")\n",
    "for behavior in sorted(behavior_types):\n",
    "    count = len(therapist_df[therapist_df['main_therapist_behaviour'] == behavior])\n",
    "    print(f\"   {behavior:30s}: {count:4d} utterances\")\n",
    "\n",
    "# Analyze top behaviors\n",
    "behavior_actions = {}\n",
    "\n",
    "for behavior in behavior_types:\n",
    "    behavior_df = therapist_df[therapist_df['main_therapist_behaviour'] == behavior]\n",
    "    actions = Counter()\n",
    "    \n",
    "    for _, row in behavior_df.iterrows():\n",
    "        for action_data in row['active_actions']:\n",
    "            actions[action_data['action']] += action_data['confidence']\n",
    "    \n",
    "    behavior_actions[behavior] = actions\n",
    "\n",
    "# Display top actions for each behavior type\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP COGNITIVE ACTIONS BY THERAPIST BEHAVIOR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for behavior in sorted(behavior_types):\n",
    "    print(f\"\\n{behavior.upper()}:\")\n",
    "    print(\"-\" * 80)\n",
    "    for action, score in behavior_actions[behavior].most_common(10):\n",
    "        print(f\"   {action:30s} {score:7.2f}\")"
   ],
   "id": "cell-26"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Visualization: Distribution of Active Actions per Utterance"
   ],
   "id": "cell-27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distribution of number of active actions\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Histogram\n",
    "axes[0].hist(df_filtered['num_active_actions'], bins=range(0, df_filtered['num_active_actions'].max()+2), \n",
    "             color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Number of Active Cognitive Actions', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].set_title('Distribution of Active Actions per Utterance', fontsize=14, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# By interlocutor\n",
    "therapist_counts = df_filtered[df_filtered['interlocutor'] == 'therapist']['num_active_actions']\n",
    "client_counts = df_filtered[df_filtered['interlocutor'] == 'client']['num_active_actions']\n",
    "\n",
    "axes[1].hist([therapist_counts, client_counts], bins=range(0, df_filtered['num_active_actions'].max()+2),\n",
    "             label=['Therapist', 'Client'], color=['steelblue', 'coral'], alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Number of Active Cognitive Actions', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].set_title('Active Actions Distribution by Interlocutor', fontsize=14, fontweight='bold')\n",
    "axes[1].legend(fontsize=11)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/filtered_viz_4_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/filtered_viz_4_distribution.png\")"
   ],
   "id": "cell-28"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£3Ô∏è‚É£ Summary Statistics"
   ],
   "id": "cell-29"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE SUMMARY STATISTICS (FILTERED DATA)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Dataset:\")\n",
    "print(f\"   Total utterances (filtered): {len(df_filtered)}\")\n",
    "print(f\"   Removed short utterances: {len(df) - len(df_filtered)} ({(len(df) - len(df_filtered))/len(df)*100:.1f}%)\")\n",
    "print(f\"   Transcripts: {df_filtered['transcript_id'].nunique()}\")\n",
    "print(f\"   Topics: {df_filtered['topic'].nunique()}\")\n",
    "print(f\"   Mean utterance length: {df_filtered['utterance_length'].mean():.1f} characters\")\n",
    "\n",
    "print(f\"\\nüß† Cognitive Actions:\")\n",
    "print(f\"   Unique cognitive actions detected: {len(all_action_names)}\")\n",
    "print(f\"   Mean actions per utterance: {df_filtered['num_active_actions'].mean():.2f}\")\n",
    "print(f\"   Median actions per utterance: {df_filtered['num_active_actions'].median():.1f}\")\n",
    "print(f\"   Utterances with actions: {len(df_filtered[df_filtered['num_active_actions'] > 0])} ({len(df_filtered[df_filtered['num_active_actions'] > 0])/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüë• By Interlocutor:\")\n",
    "print(f\"   Therapist utterances: {len(df_filtered[df_filtered['interlocutor'] == 'therapist'])}\")\n",
    "print(f\"   Client utterances: {len(df_filtered[df_filtered['interlocutor'] == 'client'])}\")\n",
    "print(f\"   Therapist avg actions/utterance: {df_filtered[df_filtered['interlocutor'] == 'therapist']['num_active_actions'].mean():.2f}\")\n",
    "print(f\"   Client avg actions/utterance: {df_filtered[df_filtered['interlocutor'] == 'client']['num_active_actions'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nüéØ MI Quality:\")\n",
    "print(f\"   High quality: {len(df_filtered[df_filtered['mi_quality'] == 'high'])} ({len(df_filtered[df_filtered['mi_quality'] == 'high'])/len(df_filtered)*100:.1f}%)\")\n",
    "print(f\"   Low quality: {len(df_filtered[df_filtered['mi_quality'] == 'low'])} ({len(df_filtered[df_filtered['mi_quality'] == 'low'])/len(df_filtered)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìà Top Findings:\")\n",
    "print(f\"   Most common therapist action: {therapist_actions.most_common(1)[0][0]}\")\n",
    "print(f\"   Most common client action: {client_actions.most_common(1)[0][0]}\")\n",
    "print(f\"   Most common co-occurrence: {cooccurrence_pairs[0][0]} + {cooccurrence_pairs[0][1]}\")\n",
    "print(f\"   Most common transition: {transitions.most_common(1)[0][0][0]} -> {transitions.most_common(1)[0][0][1]}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ Analysis complete!\")\n",
    "print(\"=\"*80)"
   ],
   "id": "cell-30"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£4Ô∏è‚É£ Export Summary Report"
   ],
   "id": "cell-31"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary report\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'total_utterances_original': len(df),\n",
    "        'total_utterances_filtered': len(df_filtered),\n",
    "        'removed_utterances': len(df) - len(df_filtered),\n",
    "        'min_length_threshold': MIN_LENGTH,\n",
    "        'transcripts': int(df_filtered['transcript_id'].nunique()),\n",
    "        'topics': int(df_filtered['topic'].nunique()),\n",
    "        'mean_utterance_length': float(df_filtered['utterance_length'].mean()),\n",
    "    },\n",
    "    'cognitive_actions': {\n",
    "        'unique_actions': len(all_action_names),\n",
    "        'mean_per_utterance': float(df_filtered['num_active_actions'].mean()),\n",
    "        'median_per_utterance': float(df_filtered['num_active_actions'].median()),\n",
    "    },\n",
    "    'top_therapist_actions': dict(therapist_actions.most_common(10)),\n",
    "    'top_client_actions': dict(client_actions.most_common(10)),\n",
    "    'top_cooccurrences': [(a1, a2, int(c)) for a1, a2, c in cooccurrence_pairs[:10]],\n",
    "    'top_transitions': [(a1, a2, int(c)) for (a1, a2), c in transitions.most_common(10)],\n",
    "    'high_quality_top_actions': dict(high_quality_actions.most_common(10)),\n",
    "    'low_quality_top_actions': dict(low_quality_actions.most_common(10)),\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open('output/analysis_AnnoMI/filtered_summary_report.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"‚úÖ Summary report saved to: output/analysis_AnnoMI/filtered_summary_report.json\")"
   ],
   "id": "cell-32"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}