{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": "# Sentiment Probe Full Pipeline - Google Colab\n## üÜï Regression-Based Continuous Sentiment Scoring\n\nComplete end-to-end pipeline for **regression-based sentiment probes** on Gemma 3 4B.\n\n### What makes this different?\n**Traditional approach**: Binary classification (0 or 1) with sigmoid probabilities  \n**Our approach**: **Linear regression** producing continuous sentiment scores (-‚àû to +‚àû)\n\n### Why Regression?\n‚úÖ **Smoother predictions** - No sigmoid compression  \n‚úÖ **Natural intensity** - Magnitude reflects sentiment strength  \n‚úÖ **Better granularity** - Detects subtle shifts in sentiment  \n‚úÖ **Unbounded scores** - Can capture extreme emotions  \n\n### Pipeline Steps:\n1. ‚úÖ Setup and clone repository\n2. üìù Generate sentiment data (700 positive + 700 negative)\n3. üöÄ Capture activations from ALL layers (1-34) in batches\n4. üéØ **Train regression-based sentiment probes** (MSE loss, continuous outputs)\n5. üìä Visualize regression performance across layers\n6. üíæ Download trained models\n\n### Features:\n- **üöÄ Optimized for Big GPUs**: Parallel batch processing (40GB VRAM)\n- **‚ö° Fast Generation**: 8 parallel workers with batch sizes of 50\n- **OOM Prevention**: Process layers in batches of 10 to avoid memory issues\n- **Progress Tracking**: Clear ETA and progress bars\n- **Auto-backup**: Save to Google Drive after each step\n- **Resume Capability**: Can resume from any batch if interrupted\n- **Continuous Scoring**: Get sentiment intensity scores, not just classifications\n\n### Performance Improvements:\n- **Old approach**: Sequential generation (~4-6 hours total)\n- **New approach**: Parallel batch generation (~1-2 hours total)\n- **Speedup**: ~3-4x faster with 8 parallel workers\n\n### Requirements:\n- Google Colab with GPU (T4 or better, A100/V100 recommended for 40GB)\n- Runtime: ~1-2 hours total (with parallel generation)\n- Hugging Face token for Gemma access\n\n### Example Outputs:\n```\nText: \"I'm absolutely thrilled about this opportunity!\"\nScore: +2.8  (Strong positive)\n\nText: \"Feeling a bit uncertain about the decision.\"\nScore: -0.4  (Slightly negative)\n\nText: \"This is the worst experience I've ever had.\"\nScore: -3.2  (Very strong negative)\n```"
   "source": "# Sentiment Probe Full Pipeline - Google Colab\n## üÜï Regression-Based Continuous Sentiment Scoring\n## ‚ö° Powered by vLLM for Maximum Throughput\n\nComplete end-to-end pipeline for **regression-based sentiment probes** on Gemma 3 4B.\n\n### What makes this different?\n**Traditional approach**: Binary classification (0 or 1) with sigmoid probabilities  \n**Our approach**: **Linear regression** producing continuous sentiment scores (-‚àû to +‚àû)\n\n### Why Regression?\n‚úÖ **Smoother predictions** - No sigmoid compression  \n‚úÖ **Natural intensity** - Magnitude reflects sentiment strength  \n‚úÖ **Better granularity** - Detects subtle shifts in sentiment  \n‚úÖ **Unbounded scores** - Can capture extreme emotions  \n\n### Why vLLM?\n‚ö° **Massive throughput** - Process 256 sequences in parallel  \n‚ö° **Efficient batching** - Automatic continuous batching optimization  \n‚ö° **Full GPU utilization** - Uses 85% of A100 40GB VRAM (~34GB)  \n‚ö° **Smart memory management** - PagedAttention for KV-cache  \n‚ö° **10-20x faster** - Compared to standard HuggingFace generation  \n\n### Pipeline Steps:\n1. ‚úÖ Setup and clone repository\n2. üìù **Generate sentiment data with vLLM batch inference** (700 positive + 700 negative)\n3. üöÄ Capture activations from ALL layers (1-34) in batches\n4. üéØ **Train regression-based sentiment probes** (MSE loss, continuous outputs)\n5. üìä Visualize regression performance across layers\n6. üíæ Download trained models\n\n### Features:\n- **vLLM Optimization**: Process up to 256 prompts in parallel batches\n- **GPU Maximization**: Configured for A100 40GB with 85% memory utilization\n- **OOM Prevention**: Process layers in batches of 10 to avoid memory issues\n- **Progress Tracking**: Clear ETA and progress bars\n- **Auto-backup**: Save to Google Drive after each step\n- **Resume Capability**: Can resume from any batch if interrupted\n- **Continuous Scoring**: Get sentiment intensity scores, not just classifications\n\n### Requirements:\n- Google Colab with **A100 40GB GPU** (for optimal vLLM performance)\n- Runtime: ~2-3 hours total (vLLM is much faster!)\n- Hugging Face token for Gemma access\n\n### Expected Performance (A100 40GB):\n- **Data Generation**: ~5-10 minutes for 1,400 examples (with vLLM batch inference)\n- **Activation Capture**: ~2-3 hours for all 34 layers\n- **Probe Training**: ~30-45 minutes for all layers\n- **vLLM Throughput**: 50-100+ examples/second (vs 2-5 with standard generation)\n\n### Example Outputs:\n```\nText: \"I'm absolutely thrilled about this opportunity!\"\nScore: +2.8  (Strong positive)\n\nText: \"Feeling a bit uncertain about the decision.\"\nScore: -0.4  (Slightly negative)\n\nText: \"This is the worst experience I've ever had.\"\nScore: -3.2  (Very strong negative)\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-gpu"
   },
   "source": [
    "## 1Ô∏è‚É£ Check GPU and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-gpu"
   },
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No GPU detected!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## 2Ô∏è‚É£ Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone repository\n",
    "repo_url = \"https://github.com/ChuloIva/brije.git\"\n",
    "repo_name = \"brije\"\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    print(\"üì• Cloning repository...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository exists\")\n",
    "    !cd {repo_name} && git pull\n",
    "\n",
    "os.chdir(repo_name)\n",
    "print(f\"\\nüìÅ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": "# Install dependencies\nprint(\"üì¶ Installing dependencies...\\n\")\n!pip install -q torch transformers h5py scikit-learn tqdm matplotlib seaborn pandas nest-asyncio\n\n# Install nnsight\nnnsight_dir = \"third_party/nnsight\"\nnnsight_repo = \"https://github.com/ndif-team/nnsight\"\n\nprint(\"\\nüì¶ Setting up nnsight...\")\nif not os.path.exists(nnsight_dir) or not os.listdir(nnsight_dir):\n    os.makedirs(\"third_party\", exist_ok=True)\n    !git clone {nnsight_repo} {nnsight_dir}\n    print(\"   ‚úÖ nnsight cloned\")\nelse:\n    print(\"   ‚úÖ nnsight exists\")\n\n!pip install -q -e {nnsight_dir}\nprint(\"\\n‚úÖ All dependencies installed!\")"
   "source": "# Install dependencies\nprint(\"üì¶ Installing dependencies...\\n\")\n!pip install -q torch transformers h5py scikit-learn tqdm matplotlib seaborn pandas\n\n# Install vLLM for efficient batch generation\nprint(\"\\nüì¶ Installing vLLM for optimized batch inference...\")\n!pip install -q vllm\n\n# Install nnsight\nnnsight_dir = \"third_party/nnsight\"\nnnsight_repo = \"https://github.com/ndif-team/nnsight\"\n\nprint(\"\\nüì¶ Setting up nnsight...\")\nif not os.path.exists(nnsight_dir) or not os.listdir(nnsight_dir):\n    os.makedirs(\"third_party\", exist_ok=True)\n    !git clone {nnsight_repo} {nnsight_dir}\n    print(\"   ‚úÖ nnsight cloned\")\nelse:\n    print(\"   ‚úÖ nnsight exists\")\n\n!pip install -q -e {nnsight_dir}\nprint(\"\\n‚úÖ All dependencies installed!\")\nprint(\"üí° vLLM will enable highly optimized batch generation for A100 GPUs\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount-drive"
   },
   "source": [
    "## 3Ô∏è‚É£ Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create output directories\n",
    "drive_output_dir = '/content/drive/MyDrive/brije_sentiment_outputs'\n",
    "os.makedirs(drive_output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/data\", exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/activations\", exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/probes\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted\")\n",
    "print(f\"   Outputs will be saved to: {drive_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hf-login"
   },
   "source": [
    "## 4Ô∏è‚É£ Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hf"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "datagen-header"
   },
   "source": "## 5Ô∏è‚É£ Generate Sentiment Data (700 Positive + 700 Negative)\n\n### üöÄ Optimized for High-Performance GPUs\n\nThis notebook uses **parallel batch processing** to maximize GPU utilization:\n\n**How it works:**\n1. **Batch preparation**: Prepare all example configurations upfront\n2. **Parallel execution**: Generate multiple examples simultaneously using ThreadPoolExecutor\n3. **Batch processing**: Process examples in configurable batches (default: 50)\n4. **Memory management**: Periodic GPU cleanup to prevent OOM\n\n**Performance comparison:**\n- **Sequential (old)**: ~4-6 hours for 1400 examples\n- **Parallel (new)**: ~1-2 hours for 1400 examples with 8 workers\n- **Speedup**: ~3-4x faster\n\n**Configuration options** (adjust `GPU_CONFIG` above):\n- **T4 (16GB)**: `max_parallel=4, batch_size=25` (~2-3 hours)\n- **A100 (40GB)**: `max_parallel=8, batch_size=50` (~1-2 hours)\n- **A100 (80GB)**: `max_parallel=16, batch_size=100` (~30-60 min)"
   "source": "## 5Ô∏è‚É£ Generate Sentiment Data (700 Positive + 700 Negative)\n\n**Using vLLM for highly optimized batch generation** - processes all 1,400 examples efficiently in large batches.\n\n### vLLM Benefits:\n- **Parallel Processing**: Handles up to 256 sequences simultaneously\n- **Automatic Batching**: Internal continuous batching for maximum GPU utilization\n- **Memory Efficiency**: PagedAttention algorithm for efficient KV-cache management\n- **10-20x Speedup**: Compared to sequential generation with standard transformers\n\nExpected generation time: **5-10 minutes** for all 1,400 examples on A100 40GB."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datagen-imports"
   },
   "outputs": [],
   "source": "import json\nimport random\nimport asyncio\nfrom dataclasses import dataclass, asdict\nfrom tqdm import tqdm\n\n@dataclass\nclass SentimentExample:\n    text: str\n    sentiment: str  # \"positive\" or \"negative\"\n    emotion: str\n\n# Sentiment definitions\nSENTIMENTS = {\n    \"positive\": {\n        \"emotions\": [\n            \"joy\", \"gratitude\", \"hope\", \"excitement\", \"love\", \n            \"pride\", \"contentment\", \"inspiration\", \"relief\", \"satisfaction\"\n        ]\n    },\n    \"negative\": {\n        \"emotions\": [\n            \"sadness\", \"anger\", \"fear\", \"disgust\", \"shame\",\n            \"anxiety\", \"frustration\", \"disappointment\", \"guilt\", \"loneliness\"\n        ]\n    }\n}\n\nCONTEXTS = [\n    \"relationships\", \"work\", \"family\", \"friends\", \"health\",\n    \"achievements\", \"hobbies\", \"learning\", \"challenges\", \"life changes\"\n]\n\n# ‚öôÔ∏è GPU-SPECIFIC CONFIGURATION\n# Adjust these based on your available VRAM:\n# - T4 (16GB):  max_parallel=4, batch_size=25\n# - A100 (40GB): max_parallel=8, batch_size=50\n# - A100 (80GB): max_parallel=12, batch_size=75\n\nGPU_CONFIG = {\n    'max_parallel': 8,   # Number of parallel generations (controls semaphore)\n    'batch_size': 25,    # Examples per batch (smaller batches for better progress tracking)\n}\n\nprint(\"‚úÖ Sentiment definitions loaded\")\nprint(f\"\\n‚öôÔ∏è  GPU Configuration:\")\nprint(f\"   Max parallel: {GPU_CONFIG['max_parallel']}\")\nprint(f\"   Batch size: {GPU_CONFIG['batch_size']}\")\nprint(f\"\\nüí° Adjust GPU_CONFIG above for your specific GPU\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datagen-model"
   },
   "outputs": [],
   "source": "# Load Gemma for data generation\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nimport time\n\nprint(\"Loading Gemma for data generation...\")\nmodel_name = \"google/gemma-3-4b-it\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\ngen_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\",\n    torch_dtype=\"auto\"\n)\nprint(\"‚úÖ Model loaded\")\n\n# Create a semaphore to control parallel access to the model\ngeneration_semaphore = asyncio.Semaphore(GPU_CONFIG['max_parallel'])\n\ndef generate_sentiment_example_sync(sentiment: str, emotion: str, context: str) -> str:\n    \"\"\"Generate one sentiment example using Gemma (synchronous)\"\"\"\n    prompt = f\"\"\"Generate a brief first-person example expressing {sentiment} sentiment, specifically {emotion}, in the context of {context}.\n\nRequirements:\n- 2-3 sentences\n- First person (I, my, me)\n- Show genuine {emotion}, don't just state it\n- Natural and authentic\n\nExample only:\"\"\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(gen_model.device)\n    outputs = gen_model.generate(\n        **inputs,\n        max_new_tokens=100,\n        temperature=0.9,\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id\n    )\n    \n    text = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n    return text.strip()\n\nasync def generate_one_async(sentiment: str, emotion: str, context: str) -> SentimentExample:\n    \"\"\"Generate one example asynchronously with semaphore control\"\"\"\n    async with generation_semaphore:\n        # Run the synchronous generation in the default executor\n        loop = asyncio.get_event_loop()\n        try:\n            text = await loop.run_in_executor(None, generate_sentiment_example_sync, sentiment, emotion, context)\n            \n            if text and len(text) > 20:  # Valid example\n                return SentimentExample(\n                    text=text,\n                    sentiment=sentiment,\n                    emotion=emotion\n                )\n            return None\n        except Exception as e:\n            print(f\"Error generating example: {e}\")\n            return None\n\nasync def generate_batch_async(batch_configs: list, pbar=None) -> list:\n    \"\"\"Generate a batch of examples asynchronously\"\"\"\n    tasks = [\n        generate_one_async(cfg['sentiment'], cfg['emotion'], cfg['context'])\n        for cfg in batch_configs\n    ]\n    \n    results = await asyncio.gather(*tasks)\n    valid_results = [r for r in results if r is not None]\n    \n    # Update progress bar\n    if pbar:\n        pbar.update(len(batch_configs))\n    \n    return valid_results\n\nprint(\"‚úÖ Generation functions ready\")\nprint(f\"   Parallel workers: {GPU_CONFIG['max_parallel']}\")\nprint(f\"   Batch size: {GPU_CONFIG['batch_size']}\")\nprint(f\"\\nüí° With {GPU_CONFIG['max_parallel']} parallel workers, generation should be ~3-4x faster\")"
   "source": "# Initialize vLLM for efficient batch generation\nfrom vllm import LLM, SamplingParams\n\nprint(\"=\" * 70)\nprint(\"INITIALIZING vLLM FOR OPTIMIZED BATCH GENERATION\")\nprint(\"=\" * 70)\n\nmodel_name = \"google/gemma-3-4b-it\"\n\n# vLLM configuration optimized for A100 40GB\n# Based on benchmarks: A100 40GB can handle Gemma-4B with high throughput\nvllm_config = {\n    \"model\": model_name,\n    \"tensor_parallel_size\": 1,  # Single GPU\n    \"gpu_memory_utilization\": 0.85,  # Use 85% of 40GB VRAM (~34GB)\n    \"max_model_len\": 2048,  # Context length for generation\n    \"max_num_batched_tokens\": 4096,  # Higher for throughput (A100 can handle this)\n    \"max_num_seqs\": 256,  # Process up to 256 sequences in parallel\n    \"trust_remote_code\": True,\n    \"dtype\": \"auto\",  # Will use bfloat16 on A100\n}\n\nprint(\"\\nvLLM Configuration:\")\nprint(f\"  GPU Memory Utilization: {vllm_config['gpu_memory_utilization']*100}%\")\nprint(f\"  Max Batched Tokens: {vllm_config['max_num_batched_tokens']}\")\nprint(f\"  Max Parallel Sequences: {vllm_config['max_num_seqs']}\")\nprint(f\"  Max Model Length: {vllm_config['max_model_len']}\")\nprint()\n\n# Initialize vLLM engine\nprint(\"Loading model with vLLM engine...\")\nllm = LLM(**vllm_config)\nprint(\"‚úÖ vLLM engine initialized\")\n\n# Configure sampling parameters for diverse, natural text\nsampling_params = SamplingParams(\n    temperature=0.9,\n    top_p=0.95,\n    max_tokens=150,  # 2-3 sentences\n    repetition_penalty=1.1,\n)\n\nprint(\"\\nSampling Parameters:\")\nprint(f\"  Temperature: {sampling_params.temperature}\")\nprint(f\"  Top-p: {sampling_params.top_p}\")\nprint(f\"  Max Tokens: {sampling_params.max_tokens}\")\nprint(f\"  Repetition Penalty: {sampling_params.repetition_penalty}\")\n\nprint(\"\\n‚úÖ vLLM ready for batch generation\")\nprint(\"üí° This configuration will fully utilize A100 40GB VRAM for maximum throughput\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datagen-generate"
   },
   "outputs": [],
   "source": "# Generate sentiment data with async parallel processing\nimport nest_asyncio\nnest_asyncio.apply()  # Allow nested event loops in Jupyter/Colab\n\nprint(\"=\"*70)\nprint(\"GENERATING SENTIMENT DATA (ASYNC PARALLEL MODE)\")\nprint(\"=\"*70)\n\nexamples_per_sentiment = 700\nsentiment_data = {\"positive\": [], \"negative\": []}\n\nstart_time = time.time()\n\n# Create one progress bar for all generations\ntotal_examples = examples_per_sentiment * 2\npbar = tqdm(total=total_examples, desc=\"Generating\", unit=\"examples\")\n\nfor sentiment in [\"positive\", \"negative\"]:\n    emotions = SENTIMENTS[sentiment][\"emotions\"]\n    \n    # Update progress bar description\n    pbar.set_description(f\"{sentiment.capitalize()}\")\n    \n    # Prepare all batch configurations upfront\n    batch_configs = []\n    for i in range(examples_per_sentiment):\n        emotion = random.choice(emotions)\n        context = random.choice(CONTEXTS)\n        batch_configs.append({\n            'sentiment': sentiment,\n            'emotion': emotion,\n            'context': context\n        })\n    \n    # Process in smaller batches for better progress tracking\n    total_batches = (examples_per_sentiment + GPU_CONFIG['batch_size'] - 1) // GPU_CONFIG['batch_size']\n    \n    for batch_idx in range(total_batches):\n        start_idx = batch_idx * GPU_CONFIG['batch_size']\n        end_idx = min(start_idx + GPU_CONFIG['batch_size'], examples_per_sentiment)\n        batch = batch_configs[start_idx:end_idx]\n        \n        # Generate batch asynchronously\n        batch_examples = await generate_batch_async(batch, pbar=pbar)\n        sentiment_data[sentiment].extend(batch_examples)\n        \n        # Update progress info\n        elapsed = time.time() - start_time\n        total_generated = len(sentiment_data['positive']) + len(sentiment_data['negative'])\n        rate = total_generated / elapsed if elapsed > 0 else 0\n        remaining = total_examples - total_generated\n        eta = remaining / rate if rate > 0 else 0\n        \n        pbar.set_postfix({\n            'rate': f'{rate:.1f}/s',\n            'ETA': f'{eta/60:.0f}m'\n        })\n        \n        # Cleanup every 5 batches\n        if batch_idx % 5 == 0:\n            torch.cuda.empty_cache()\n\npbar.close()\n\nelapsed_time = time.time() - start_time\n\nprint(f\"\\n‚úÖ Generated {len(sentiment_data['positive'])} positive and {len(sentiment_data['negative'])} negative examples\")\nprint(f\"   Total: {len(sentiment_data['positive']) + len(sentiment_data['negative'])} examples\")\nprint(f\"   Time: {elapsed_time/60:.1f} minutes ({elapsed_time/3600:.2f} hours)\")\nprint(f\"   Rate: {(len(sentiment_data['positive']) + len(sentiment_data['negative']))/elapsed_time:.1f} examples/sec\")\n\n# Clean up generation model\ndel gen_model\ndel tokenizer\ntorch.cuda.empty_cache()\nprint(\"\\n‚úÖ Freed generation model memory\")"
   "source": "# Generate sentiment data using vLLM batch processing\nprint(\"=\"*70)\nprint(\"GENERATING SENTIMENT DATA WITH vLLM BATCH INFERENCE\")\nprint(\"=\"*70)\n\nexamples_per_sentiment = 700\nsentiment_data = {\"positive\": [], \"negative\": []}\n\n# Build all prompts first for maximum batch efficiency\nall_prompts = []\nprompt_metadata = []  # Track which sentiment/emotion/context each prompt belongs to\n\nfor sentiment in [\"positive\", \"negative\"]:\n    print(f\"\\nPreparing {sentiment} prompts...\")\n    emotions = SENTIMENTS[sentiment][\"emotions\"]\n    \n    for i in range(examples_per_sentiment):\n        emotion = random.choice(emotions)\n        context = random.choice(CONTEXTS)\n        \n        prompt = f\"\"\"Generate a brief first-person example expressing {sentiment} sentiment, specifically {emotion}, in the context of {context}.\n\nRequirements:\n- 2-3 sentences\n- First person (I, my, me)\n- Show genuine {emotion}, don't just state it\n- Natural and authentic\n\nExample only:\"\"\"\n        \n        all_prompts.append(prompt)\n        prompt_metadata.append({\n            \"sentiment\": sentiment,\n            \"emotion\": emotion,\n            \"context\": context\n        })\n\nprint(f\"\\n‚úÖ Prepared {len(all_prompts)} prompts\")\nprint(f\"   Positive: {examples_per_sentiment}\")\nprint(f\"   Negative: {examples_per_sentiment}\")\n\n# Generate ALL examples in efficient batches using vLLM\nprint(f\"\\nüöÄ Starting vLLM batch generation...\")\nprint(f\"   Batch size: {vllm_config['max_num_seqs']} parallel sequences\")\nprint(f\"   This will fully utilize A100 40GB VRAM\\n\")\n\nimport time\nstart_time = time.time()\n\n# vLLM automatically handles batching internally for maximum throughput\noutputs = llm.generate(all_prompts, sampling_params)\n\nelapsed = time.time() - start_time\nthroughput = len(all_prompts) / elapsed\n\nprint(f\"\\n‚úÖ Generation complete!\")\nprint(f\"   Total time: {elapsed:.1f}s ({elapsed/60:.2f} minutes)\")\nprint(f\"   Throughput: {throughput:.1f} examples/second\")\nprint(f\"   Average: {elapsed/len(all_prompts):.3f}s per example\")\n\n# Process outputs and organize by sentiment\nprint(f\"\\nüìä Processing generated examples...\")\nvalid_counts = {\"positive\": 0, \"negative\": 0}\n\nfor output, metadata in zip(outputs, prompt_metadata):\n    text = output.outputs[0].text.strip()\n    \n    # Validate example (must have reasonable length)\n    if len(text) > 20 and len(text) < 500:\n        example = SentimentExample(\n            text=text,\n            sentiment=metadata[\"sentiment\"],\n            emotion=metadata[\"emotion\"]\n        )\n        sentiment_data[metadata[\"sentiment\"]].append(example)\n        valid_counts[metadata[\"sentiment\"]] += 1\n\nprint(f\"\\n‚úÖ Generated valid examples:\")\nprint(f\"   Positive: {valid_counts['positive']}/{examples_per_sentiment} ({valid_counts['positive']/examples_per_sentiment*100:.1f}%)\")\nprint(f\"   Negative: {valid_counts['negative']}/{examples_per_sentiment} ({valid_counts['negative']/examples_per_sentiment*100:.1f}%)\")\nprint(f\"   Total: {sum(valid_counts.values())}\")\n\n# Show sample outputs\nprint(f\"\\nüìù Sample Generated Examples:\")\nprint(\"=\"*70)\nfor sentiment in [\"positive\", \"negative\"]:\n    if sentiment_data[sentiment]:\n        sample = sentiment_data[sentiment][0]\n        print(f\"\\n{sentiment.upper()} ({sample.emotion}):\")\n        print(f'  \"{sample.text}\"')\nprint(\"=\"*70)\n\n# Memory cleanup\nprint(f\"\\nüßπ vLLM uses efficient KV-cache, no manual cleanup needed\")\nprint(f\"   GPU memory managed automatically by vLLM engine\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "datagen-save"
   },
   "outputs": [],
   "source": [
    "# Save sentiment data\n",
    "os.makedirs('third_party/datagen/generated_data', exist_ok=True)\n",
    "\n",
    "# Save separate files\n",
    "for sentiment in ['positive', 'negative']:\n",
    "    filename = f'third_party/datagen/generated_data/{sentiment}_sentiment_{len(sentiment_data[sentiment])}.jsonl'\n",
    "    with open(filename, 'w') as f:\n",
    "        for ex in sentiment_data[sentiment]:\n",
    "            f.write(json.dumps(asdict(ex)) + '\\n')\n",
    "    print(f\"‚úÖ Saved {filename}\")\n",
    "\n",
    "# Save combined\n",
    "combined_file = 'third_party/datagen/generated_data/sentiment_combined_1400.jsonl'\n",
    "with open(combined_file, 'w') as f:\n",
    "    for sentiment in ['positive', 'negative']:\n",
    "        for ex in sentiment_data[sentiment]:\n",
    "            f.write(json.dumps(asdict(ex)) + '\\n')\n",
    "\n",
    "print(f\"\\n‚úÖ Saved combined file: {combined_file}\")\n",
    "\n",
    "# Backup to Drive\n",
    "!cp third_party/datagen/generated_data/*.jsonl {drive_output_dir}/data/\n",
    "print(\"‚úÖ Backed up to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Verify dataset format for activation capture\nprint(\"\\n\" + \"=\"*70)\nprint(\"VERIFYING DATASET FORMAT\")\nprint(\"=\"*70)\n\n# Check one example from combined file\nwith open(combined_file, 'r') as f:\n    first_example = json.loads(f.readline())\n    \nprint(\"\\n‚úÖ Sample example from dataset:\")\nprint(f\"  Keys: {list(first_example.keys())}\")\nprint(f\"  Sentiment: {first_example['sentiment']}\")\nprint(f\"  Emotion: {first_example['emotion']}\")\nprint(f\"  Text preview: {first_example['text'][:100]}...\")\n\nprint(\"\\nüí° Dataset format is compatible with activation capture script\")\nprint(\"   The script will auto-detect this as a sentiment dataset\")\nprint(\"=\"*70)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "capture-header"
   },
   "source": "## 6Ô∏è‚É£ Capture Activations from ALL Layers (1-34)\n\nProcess in batches of 10 layers to avoid OOM:\n- Batch 1: Layers 1-10\n- Batch 2: Layers 11-20\n- Batch 3: Layers 21-30\n- Batch 4: Layers 31-34\n\n### ‚≠ê Key Difference from Cognitive Action Probes\n\n**Cognitive Action Probes**:\n- Append: `\"The cognitive action being demonstrated here is\"`\n- Primes model to think about cognitive actions\n\n**Sentiment Probes (this notebook)**:\n- Append: `\"The sentiment of this section is\"`\n- Primes model to think about sentiment/emotion\n- Captures the last token after processing this prompt\n\nThis ensures the model's internal representations are oriented toward sentiment analysis rather than cognitive action classification.\n\n**Note**: You can skip already captured layers (21-30) by modifying the batches list."
   "source": "## 6Ô∏è‚É£ Capture Activations from ALL Layers (1-34)\n\nProcess in batches of 10 layers to avoid OOM:\n- Batch 1: Layers 1-10\n- Batch 2: Layers 11-20\n- Batch 3: Layers 21-30\n- Batch 4: Layers 31-34\n\n**Note**: The capture script automatically detects sentiment datasets and handles them correctly.\n\n**Important**: If you already have captured activations from layers 21-30, you can skip Batch 3 by removing it from the configuration in the next cell."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "capture-config"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Configuration\n",
    "CAPTURE_CONFIG = {\n",
    "    'model': 'google/gemma-3-4b-it',\n",
    "    'dataset': combined_file,\n",
    "    'device': 'auto',\n",
    "    'batch_size': 1000,\n",
    "    \n",
    "    # Define layer batches (10 layers each to avoid OOM)\n",
    "    'layer_batches': [\n",
    "        list(range(1, 11)),   # Batch 1: Layers 1-10\n",
    "        list(range(11, 21)),  # Batch 2: Layers 11-20\n",
    "        list(range(21, 31)),  # Batch 3: Layers 21-30 (already done, can skip)\n",
    "        list(range(31, 35))   # Batch 4: Layers 31-34\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Remove batch 3 if you already have layers 21-30 from cognitive actions\n",
    "# CAPTURE_CONFIG['layer_batches'] = [b for i, b in enumerate(CAPTURE_CONFIG['layer_batches']) if i != 2]\n",
    "\n",
    "print(\"Capture configuration:\")\n",
    "print(f\"  Model: {CAPTURE_CONFIG['model']}\")\n",
    "print(f\"  Dataset: {CAPTURE_CONFIG['dataset']}\")\n",
    "print(f\"  Total batches: {len(CAPTURE_CONFIG['layer_batches'])}\")\n",
    "print(f\"  Total layers: {sum(len(b) for b in CAPTURE_CONFIG['layer_batches'])}\")\n",
    "print(\"\\nBatches:\")\n",
    "for i, batch in enumerate(CAPTURE_CONFIG['layer_batches'], 1):\n",
    "    print(f\"  Batch {i}: Layers {batch[0]}-{batch[-1]} ({len(batch)} layers)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "capture-run"
   },
   "outputs": [],
   "source": "# Run activation capture for each batch\nprint(\"\\n\" + \"=\"*70)\nprint(\"STARTING ACTIVATION CAPTURE\")\nprint(\"=\"*70)\n\ntotal_start = time.time()\n\nfor batch_idx, layer_batch in enumerate(CAPTURE_CONFIG['layer_batches'], 1):\n    batch_start = time.time()\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"BATCH {batch_idx}/{len(CAPTURE_CONFIG['layer_batches'])}: Layers {layer_batch[0]}-{layer_batch[-1]}\")\n    print(f\"{'='*70}\")\n    \n    # Build command - USE SENTIMENT-SPECIFIC CAPTURE SCRIPT\n    cmd = [\n        'python', 'src/probes/capture_activations_sentiment.py',  # ‚≠ê Changed to sentiment script\n        '--dataset', CAPTURE_CONFIG['dataset'],\n        '--output-dir', 'data/activations/sentiment',\n        '--model', CAPTURE_CONFIG['model'],\n        '--layers', *[str(l) for l in layer_batch],\n        '--device', CAPTURE_CONFIG['device'],\n        '--format', 'hdf5',\n        '--single-pass',  # Optimized mode\n        '--batch-size', str(CAPTURE_CONFIG['batch_size'])\n    ]\n    \n    # Run capture\n    !{' '.join(cmd)}\n    \n    batch_elapsed = time.time() - batch_start\n    print(f\"\\n‚úÖ Batch {batch_idx} complete in {batch_elapsed/60:.1f} minutes\")\n    \n    # Backup to Google Drive\n    print(\"\\nüì• Backing up to Google Drive...\")\n    !cp -r data/activations/sentiment/* {drive_output_dir}/activations/\n    print(\"‚úÖ Backup complete\")\n    \n    # Cleanup between batches\n    torch.cuda.empty_cache()\n    print(\"üßπ Cleared GPU memory\\n\")\n\ntotal_elapsed = time.time() - total_start\n\nprint(f\"\\n{'='*70}\")\nprint(\"‚úÖ ALL ACTIVATION CAPTURE COMPLETE\")\nprint(f\"{'='*70}\")\nprint(f\"Total time: {total_elapsed/60:.1f} minutes ({total_elapsed/3600:.2f} hours)\")\nprint(f\"Layers captured: {sum(len(b) for b in CAPTURE_CONFIG['layer_batches'])}\")\nprint(f\"\\nüí° Prompt used: '[text]\\\\n\\\\nThe sentiment of this section is'\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train-header"
   },
   "source": "## 7Ô∏è‚É£ Train Regression-Based Sentiment Probes\n\nTrain **linear regression probes** for each layer to predict continuous sentiment scores.\n\n### Why Regression Instead of Classification?\n\n**Classification (0 or 1)**:\n- Binary output: positive=1, negative=0\n- Sharp boundary, no nuance\n- Probabilities from sigmoid (0.0-1.0)\n\n**Regression (continuous scores)**:\n- Continuous output: -3 to +3 (unbounded)\n- Smooth transitions, captures intensity\n- Natural interpretation: negative scores = negative sentiment, positive scores = positive sentiment\n- Better for detecting subtle sentiment shifts\n\n### Score Interpretation:\n- **Strong negative**: -2.5 to -1.5\n- **Mild negative**: -1.5 to -0.5\n- **Neutral**: -0.5 to +0.5\n- **Mild positive**: +0.5 to +1.5\n- **Strong positive**: +1.5 to +2.5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-config"
   },
   "outputs": [],
   "source": "# Training configuration for REGRESSION probes\nTRAIN_CONFIG = {\n    'batch_size': 32,\n    'epochs': 50,\n    'learning_rate': 0.0005,\n    'weight_decay': 0.001,\n    'early_stopping_patience': 10,\n    'use_scheduler': True,\n    'device': 'auto'\n}\n\nprint(\"Regression Training Configuration:\")\nprint(\"=\"*60)\nfor key, value in TRAIN_CONFIG.items():\n    print(f\"  {key:25s}: {value}\")\nprint(\"=\"*60)\nprint(\"\\nüí° Using MSE loss (Mean Squared Error) for continuous prediction\")\nprint(\"üí° Targets: negative=-1, positive=+1 (will extrapolate beyond)\")\nprint(\"üí° Output: Unbounded continuous scores (smoother than sigmoid)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train-run"
   },
   "outputs": [],
   "source": "# Get list of captured layers\nimport glob\n\nactivation_files = sorted(glob.glob('data/activations/sentiment/layer_*_activations.h5'))\ncaptured_layers = [int(f.split('layer_')[1].split('_')[0]) for f in activation_files]\n\nprint(f\"Found {len(captured_layers)} captured layers\")\nif captured_layers:\n    print(f\"  Layers: {captured_layers[:5]}...{captured_layers[-5:]}\" if len(captured_layers) > 10 else f\"  Layers: {captured_layers}\")\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üöÄ TRAINING REGRESSION-BASED SENTIMENT PROBES\")\nprint(\"=\"*70)\nprint(\"Using: src/probes/sentiment_regression_probe.py\")\nprint(\"Output: Continuous sentiment scores (-‚àû to +‚àû)\")\nprint(\"=\"*70 + \"\\n\")\n\ntrain_start = time.time()\nlayer_results = []\n\nfor layer_idx in captured_layers:\n    layer_start = time.time()\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Training Layer {layer_idx} ({captured_layers.index(layer_idx) + 1}/{len(captured_layers)})\")\n    print(f\"{'='*70}\")\n    \n    activation_file = f\"data/activations/sentiment/layer_{layer_idx}_activations.h5\"\n    output_dir = f\"data/probes_regression/sentiment/layer_{layer_idx}\"\n    \n    if not os.path.exists(activation_file):\n        print(f\"‚ö†Ô∏è  Skipping - activation file not found\")\n        continue\n    \n    # Build training command for REGRESSION probe\n    cmd = [\n        'python', 'src/probes/sentiment_regression_probe.py',\n        '--activations', activation_file,\n        '--output-dir', output_dir,\n        '--batch-size', str(TRAIN_CONFIG['batch_size']),\n        '--epochs', str(TRAIN_CONFIG['epochs']),\n        '--lr', str(TRAIN_CONFIG['learning_rate']),\n        '--weight-decay', str(TRAIN_CONFIG['weight_decay']),\n        '--early-stopping-patience', str(TRAIN_CONFIG['early_stopping_patience']),\n        '--device', TRAIN_CONFIG['device']\n    ]\n    \n    if not TRAIN_CONFIG['use_scheduler']:\n        cmd.append('--no-scheduler')\n    \n    # Run regression training\n    !{' '.join(cmd)}\n    \n    layer_elapsed = time.time() - layer_start\n    \n    # Load regression metrics\n    metrics_file = f\"{output_dir}/metrics.json\"\n    if os.path.exists(metrics_file):\n        with open(metrics_file, 'r') as f:\n            metrics = json.load(f)\n        \n        layer_results.append({\n            'layer': layer_idx,\n            'mse': metrics['mse'],\n            'mae': metrics['mae'],\n            'r2': metrics['r2'],\n            'accuracy': metrics['accuracy'],  # Binary accuracy at threshold 0\n            'score_range': (metrics['min_prediction'], metrics['max_prediction']),\n            'time_minutes': layer_elapsed / 60\n        })\n        \n        print(f\"\\n‚úÖ Layer {layer_idx} complete in {layer_elapsed/60:.1f} minutes\")\n        print(f\"   MSE: {metrics['mse']:.4f}, MAE: {metrics['mae']:.4f}, \"\n              f\"R¬≤: {metrics['r2']:.4f}, Accuracy: {metrics['accuracy']:.4f}\")\n        print(f\"   Score range: [{metrics['min_prediction']:.2f}, {metrics['max_prediction']:.2f}]\")\n    \n    # Backup to Drive\n    !mkdir -p {drive_output_dir}/probes_regression/\n    !cp -r {output_dir} {drive_output_dir}/probes_regression/\n\ntrain_elapsed = time.time() - train_start\n\nprint(f\"\\n{'='*70}\")\nprint(\"‚úÖ ALL REGRESSION TRAINING COMPLETE\")\nprint(f\"{'='*70}\")\nprint(f\"Total time: {train_elapsed/60:.1f} minutes ({train_elapsed/3600:.2f} hours)\")\nprint(f\"Trained {len(layer_results)} layers with continuous sentiment scoring\")\nprint(f\"{'='*70}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "viz-header"
   },
   "source": [
    "## 8Ô∏è‚É£ Visualize Performance Across Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "viz-plot"
   },
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport numpy as np\n\nif layer_results:\n    fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n    \n    layers = [r['layer'] for r in layer_results]\n    r2_scores = [r['r2'] for r in layer_results]\n    mae_scores = [r['mae'] for r in layer_results]\n    accuracy_scores = [r['accuracy'] for r in layer_results]\n    score_ranges = [r['score_range'][1] - r['score_range'][0] for r in layer_results]\n    \n    # Plot 1: R¬≤ Score (coefficient of determination)\n    axes[0, 0].plot(layers, r2_scores, 'b-o', linewidth=2, markersize=6)\n    axes[0, 0].axhline(y=np.mean(r2_scores), color='r', linestyle='--', alpha=0.5, label='Mean')\n    axes[0, 0].set_xlabel('Layer', fontsize=12)\n    axes[0, 0].set_ylabel('R¬≤ Score', fontsize=12)\n    axes[0, 0].set_title('Regression Quality Across Layers (R¬≤)', fontsize=14, fontweight='bold')\n    axes[0, 0].grid(True, alpha=0.3)\n    axes[0, 0].legend()\n    \n    # Mark best layer\n    best_idx = np.argmax(r2_scores)\n    axes[0, 0].annotate(f'Best: {layers[best_idx]}\\nR¬≤={r2_scores[best_idx]:.4f}',\n                       xy=(layers[best_idx], r2_scores[best_idx]),\n                       xytext=(10, -20), textcoords='offset points',\n                       bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n                       arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n    \n    # Plot 2: MAE (Mean Absolute Error)\n    axes[0, 1].plot(layers, mae_scores, 'g-s', linewidth=2, markersize=6)\n    axes[0, 1].axhline(y=np.mean(mae_scores), color='r', linestyle='--', alpha=0.5, label='Mean')\n    axes[0, 1].set_xlabel('Layer', fontsize=12)\n    axes[0, 1].set_ylabel('Mean Absolute Error', fontsize=12)\n    axes[0, 1].set_title('Prediction Error Across Layers (MAE)', fontsize=14, fontweight='bold')\n    axes[0, 1].grid(True, alpha=0.3)\n    axes[0, 1].legend()\n    axes[0, 1].invert_yaxis()  # Lower is better\n    \n    # Plot 3: Classification Accuracy (at threshold 0)\n    axes[1, 0].plot(layers, accuracy_scores, 'purple', marker='D', linewidth=2, markersize=6)\n    axes[1, 0].axhline(y=np.mean(accuracy_scores), color='r', linestyle='--', alpha=0.5, label='Mean')\n    axes[1, 0].set_xlabel('Layer', fontsize=12)\n    axes[1, 0].set_ylabel('Binary Accuracy', fontsize=12)\n    axes[1, 0].set_title('Classification Accuracy (threshold=0)', fontsize=14, fontweight='bold')\n    axes[1, 0].grid(True, alpha=0.3)\n    axes[1, 0].legend()\n    axes[1, 0].set_ylim([0.5, 1.0])\n    \n    # Plot 4: Score Range (dynamic range)\n    axes[1, 1].plot(layers, score_ranges, 'orange', marker='^', linewidth=2, markersize=6)\n    axes[1, 1].axhline(y=np.mean(score_ranges), color='r', linestyle='--', alpha=0.5, label='Mean')\n    axes[1, 1].set_xlabel('Layer', fontsize=12)\n    axes[1, 1].set_ylabel('Score Range', fontsize=12)\n    axes[1, 1].set_title('Dynamic Range of Predictions', fontsize=14, fontweight='bold')\n    axes[1, 1].grid(True, alpha=0.3)\n    axes[1, 1].legend()\n    \n    plt.tight_layout()\n    plt.savefig('sentiment_regression_layer_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    # Save to Drive\n    !cp sentiment_regression_layer_comparison.png {drive_output_dir}/\n    \n    print(\"\\nüìä Regression Performance Summary:\")\n    print(\"=\"*70)\n    print(f\"  Best layer (R¬≤): {layers[best_idx]} (R¬≤={r2_scores[best_idx]:.4f})\")\n    print(f\"  Mean R¬≤: {np.mean(r2_scores):.4f} ¬± {np.std(r2_scores):.4f}\")\n    print(f\"  Mean MAE: {np.mean(mae_scores):.4f} ¬± {np.std(mae_scores):.4f}\")\n    print(f\"  Mean Accuracy: {np.mean(accuracy_scores):.4f} ¬± {np.std(accuracy_scores):.4f}\")\n    print(f\"  Mean Score Range: {np.mean(score_ranges):.2f} ¬± {np.std(score_ranges):.2f}\")\n    print(\"=\"*70)\n    \n    # Show example score ranges\n    print(\"\\nüí° Typical Sentiment Score Ranges by Layer:\")\n    for i, layer_idx in enumerate(layers[:5]):  # Show first 5\n        min_score, max_score = layer_results[i]['score_range']\n        print(f\"  Layer {layer_idx:2d}: [{min_score:+.2f}, {max_score:+.2f}]\")\n    if len(layers) > 5:\n        print(f\"  ... ({len(layers)-5} more layers)\")\n        \nelse:\n    print(\"‚ö†Ô∏è  No results to visualize\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-header"
   },
   "source": [
    "## 9Ô∏è‚É£ Download Trained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download"
   },
   "outputs": [],
   "source": "from google.colab import files\n\n# Find best performing layer (by R¬≤ score)\nif layer_results:\n    best_layer = max(layer_results, key=lambda x: x['r2'])['layer']\n    \n    print(f\"Creating download package for best layer ({best_layer})...\")\n    print(f\"  R¬≤: {max(layer_results, key=lambda x: x['r2'])['r2']:.4f}\")\n    print(f\"  MAE: {max(layer_results, key=lambda x: x['r2'])['mae']:.4f}\")\n    \n    # Create zip of best layer\n    best_layer_zip = f'sentiment_regression_probes_layer_{best_layer}.zip'\n    !cd data/probes_regression/sentiment && zip -r ../../../{best_layer_zip} layer_{best_layer}/ -q\n    \n    print(f\"\\nüì• Downloading {best_layer_zip}...\")\n    files.download(best_layer_zip)\n    \n    print(\"\\n‚úÖ Download complete!\")\n    print(f\"\\nPackage contains:\")\n    print(f\"  ‚Ä¢ Layer {best_layer} regression sentiment probe\")\n    print(f\"  ‚Ä¢ Outputs continuous scores (not 0-1 probabilities!)\")\n    print(f\"  ‚Ä¢ Performance metrics (MSE, MAE, R¬≤)\")\n    print(f\"  ‚Ä¢ Training history\")\n    \n    print(\"\\nüí° Usage example:\")\n    print(f\"```python\")\n    print(f\"# Load the probe\")\n    print(f\"probe, metadata = load_probe('sentiment_regression_probe.pth')\")\n    print(f\"\")\n    print(f\"# Get continuous sentiment score\")\n    print(f\"score = probe.predict(activations)  # Returns: -2.5 to +2.5 (unbounded)\")\n    print(f\"\")\n    print(f\"# Interpret:\")\n    print(f\"# Negative scores = negative sentiment\")\n    print(f\"# Positive scores = positive sentiment\")\n    print(f\"# Magnitude = intensity\")\n    print(f\"```\")\n    \n    print(\"\\nüí° To download all layers, uncomment and run:\")\n    print(\"  # !cd data/probes_regression && zip -r ../../sentiment_regression_all_layers.zip sentiment/ -q\")\n    print(\"  # files.download('sentiment_regression_all_layers.zip')\")\nelse:\n    print(\"‚ö†Ô∏è  No trained models to download\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "summary"
   },
   "source": "## üéâ Pipeline Complete!\n\n### What was accomplished:\n1. ‚úÖ Generated 1,400 sentiment examples (700 positive + 700 negative)\n2. ‚úÖ Captured activations from all 34 Gemma layers\n3. ‚úÖ **Trained REGRESSION-BASED sentiment probes for each layer**\n4. ‚úÖ Visualized regression performance across layers\n5. ‚úÖ Backed up everything to Google Drive\n\n### üÜï Key Difference: Regression vs Classification\n\n**Traditional Binary Classification**:\n- Output: 0 or 1 (negative or positive)\n- Probabilities: 0.0-1.0 via sigmoid\n- Sharp boundary, no intensity information\n\n**Our Regression Approach** ‚≠ê:\n- Output: Continuous scores (-‚àû to +‚àû)\n- Natural interpretation: sign indicates polarity, magnitude indicates intensity\n- Smooth transitions, captures subtle sentiment shifts\n- Examples:\n  - `-2.5` = Very negative\n  - `-0.3` = Slightly negative\n  - `+0.2` = Slightly positive\n  - `+2.8` = Very positive\n\n### Files saved to Google Drive:\n- `{drive_output_dir}/data/` - Generated sentiment data\n- `{drive_output_dir}/activations/` - Layer activations\n- `{drive_output_dir}/probes_regression/` - **Regression-based probes**\n- `{drive_output_dir}/sentiment_regression_layer_comparison.png` - Performance visualization\n\n### How to use the regression probes:\n\n```python\n# Load probe\nfrom src.probes.probe_models import load_probe\nprobe, metadata = load_probe('sentiment_regression_probe.pth')\n\n# Get activation from text\nfrom src.probes.capture_activations import ActivationCapture\ncapture = ActivationCapture('google/gemma-3-4b-it', layers_to_capture=[best_layer])\nactivation = capture.capture_single_example(text, best_layer)\n\n# Predict continuous sentiment score\nscore = probe.predict(activation.unsqueeze(0))\n# Returns: tensor([[-1.85]]) for negative or tensor([[+2.31]]) for positive\n\n# Interpret the score:\nif score < -1.5:\n    print(f\"Strong negative sentiment: {score:.2f}\")\nelif score < -0.5:\n    print(f\"Mild negative sentiment: {score:.2f}\")\nelif score < 0.5:\n    print(f\"Neutral sentiment: {score:.2f}\")\nelif score < 1.5:\n    print(f\"Mild positive sentiment: {score:.2f}\")\nelse:\n    print(f\"Strong positive sentiment: {score:.2f}\")\n```\n\n### Next steps:\n1. Download trained regression probes for local use\n2. Integrate with existing cognitive action probes\n3. Use continuous scores for more nuanced sentiment analysis\n4. Create visualizations showing sentiment intensity over time\n5. Experiment with different score thresholds for your application\n\n### Advantages of regression-based probes:\n‚úÖ Smoother predictions (no sigmoid compression)  \n‚úÖ Natural intensity interpretation  \n‚úÖ Better for detecting subtle sentiment shifts  \n‚úÖ Can detect extreme sentiments (scores beyond ¬±1)  \n‚úÖ More suitable for continuous sentiment tracking"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}