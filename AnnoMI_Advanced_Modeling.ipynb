{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnnoMI Advanced Modeling - Therapy Dynamics & Human Behavior\n",
    "\n",
    "This notebook implements advanced analyses to understand therapy processes and human behavioral patterns.\n",
    "\n",
    "**Analyses Included:**\n",
    "1. üîÑ **Cognitive Synchrony Analysis** - Therapist-client alignment over time\n",
    "2. üéØ **Change Talk Prediction** - Predicting client commitment from cognitive patterns\n",
    "3. üîç **Critical Moment Detection** - Identifying breakthrough and stuck points\n",
    "4. üë• **Therapist Style Profiling** - Clustering therapists by cognitive strategies\n",
    "5. üìà **Sequential Pattern Modeling** - Markov chains for conversation flow\n",
    "6. üï∏Ô∏è **Network Analysis** - Graph-based insights into cognitive action relationships\n",
    "\n",
    "**Requirements:**\n",
    "- Pre-computed predictions from `all_predictions.json`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries loaded\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading predictions...\n",
      "‚úÖ Loaded 7372 utterances (filtered)\n",
      "   Unique cognitive actions: 45\n",
      "   Transcripts: 133\n",
      "   Topics: 44\n"
     ]
    }
   ],
   "source": [
    "# Load predictions\n",
    "predictions_path = 'output/analysis_AnnoMI/all_predictions.json'\n",
    "\n",
    "print(\"üì• Loading predictions...\")\n",
    "with open(predictions_path, 'r') as f:\n",
    "    all_predictions = json.load(f)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(all_predictions)\n",
    "\n",
    "# Filter short utterances\n",
    "MIN_LENGTH = 10\n",
    "df['utterance_length'] = df['utterance_text'].str.len()\n",
    "df = df[df['utterance_length'] >= MIN_LENGTH].copy()\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} utterances (filtered)\")\n",
    "\n",
    "# Extract active actions\n",
    "def extract_active_actions(predictions_dict):\n",
    "    active = []\n",
    "    for action, data in predictions_dict.items():\n",
    "        if data.get('is_active', False):\n",
    "            active.append({\n",
    "                'action': action,\n",
    "                'confidence': data['aggregate'],\n",
    "                'best_layer': data['best_layer']\n",
    "            })\n",
    "    return active\n",
    "\n",
    "df['active_actions'] = df['predictions'].apply(extract_active_actions)\n",
    "df['num_active_actions'] = df['active_actions'].apply(len)\n",
    "df['action_names'] = df['active_actions'].apply(lambda x: [a['action'] for a in x])\n",
    "\n",
    "# Get all unique actions\n",
    "all_actions = sorted(set(\n",
    "    action for actions in df['action_names'] for action in actions\n",
    "))\n",
    "\n",
    "print(f\"   Unique cognitive actions: {len(all_actions)}\")\n",
    "print(f\"   Transcripts: {df['transcript_id'].nunique()}\")\n",
    "print(f\"   Topics: {df['topic'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-filter-header",
   "metadata": {},
   "source": [
    "## 0Ô∏è‚É£.1 Filter High-Confidence Cognitive Actions\n",
    "\n",
    "Filter predictions to keep only cognitive actions that:\n",
    "- Appear on **more than 2 layers**, OR\n",
    "- Have **100% confidence score**\n",
    "\n",
    "This removes noise and focuses on robust cognitive action detections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-filter-code",
   "metadata": {},
   "outputs": [],
   "source": "print(\"üîç Filtering cognitive actions for high confidence...\")\nprint(f\"   Total predictions before filtering: {len(all_predictions)}\")\n\n# Filter each prediction to keep only high-confidence actions\nfiltered_predictions_list = []\n\nfor pred_data in all_predictions:\n    action_layer_details = pred_data.get('action_layer_details', {})\n    predictions = pred_data['predictions']\n    \n    # Create filtered predictions dict\n    filtered_predictions = {}\n    \n    for action_name, action_info in predictions.items():\n        if not action_info.get('is_active', False):\n            continue\n        \n        # Check filtering criteria\n        num_layers = len(action_layer_details.get(action_name, []))\n        max_confidence = max([layer_info['confidence'] for layer_info in action_layer_details.get(action_name, [])], default=0)\n        \n        # Keep if: appears on >2 layers OR has 100% confidence\n        if num_layers > 2 or max_confidence >= 1.0:\n            filtered_predictions[action_name] = action_info\n    \n    # Create filtered prediction entry\n    filtered_pred_data = pred_data.copy()\n    filtered_pred_data['predictions'] = filtered_predictions\n    filtered_predictions_list.append(filtered_pred_data)\n\n# Count statistics\ntotal_actions_before = sum(len([a for a, d in pred['predictions'].items() if d.get('is_active', False)]) \n                          for pred in all_predictions)\ntotal_actions_after = sum(len([a for a, d in pred['predictions'].items() if d.get('is_active', False)]) \n                         for pred in filtered_predictions_list)\n\nprint(f\"   Total predictions after filtering: {len(filtered_predictions_list)}\")\nprint(f\"   Active actions before: {total_actions_before}\")\nprint(f\"   Active actions after: {total_actions_after}\")\nprint(f\"   Removed: {total_actions_before - total_actions_after} actions ({(total_actions_before - total_actions_after)/total_actions_before*100:.1f}%)\")\n\n# Replace all_predictions with filtered version\nall_predictions = filtered_predictions_list\n\n# Recreate DataFrame with filtered data\ndf = pd.DataFrame(all_predictions)\ndf['utterance_length'] = df['utterance_text'].str.len()\ndf = df[df['utterance_length'] >= MIN_LENGTH].copy()\n\n# Re-extract active actions from filtered predictions\ndef extract_active_actions(predictions_dict):\n    active = []\n    for action, data in predictions_dict.items():\n        if data.get('is_active', False):\n            active.append({\n                'action': action,\n                'confidence': data['aggregate'],\n                'best_layer': data['best_layer']\n            })\n    return active\n\ndf['active_actions'] = df['predictions'].apply(extract_active_actions)\ndf['num_active_actions'] = df['active_actions'].apply(len)\ndf['action_names'] = df['active_actions'].apply(lambda x: [a['action'] for a in x])\n\n# Get all unique actions (from filtered data)\nall_actions = sorted(set(\n    action for actions in df['action_names'] for action in actions\n))\n\nprint(f\"‚úÖ Filtering complete! Using {len(df)} filtered predictions for all analyses.\")\nprint(f\"   Unique cognitive actions after filtering: {len(all_actions)}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Cognitive Synchrony Analysis\n",
    "\n",
    "Measure alignment between therapist and client cognitive patterns over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cell-7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COGNITIVE SYNCHRONY ANALYSIS\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'action_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/ment_helth/brije/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'action_names'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m             vector[idx] = \u001b[32m1\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m vector\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m df[\u001b[33m'\u001b[39m\u001b[33maction_vector\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43maction_names\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.apply(\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: create_action_vector(x, all_actions)\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Calculate synchrony for each transcript\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_synchrony\u001b[39m(transcript_df, window_size=\u001b[32m5\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/ment_helth/brije/.venv/lib/python3.11/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Projects/ment_helth/brije/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'action_names'"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COGNITIVE SYNCHRONY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create cognitive action vectors for each utterance\n",
    "def create_action_vector(action_names, all_actions):\n",
    "    \"\"\"Create binary vector for cognitive actions\"\"\"\n",
    "    vector = np.zeros(len(all_actions))\n",
    "    for action in action_names:\n",
    "        if action in all_actions:\n",
    "            idx = all_actions.index(action)\n",
    "            vector[idx] = 1\n",
    "    return vector\n",
    "\n",
    "df['action_vector'] = df['action_names'].apply(\n",
    "    lambda x: create_action_vector(x, all_actions)\n",
    ")\n",
    "\n",
    "# Calculate synchrony for each transcript\n",
    "def calculate_synchrony(transcript_df, window_size=5):\n",
    "    \"\"\"\n",
    "    Calculate cognitive synchrony between therapist and client.\n",
    "    Uses sliding window to compute cosine similarity.\n",
    "    \"\"\"\n",
    "    therapist_vectors = []\n",
    "    client_vectors = []\n",
    "    positions = []\n",
    "    \n",
    "    for i in range(len(transcript_df) - window_size + 1):\n",
    "        window = transcript_df.iloc[i:i+window_size]\n",
    "        \n",
    "        t_vecs = [v for v, interlocutor in zip(window['action_vector'], window['interlocutor']) \n",
    "                  if interlocutor == 'therapist']\n",
    "        c_vecs = [v for v, interlocutor in zip(window['action_vector'], window['interlocutor']) \n",
    "                  if interlocutor == 'client']\n",
    "        \n",
    "        if len(t_vecs) > 0 and len(c_vecs) > 0:\n",
    "            # Average vectors in window\n",
    "            t_avg = np.mean(t_vecs, axis=0)\n",
    "            c_avg = np.mean(c_vecs, axis=0)\n",
    "            \n",
    "            # Cosine similarity\n",
    "            norm_t = np.linalg.norm(t_avg)\n",
    "            norm_c = np.linalg.norm(c_avg)\n",
    "            \n",
    "            if norm_t > 0 and norm_c > 0:\n",
    "                similarity = np.dot(t_avg, c_avg) / (norm_t * norm_c)\n",
    "                therapist_vectors.append(t_avg)\n",
    "                client_vectors.append(c_avg)\n",
    "                positions.append(i / len(transcript_df))  # Normalized position\n",
    "    \n",
    "    return positions, therapist_vectors, client_vectors\n",
    "\n",
    "# Compute synchrony for each transcript\n",
    "synchrony_results = []\n",
    "\n",
    "for transcript_id in df['transcript_id'].unique():\n",
    "    transcript_df = df[df['transcript_id'] == transcript_id].sort_values('utterance_id')\n",
    "    \n",
    "    if len(transcript_df) < 10:  # Skip very short transcripts\n",
    "        continue\n",
    "    \n",
    "    positions, t_vecs, c_vecs = calculate_synchrony(transcript_df, window_size=5)\n",
    "    \n",
    "    if len(positions) > 0:\n",
    "        # Calculate synchrony scores\n",
    "        synchrony_scores = []\n",
    "        for t_vec, c_vec in zip(t_vecs, c_vecs):\n",
    "            norm_t = np.linalg.norm(t_vec)\n",
    "            norm_c = np.linalg.norm(c_vec)\n",
    "            if norm_t > 0 and norm_c > 0:\n",
    "                similarity = np.dot(t_vec, c_vec) / (norm_t * norm_c)\n",
    "                synchrony_scores.append(similarity)\n",
    "        \n",
    "        mi_quality = transcript_df['mi_quality'].iloc[0]\n",
    "        topic = transcript_df['topic'].iloc[0]\n",
    "        \n",
    "        synchrony_results.append({\n",
    "            'transcript_id': transcript_id,\n",
    "            'mi_quality': mi_quality,\n",
    "            'topic': topic,\n",
    "            'positions': positions,\n",
    "            'synchrony_scores': synchrony_scores,\n",
    "            'mean_synchrony': np.mean(synchrony_scores),\n",
    "            'synchrony_trend': np.polyfit(positions, synchrony_scores, 1)[0]  # Linear trend\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚úÖ Analyzed {len(synchrony_results)} transcripts\")\n",
    "\n",
    "# Compare by MI quality\n",
    "high_quality_sync = [r['mean_synchrony'] for r in synchrony_results if r['mi_quality'] == 'high']\n",
    "low_quality_sync = [r['mean_synchrony'] for r in synchrony_results if r['mi_quality'] == 'low']\n",
    "\n",
    "print(f\"\\nüìä Mean Cognitive Synchrony:\")\n",
    "print(f\"   High quality MI: {np.mean(high_quality_sync):.3f} (¬±{np.std(high_quality_sync):.3f})\")\n",
    "print(f\"   Low quality MI:  {np.mean(low_quality_sync):.3f} (¬±{np.std(low_quality_sync):.3f})\")\n",
    "\n",
    "# Statistical test\n",
    "if len(high_quality_sync) > 0 and len(low_quality_sync) > 0:\n",
    "    t_stat, p_val = stats.ttest_ind(high_quality_sync, low_quality_sync)\n",
    "    print(f\"   t-test: t={t_stat:.3f}, p={p_val:.4f}\")\n",
    "    if p_val < 0.05:\n",
    "        print(f\"   ‚úÖ Significant difference in synchrony by MI quality!\")\n",
    "\n",
    "# Analyze trends\n",
    "high_quality_trend = [r['synchrony_trend'] for r in synchrony_results if r['mi_quality'] == 'high']\n",
    "low_quality_trend = [r['synchrony_trend'] for r in synchrony_results if r['mi_quality'] == 'low']\n",
    "\n",
    "print(f\"\\nüìà Synchrony Trend (change over session):\")\n",
    "print(f\"   High quality MI: {np.mean(high_quality_trend):.4f} (positive = increasing synchrony)\")\n",
    "print(f\"   Low quality MI:  {np.mean(low_quality_trend):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synchrony over time\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Example trajectories (high quality)\n",
    "high_quality_examples = [r for r in synchrony_results if r['mi_quality'] == 'high'][:5]\n",
    "for result in high_quality_examples:\n",
    "    axes[0, 0].plot(result['positions'], result['synchrony_scores'], alpha=0.6, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Session Progress (0=start, 1=end)', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Cognitive Synchrony', fontsize=11)\n",
    "axes[0, 0].set_title('Cognitive Synchrony Trajectories\\n(High Quality MI Sessions)', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "\n",
    "# Plot 2: Example trajectories (low quality)\n",
    "low_quality_examples = [r for r in synchrony_results if r['mi_quality'] == 'low'][:5]\n",
    "for result in low_quality_examples:\n",
    "    axes[0, 1].plot(result['positions'], result['synchrony_scores'], alpha=0.6, linewidth=2, color='red')\n",
    "axes[0, 1].set_xlabel('Session Progress (0=start, 1=end)', fontsize=11)\n",
    "axes[0, 1].set_ylabel('Cognitive Synchrony', fontsize=11)\n",
    "axes[0, 1].set_title('Cognitive Synchrony Trajectories\\n(Low Quality MI Sessions)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "\n",
    "# Plot 3: Distribution comparison\n",
    "axes[1, 0].hist([high_quality_sync, low_quality_sync], bins=20, label=['High Quality', 'Low Quality'],\n",
    "                color=['green', 'red'], alpha=0.6, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Mean Cognitive Synchrony', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Count', fontsize=11)\n",
    "axes[1, 0].set_title('Distribution of Cognitive Synchrony\\nby MI Quality', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Synchrony trends\n",
    "trend_data = pd.DataFrame([\n",
    "    {'MI Quality': 'High', 'Trend': t} for t in high_quality_trend\n",
    "] + [\n",
    "    {'MI Quality': 'Low', 'Trend': t} for t in low_quality_trend\n",
    "])\n",
    "sns.boxplot(data=trend_data, x='MI Quality', y='Trend', ax=axes[1, 1], palette=['green', 'red'])\n",
    "axes[1, 1].axhline(y=0, color='black', linestyle='--', linewidth=1, alpha=0.5)\n",
    "axes[1, 1].set_ylabel('Synchrony Trend (slope)', fontsize=11)\n",
    "axes[1, 1].set_title('Change in Synchrony Over Session\\n(positive = increasing)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/advanced_1_synchrony.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/advanced_1_synchrony.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Change Talk Prediction\n",
    "\n",
    "Predict client change talk from cognitive action patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHANGE TALK PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter client utterances with talk type labels\n",
    "client_df = df[df['interlocutor'] == 'client'].copy()\n",
    "client_df = client_df[client_df['client_talk_type'].notna()].copy()\n",
    "\n",
    "print(f\"\\nüìä Client talk type distribution:\")\n",
    "talk_type_counts = client_df['client_talk_type'].value_counts()\n",
    "for talk_type, count in talk_type_counts.items():\n",
    "    print(f\"   {talk_type:20s}: {count:4d} ({count/len(client_df)*100:.1f}%)\")\n",
    "\n",
    "# Define change talk categories\n",
    "change_talk_types = ['change']\n",
    "sustain_talk_types = ['sustain']\n",
    "neutral_types = ['neutral', 'follow/neutral']\n",
    "\n",
    "client_df['is_change_talk'] = client_df['client_talk_type'].isin(change_talk_types)\n",
    "client_df['is_sustain_talk'] = client_df['client_talk_type'].isin(sustain_talk_types)\n",
    "\n",
    "# Analyze cognitive patterns for change vs sustain talk\n",
    "change_talk_actions = Counter()\n",
    "sustain_talk_actions = Counter()\n",
    "neutral_talk_actions = Counter()\n",
    "\n",
    "for _, row in client_df.iterrows():\n",
    "    for action_data in row['active_actions']:\n",
    "        action = action_data['action']\n",
    "        confidence = action_data['confidence']\n",
    "        \n",
    "        if row['is_change_talk']:\n",
    "            change_talk_actions[action] += confidence\n",
    "        elif row['is_sustain_talk']:\n",
    "            sustain_talk_actions[action] += confidence\n",
    "        elif row['client_talk_type'] in neutral_types:\n",
    "            neutral_talk_actions[action] += confidence\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CHANGE TALK - Top 15 Cognitive Actions\")\n",
    "print(\"=\"*80)\n",
    "for action, score in change_talk_actions.most_common(15):\n",
    "    print(f\"{action:35s} {score:7.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUSTAIN TALK - Top 15 Cognitive Actions\")\n",
    "print(\"=\"*80)\n",
    "for action, score in sustain_talk_actions.most_common(15):\n",
    "    print(f\"{action:35s} {score:7.2f}\")\n",
    "\n",
    "# Find distinctive actions for change talk\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DISTINCTIVE ACTIONS FOR CHANGE TALK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate ratio: change_talk / (sustain_talk + epsilon)\n",
    "change_ratios = []\n",
    "for action in all_actions:\n",
    "    change_score = change_talk_actions.get(action, 0) + 1e-6\n",
    "    sustain_score = sustain_talk_actions.get(action, 0) + 1e-6\n",
    "    ratio = change_score / sustain_score\n",
    "    if change_score > 5:  # Only actions with reasonable frequency\n",
    "        change_ratios.append((action, ratio, change_score, sustain_score))\n",
    "\n",
    "change_ratios.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nActions MORE common in change talk:\")\n",
    "for action, ratio, change_score, sustain_score in change_ratios[:10]:\n",
    "    print(f\"{action:35s} ratio: {ratio:6.2f}x  (change: {change_score:6.2f}, sustain: {sustain_score:6.2f})\")\n",
    "\n",
    "print(\"\\nActions MORE common in sustain talk:\")\n",
    "for action, ratio, change_score, sustain_score in sorted(change_ratios, key=lambda x: x[1])[:10]:\n",
    "    print(f\"{action:35s} ratio: {ratio:6.2f}x  (change: {change_score:6.2f}, sustain: {sustain_score:6.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize change talk patterns\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Get top actions for each type\n",
    "top_change = dict(change_talk_actions.most_common(12))\n",
    "top_sustain = dict(sustain_talk_actions.most_common(12))\n",
    "\n",
    "# Combine\n",
    "all_talk_actions = sorted(\n",
    "    set(list(top_change.keys()) + list(top_sustain.keys())),\n",
    "    key=lambda x: max(top_change.get(x, 0), top_sustain.get(x, 0)),\n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "change_scores = [top_change.get(a, 0) for a in all_talk_actions]\n",
    "sustain_scores = [top_sustain.get(a, 0) for a in all_talk_actions]\n",
    "\n",
    "# Plot 1: Comparison\n",
    "x = np.arange(len(all_talk_actions))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].barh(x - width/2, change_scores, width, label='Change Talk', color='green', alpha=0.7)\n",
    "axes[0].barh(x + width/2, sustain_scores, width, label='Sustain Talk', color='red', alpha=0.7)\n",
    "axes[0].set_yticks(x)\n",
    "axes[0].set_yticklabels(all_talk_actions, fontsize=10)\n",
    "axes[0].set_xlabel('Aggregate Confidence Score', fontsize=11)\n",
    "axes[0].set_title('Cognitive Actions:\\nChange Talk vs Sustain Talk', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Plot 2: Ratio (log scale)\n",
    "ratios = []\n",
    "ratio_actions = []\n",
    "for action in all_talk_actions:\n",
    "    c_score = top_change.get(action, 0.1)\n",
    "    s_score = top_sustain.get(action, 0.1)\n",
    "    ratio = np.log2(c_score / s_score)\n",
    "    ratios.append(ratio)\n",
    "    ratio_actions.append(action)\n",
    "\n",
    "colors = ['green' if r > 0 else 'red' for r in ratios]\n",
    "axes[1].barh(range(len(ratio_actions)), ratios, color=colors, alpha=0.7)\n",
    "axes[1].set_yticks(range(len(ratio_actions)))\n",
    "axes[1].set_yticklabels(ratio_actions, fontsize=10)\n",
    "axes[1].set_xlabel('Log2(Change/Sustain)', fontsize=11)\n",
    "axes[1].set_title('Cognitive Action Bias:\\nChange-Associated (green) vs Sustain-Associated (red)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].axvline(x=0, color='black', linestyle='--', linewidth=1)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/advanced_2_change_talk.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/advanced_2_change_talk.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Critical Moment Detection\n",
    "\n",
    "Identify breakthrough moments and stuck points in therapy sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CRITICAL MOMENT DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define features for critical moment detection\n",
    "def calculate_utterance_features(row):\n",
    "    \"\"\"Calculate features that might indicate critical moments\"\"\"\n",
    "    actions = row['action_names']\n",
    "    \n",
    "    # Feature 1: Cognitive diversity\n",
    "    cognitive_diversity = len(actions)\n",
    "    \n",
    "    # Feature 2: Metacognitive index\n",
    "    metacog_actions = ['meta_awareness', 'metacognitive_monitoring', 'metacognitive_regulation', \n",
    "                       'self_questioning', 'cognition_awareness']\n",
    "    metacog_score = sum(1 for a in actions if a in metacog_actions)\n",
    "    \n",
    "    # Feature 3: Emotional processing\n",
    "    emotion_actions = ['emotion_perception', 'emotion_understanding', 'emotional_reappraisal',\n",
    "                       'emotion_responding', 'emotion_management']\n",
    "    emotion_score = sum(1 for a in actions if a in emotion_actions)\n",
    "    \n",
    "    # Feature 4: Perspective transformation\n",
    "    transform_actions = ['perspective_taking', 'reframing', 'reconsidering', 'updating_beliefs']\n",
    "    transform_score = sum(1 for a in actions if a in transform_actions)\n",
    "    \n",
    "    # Feature 5: Integration (combination of high-level processes)\n",
    "    integration_score = metacog_score * (emotion_score + transform_score)\n",
    "    \n",
    "    # Feature 6: Complexity (unique cognitive actions)\n",
    "    complexity = len(set(actions))\n",
    "    \n",
    "    return {\n",
    "        'cognitive_diversity': cognitive_diversity,\n",
    "        'metacognitive_index': metacog_score,\n",
    "        'emotional_processing': emotion_score,\n",
    "        'perspective_transformation': transform_score,\n",
    "        'integration_score': integration_score,\n",
    "        'complexity': complexity\n",
    "    }\n",
    "\n",
    "# Calculate features for all utterances\n",
    "features_list = []\n",
    "for _, row in df.iterrows():\n",
    "    features = calculate_utterance_features(row)\n",
    "    features_list.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(features_list)\n",
    "df = pd.concat([df.reset_index(drop=True), features_df], axis=1)\n",
    "\n",
    "# Define breakthrough moments (high integration + metacognition)\n",
    "# Use top 5% as breakthrough moments\n",
    "breakthrough_threshold = df['integration_score'].quantile(0.95)\n",
    "df['is_breakthrough'] = (df['integration_score'] >= breakthrough_threshold) & \\\n",
    "                        (df['metacognitive_index'] >= 1)\n",
    "\n",
    "print(f\"\\nüéØ Detected {df['is_breakthrough'].sum()} breakthrough moments ({df['is_breakthrough'].sum()/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Analyze breakthrough moments\n",
    "breakthrough_df = df[df['is_breakthrough']].copy()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BREAKTHROUGH MOMENT CHARACTERISTICS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   Mean cognitive diversity: {breakthrough_df['cognitive_diversity'].mean():.2f}\")\n",
    "print(f\"   Mean metacognitive index: {breakthrough_df['metacognitive_index'].mean():.2f}\")\n",
    "print(f\"   Mean emotional processing: {breakthrough_df['emotional_processing'].mean():.2f}\")\n",
    "print(f\"   Mean perspective transformation: {breakthrough_df['perspective_transformation'].mean():.2f}\")\n",
    "\n",
    "# Show examples of breakthrough moments\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXAMPLE BREAKTHROUGH MOMENTS (Top 5 by integration score)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "top_breakthroughs = df.nlargest(5, 'integration_score')\n",
    "for i, (_, row) in enumerate(top_breakthroughs.iterrows(), 1):\n",
    "    print(f\"\\n{i}. [{row['interlocutor'].upper()}] (integration={row['integration_score']:.1f})\")\n",
    "    print(f\"   Text: {row['utterance_text'][:200]}...\")\n",
    "    print(f\"   Actions: {', '.join(row['action_names'][:10])}\")\n",
    "    print(f\"   Features: metacog={row['metacognitive_index']}, emotion={row['emotional_processing']}, transform={row['perspective_transformation']}\")\n",
    "\n",
    "# Detect stuck points (low complexity, repetitive)\n",
    "# Group by transcript and detect repeated low-complexity sequences\n",
    "stuck_points = []\n",
    "\n",
    "for transcript_id in df['transcript_id'].unique():\n",
    "    transcript_df = df[df['transcript_id'] == transcript_id].sort_values('utterance_id')\n",
    "    \n",
    "    # Sliding window to detect stuck points\n",
    "    window_size = 5\n",
    "    for i in range(len(transcript_df) - window_size + 1):\n",
    "        window = transcript_df.iloc[i:i+window_size]\n",
    "        \n",
    "        # Low average complexity and diversity\n",
    "        avg_complexity = window['complexity'].mean()\n",
    "        avg_diversity = window['cognitive_diversity'].mean()\n",
    "        \n",
    "        if avg_complexity < 2 and avg_diversity < 2:\n",
    "            stuck_points.append({\n",
    "                'transcript_id': transcript_id,\n",
    "                'start_position': i / len(transcript_df),\n",
    "                'avg_complexity': avg_complexity,\n",
    "                'avg_diversity': avg_diversity,\n",
    "                'utterances': window['utterance_text'].tolist()\n",
    "            })\n",
    "\n",
    "print(f\"\\n\\nüî¥ Detected {len(stuck_points)} potential stuck points\")\n",
    "\n",
    "if len(stuck_points) > 0:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"EXAMPLE STUCK POINTS (Low complexity windows)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for i, stuck in enumerate(stuck_points[:3], 1):\n",
    "        print(f\"\\n{i}. Transcript {stuck['transcript_id']}, position {stuck['start_position']:.2%}\")\n",
    "        print(f\"   Avg complexity: {stuck['avg_complexity']:.2f}, diversity: {stuck['avg_diversity']:.2f}\")\n",
    "        print(f\"   Sample utterances:\")\n",
    "        for utt in stuck['utterances'][:3]:\n",
    "            print(f\"      - {utt[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize critical moments\n",
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Feature distributions for breakthrough vs normal\n",
    "normal_df = df[~df['is_breakthrough']]\n",
    "\n",
    "axes[0, 0].hist([normal_df['integration_score'], breakthrough_df['integration_score']], \n",
    "                bins=30, label=['Normal', 'Breakthrough'], color=['gray', 'gold'], alpha=0.6)\n",
    "axes[0, 0].set_xlabel('Integration Score', fontsize=11)\n",
    "axes[0, 0].set_ylabel('Count', fontsize=11)\n",
    "axes[0, 0].set_title('Integration Score Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: Breakthrough moments by interlocutor\n",
    "breakthrough_by_interlocutor = breakthrough_df['interlocutor'].value_counts()\n",
    "axes[0, 1].bar(breakthrough_by_interlocutor.index, breakthrough_by_interlocutor.values, \n",
    "               color=['steelblue', 'coral'], alpha=0.7)\n",
    "axes[0, 1].set_ylabel('Count', fontsize=11)\n",
    "axes[0, 1].set_title('Breakthrough Moments by Speaker', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 3: Feature comparison (breakthrough vs normal)\n",
    "feature_names = ['metacognitive_index', 'emotional_processing', 'perspective_transformation', 'complexity']\n",
    "normal_means = [normal_df[f].mean() for f in feature_names]\n",
    "breakthrough_means = [breakthrough_df[f].mean() for f in feature_names]\n",
    "\n",
    "x = np.arange(len(feature_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1, 0].bar(x - width/2, normal_means, width, label='Normal', color='gray', alpha=0.7)\n",
    "axes[1, 0].bar(x + width/2, breakthrough_means, width, label='Breakthrough', color='gold', alpha=0.7)\n",
    "axes[1, 0].set_xticks(x)\n",
    "axes[1, 0].set_xticklabels([f.replace('_', '\\n') for f in feature_names], fontsize=9)\n",
    "axes[1, 0].set_ylabel('Mean Score', fontsize=11)\n",
    "axes[1, 0].set_title('Feature Comparison:\\nNormal vs Breakthrough Moments', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Breakthrough moments across session (example transcript)\n",
    "# Pick a high-quality transcript with breakthrough moments\n",
    "example_transcript = None\n",
    "for tid in df['transcript_id'].unique():\n",
    "    t_df = df[df['transcript_id'] == tid]\n",
    "    if t_df['is_breakthrough'].sum() >= 2 and t_df['mi_quality'].iloc[0] == 'high':\n",
    "        example_transcript = tid\n",
    "        break\n",
    "\n",
    "if example_transcript is not None:\n",
    "    t_df = df[df['transcript_id'] == example_transcript].sort_values('utterance_id')\n",
    "    positions = np.arange(len(t_df)) / len(t_df)\n",
    "    \n",
    "    axes[1, 1].plot(positions, t_df['integration_score'].values, color='steelblue', linewidth=2, label='Integration Score')\n",
    "    \n",
    "    # Mark breakthrough moments\n",
    "    breakthrough_positions = positions[t_df['is_breakthrough'].values]\n",
    "    breakthrough_scores = t_df[t_df['is_breakthrough']]['integration_score'].values\n",
    "    axes[1, 1].scatter(breakthrough_positions, breakthrough_scores, color='gold', s=200, \n",
    "                       marker='*', edgecolors='black', linewidths=1.5, label='Breakthrough', zorder=5)\n",
    "    \n",
    "    axes[1, 1].set_xlabel('Session Progress', fontsize=11)\n",
    "    axes[1, 1].set_ylabel('Integration Score', fontsize=11)\n",
    "    axes[1, 1].set_title(f'Critical Moments in Example Session\\n(Transcript {example_transcript})', \n",
    "                         fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].legend(fontsize=10)\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "else:\n",
    "    axes[1, 1].text(0.5, 0.5, 'No suitable example transcript found', \n",
    "                    ha='center', va='center', fontsize=12)\n",
    "    axes[1, 1].set_xticks([])\n",
    "    axes[1, 1].set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/advanced_3_critical_moments.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/advanced_3_critical_moments.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Therapist Style Profiling\n",
    "\n",
    "Cluster therapists by their cognitive action patterns to identify therapeutic styles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"THERAPIST STYLE PROFILING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create therapist profiles based on cognitive action patterns\n",
    "# Since we don't have explicit therapist IDs, we'll use transcripts as proxies\n",
    "\n",
    "therapist_profiles = []\n",
    "\n",
    "for transcript_id in df['transcript_id'].unique():\n",
    "    therapist_df = df[(df['transcript_id'] == transcript_id) & \n",
    "                      (df['interlocutor'] == 'therapist')]\n",
    "    \n",
    "    if len(therapist_df) < 5:  # Skip transcripts with too few therapist utterances\n",
    "        continue\n",
    "    \n",
    "    # Create cognitive action profile (frequency vector)\n",
    "    action_counts = Counter()\n",
    "    for actions in therapist_df['action_names']:\n",
    "        for action in actions:\n",
    "            action_counts[action] += 1\n",
    "    \n",
    "    # Normalize by number of utterances\n",
    "    action_profile = {action: count / len(therapist_df) \n",
    "                     for action, count in action_counts.items()}\n",
    "    \n",
    "    # Create feature vector\n",
    "    profile_vector = [action_profile.get(action, 0) for action in all_actions]\n",
    "    \n",
    "    therapist_profiles.append({\n",
    "        'transcript_id': transcript_id,\n",
    "        'profile_vector': profile_vector,\n",
    "        'action_counts': action_counts,\n",
    "        'num_utterances': len(therapist_df),\n",
    "        'mi_quality': therapist_df['mi_quality'].iloc[0],\n",
    "        'topic': therapist_df['topic'].iloc[0]\n",
    "    })\n",
    "\n",
    "print(f\"\\n‚úÖ Created {len(therapist_profiles)} therapist profiles\")\n",
    "\n",
    "# Create feature matrix\n",
    "X = np.array([p['profile_vector'] for p in therapist_profiles])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Reduce dimensionality for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "print(f\"\\nüìä PCA explained variance: {pca.explained_variance_ratio_[0]:.2%}, {pca.explained_variance_ratio_[1]:.2%}\")\n",
    "\n",
    "# Cluster therapists\n",
    "# Try different numbers of clusters\n",
    "silhouette_scores = []\n",
    "for n_clusters in range(2, 8):\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    labels = kmeans.fit_predict(X_scaled)\n",
    "    score = silhouette_score(X_scaled, labels)\n",
    "    silhouette_scores.append((n_clusters, score))\n",
    "\n",
    "print(\"\\nüìà Silhouette scores for different cluster counts:\")\n",
    "for n, score in silhouette_scores:\n",
    "    print(f\"   {n} clusters: {score:.3f}\")\n",
    "\n",
    "# Use optimal number of clusters\n",
    "optimal_k = max(silhouette_scores, key=lambda x: x[1])[0]\n",
    "print(f\"\\n‚úÖ Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster labels to profiles\n",
    "for i, profile in enumerate(therapist_profiles):\n",
    "    profile['cluster'] = cluster_labels[i]\n",
    "    profile['pca_x'] = X_pca[i, 0]\n",
    "    profile['pca_y'] = X_pca[i, 1]\n",
    "\n",
    "# Analyze each cluster\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"THERAPIST STYLE CLUSTERS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_profiles = [p for p in therapist_profiles if p['cluster'] == cluster_id]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CLUSTER {cluster_id + 1} - {len(cluster_profiles)} therapists ({len(cluster_profiles)/len(therapist_profiles)*100:.1f}%)\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Aggregate actions for this cluster\n",
    "    cluster_actions = Counter()\n",
    "    for profile in cluster_profiles:\n",
    "        cluster_actions.update(profile['action_counts'])\n",
    "    \n",
    "    # Normalize by number of profiles\n",
    "    total_utterances = sum(p['num_utterances'] for p in cluster_profiles)\n",
    "    \n",
    "    print(f\"\\nTop 10 characteristic cognitive actions:\")\n",
    "    for action, count in cluster_actions.most_common(10):\n",
    "        avg_per_utterance = count / total_utterances\n",
    "        print(f\"   {action:35s} {avg_per_utterance:.3f} per utterance\")\n",
    "    \n",
    "    # MI quality distribution\n",
    "    high_quality = sum(1 for p in cluster_profiles if p['mi_quality'] == 'high')\n",
    "    print(f\"\\nMI Quality: {high_quality}/{len(cluster_profiles)} high quality ({high_quality/len(cluster_profiles)*100:.1f}%)\")\n",
    "    \n",
    "    # Topic distribution\n",
    "    topics = Counter(p['topic'] for p in cluster_profiles)\n",
    "    print(f\"\\nTop topics:\")\n",
    "    for topic, count in topics.most_common(3):\n",
    "        print(f\"   {topic}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize therapist styles\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Plot 1: PCA scatter colored by cluster\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, optimal_k))\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_profiles = [p for p in therapist_profiles if p['cluster'] == cluster_id]\n",
    "    x_coords = [p['pca_x'] for p in cluster_profiles]\n",
    "    y_coords = [p['pca_y'] for p in cluster_profiles]\n",
    "    axes[0].scatter(x_coords, y_coords, c=[colors[cluster_id]], \n",
    "                   s=100, alpha=0.6, edgecolors='black', linewidths=0.5,\n",
    "                   label=f'Style {cluster_id + 1}')\n",
    "\n",
    "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=11)\n",
    "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=11)\n",
    "axes[0].set_title('Therapist Styles (PCA Projection)', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10, loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: MI quality by cluster\n",
    "cluster_quality = []\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_profiles = [p for p in therapist_profiles if p['cluster'] == cluster_id]\n",
    "    high_quality_pct = sum(1 for p in cluster_profiles if p['mi_quality'] == 'high') / len(cluster_profiles) * 100\n",
    "    cluster_quality.append(high_quality_pct)\n",
    "\n",
    "axes[1].bar(range(1, optimal_k + 1), cluster_quality, color=colors, alpha=0.7, edgecolor='black')\n",
    "axes[1].set_xlabel('Therapist Style Cluster', fontsize=11)\n",
    "axes[1].set_ylabel('% High Quality MI', fontsize=11)\n",
    "axes[1].set_title('MI Quality by Therapist Style', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xticks(range(1, optimal_k + 1))\n",
    "axes[1].set_xticklabels([f'Style {i}' for i in range(1, optimal_k + 1)])\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "axes[1].set_ylim([0, 100])\n",
    "\n",
    "# Add percentage labels on bars\n",
    "for i, pct in enumerate(cluster_quality):\n",
    "    axes[1].text(i + 1, pct + 2, f'{pct:.1f}%', ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/advanced_4_therapist_styles.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/advanced_4_therapist_styles.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-18",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Sequential Pattern Modeling (Markov Chains)\n",
    "\n",
    "Model conversation flow as Markov chains to understand transition probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"SEQUENTIAL PATTERN MODELING - MARKOV CHAINS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build transition matrix for cognitive actions\n",
    "# Focus on top N most common actions for clarity\n",
    "TOP_N_ACTIONS = 20\n",
    "\n",
    "# Get top actions\n",
    "all_action_counts = Counter()\n",
    "for actions in df['action_names']:\n",
    "    for action in actions:\n",
    "        all_action_counts[action] += 1\n",
    "\n",
    "top_actions = [action for action, _ in all_action_counts.most_common(TOP_N_ACTIONS)]\n",
    "action_to_idx = {action: i for i, action in enumerate(top_actions)}\n",
    "\n",
    "print(f\"\\nüìä Building transition matrix for top {TOP_N_ACTIONS} actions...\")\n",
    "\n",
    "# Initialize transition matrix\n",
    "transition_matrix = np.zeros((TOP_N_ACTIONS, TOP_N_ACTIONS))\n",
    "\n",
    "# Count transitions\n",
    "for transcript_id in df['transcript_id'].unique():\n",
    "    transcript_df = df[df['transcript_id'] == transcript_id].sort_values('utterance_id')\n",
    "    \n",
    "    for i in range(len(transcript_df) - 1):\n",
    "        current_actions = [a for a in transcript_df.iloc[i]['action_names'] if a in top_actions]\n",
    "        next_actions = [a for a in transcript_df.iloc[i+1]['action_names'] if a in top_actions]\n",
    "        \n",
    "        for curr_action in current_actions:\n",
    "            for next_action in next_actions:\n",
    "                curr_idx = action_to_idx[curr_action]\n",
    "                next_idx = action_to_idx[next_action]\n",
    "                transition_matrix[curr_idx, next_idx] += 1\n",
    "\n",
    "# Normalize rows to get probabilities\n",
    "row_sums = transition_matrix.sum(axis=1, keepdims=True)\n",
    "row_sums[row_sums == 0] = 1  # Avoid division by zero\n",
    "transition_probs = transition_matrix / row_sums\n",
    "\n",
    "print(\"\\n‚úÖ Transition matrix built\")\n",
    "\n",
    "# Find most common transitions\n",
    "transitions = []\n",
    "for i in range(TOP_N_ACTIONS):\n",
    "    for j in range(TOP_N_ACTIONS):\n",
    "        if transition_matrix[i, j] > 0:\n",
    "            transitions.append((\n",
    "                top_actions[i],\n",
    "                top_actions[j],\n",
    "                transition_matrix[i, j],\n",
    "                transition_probs[i, j]\n",
    "            ))\n",
    "\n",
    "transitions.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 20 COGNITIVE ACTION TRANSITIONS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'From':25s} -> {'To':25s} {'Count':>6s} {'Prob':>8s}\")\n",
    "print(\"-\" * 80)\n",
    "for from_action, to_action, count, prob in transitions[:20]:\n",
    "    print(f\"{from_action:25s} -> {to_action:25s} {int(count):6d} {prob:7.1%}\")\n",
    "\n",
    "# Calculate stationary distribution (eigenvector with eigenvalue 1)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(transition_probs.T)\n",
    "stationary_idx = np.argmin(np.abs(eigenvalues - 1))\n",
    "stationary_dist = np.real(eigenvectors[:, stationary_idx])\n",
    "stationary_dist = stationary_dist / stationary_dist.sum()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATIONARY DISTRIBUTION (Long-term probabilities)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stationary_actions = sorted(zip(top_actions, stationary_dist), key=lambda x: x[1], reverse=True)\n",
    "for action, prob in stationary_actions[:15]:\n",
    "    bar = \"‚ñà\" * int(prob * 200)\n",
    "    print(f\"{action:35s} {prob:6.2%} {bar}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize transition matrix\n",
    "fig, ax = plt.subplots(figsize=(16, 14))\n",
    "\n",
    "# Use log scale for better visibility\n",
    "transition_matrix_log = np.log10(transition_matrix + 1)\n",
    "\n",
    "im = ax.imshow(transition_matrix_log, cmap='YlOrRd', aspect='auto')\n",
    "\n",
    "ax.set_xticks(range(TOP_N_ACTIONS))\n",
    "ax.set_yticks(range(TOP_N_ACTIONS))\n",
    "ax.set_xticklabels(top_actions, rotation=45, ha='right', fontsize=9)\n",
    "ax.set_yticklabels(top_actions, fontsize=9)\n",
    "\n",
    "ax.set_xlabel('To Action', fontsize=12)\n",
    "ax.set_ylabel('From Action', fontsize=12)\n",
    "ax.set_title('Cognitive Action Transition Matrix\\n(Markov Chain - log scale)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_label('log10(Transition Count + 1)', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/advanced_5_markov_chain.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/advanced_5_markov_chain.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Network Analysis\n",
    "\n",
    "Graph-based analysis of cognitive action relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"NETWORK ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create directed graph from transition matrix\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add nodes\n",
    "for action in top_actions:\n",
    "    G.add_node(action)\n",
    "\n",
    "# Add edges (only significant transitions, top 30%)\n",
    "threshold = np.percentile([t[2] for t in transitions], 70)\n",
    "\n",
    "for from_action, to_action, count, prob in transitions:\n",
    "    if count >= threshold:\n",
    "        G.add_edge(from_action, to_action, weight=count, prob=prob)\n",
    "\n",
    "print(f\"\\nüìä Network statistics:\")\n",
    "print(f\"   Nodes: {G.number_of_nodes()}\")\n",
    "print(f\"   Edges: {G.number_of_edges()}\")\n",
    "print(f\"   Density: {nx.density(G):.3f}\")\n",
    "\n",
    "# Calculate centrality measures\n",
    "degree_centrality = nx.degree_centrality(G)\n",
    "betweenness_centrality = nx.betweenness_centrality(G)\n",
    "pagerank = nx.pagerank(G)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TOP 10 ACTIONS BY CENTRALITY MEASURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nDegree Centrality (most connected):\")\n",
    "for action, score in sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"   {action:35s} {score:.3f}\")\n",
    "\n",
    "print(\"\\nBetweenness Centrality (bridges between other actions):\")\n",
    "for action, score in sorted(betweenness_centrality.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"   {action:35s} {score:.3f}\")\n",
    "\n",
    "print(\"\\nPageRank (importance in network):\")\n",
    "for action, score in sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"   {action:35s} {score:.3f}\")\n",
    "\n",
    "# Detect communities\n",
    "# Convert to undirected for community detection\n",
    "G_undirected = G.to_undirected()\n",
    "communities = list(nx.community.greedy_modularity_communities(G_undirected))\n",
    "\n",
    "print(f\"\\nüìä Detected {len(communities)} communities\")\n",
    "\n",
    "for i, community in enumerate(communities, 1):\n",
    "    print(f\"\\nCommunity {i} ({len(community)} actions):\")\n",
    "    for action in sorted(community):\n",
    "        print(f\"   ‚Ä¢ {action}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize network\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 10))\n",
    "\n",
    "# Plot 1: Network graph with communities\n",
    "pos = nx.spring_layout(G, k=2, iterations=50, seed=42)\n",
    "\n",
    "# Color nodes by community\n",
    "node_colors = []\n",
    "community_colors = plt.cm.tab10(np.linspace(0, 1, len(communities)))\n",
    "node_to_community = {}\n",
    "for i, community in enumerate(communities):\n",
    "    for node in community:\n",
    "        node_to_community[node] = i\n",
    "\n",
    "for node in G.nodes():\n",
    "    comm_id = node_to_community.get(node, 0)\n",
    "    node_colors.append(community_colors[comm_id])\n",
    "\n",
    "# Node sizes by PageRank\n",
    "node_sizes = [pagerank[node] * 20000 for node in G.nodes()]\n",
    "\n",
    "# Draw network\n",
    "nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=node_sizes, \n",
    "                       alpha=0.7, edgecolors='black', linewidths=1, ax=axes[0])\n",
    "nx.draw_networkx_labels(G, pos, font_size=7, font_weight='bold', ax=axes[0])\n",
    "\n",
    "# Draw edges with varying thickness\n",
    "edges = G.edges()\n",
    "weights = [G[u][v]['weight'] for u, v in edges]\n",
    "max_weight = max(weights)\n",
    "edge_widths = [w / max_weight * 3 for w in weights]\n",
    "\n",
    "nx.draw_networkx_edges(G, pos, width=edge_widths, alpha=0.3, \n",
    "                       arrows=True, arrowsize=10, ax=axes[0], \n",
    "                       edge_color='gray', connectionstyle='arc3,rad=0.1')\n",
    "\n",
    "axes[0].set_title('Cognitive Action Network\\n(Node size = PageRank, Colors = Communities)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Plot 2: Centrality comparison\n",
    "top_10_actions_cent = list(sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)[:10])\n",
    "actions_for_plot = [a for a, _ in top_10_actions_cent]\n",
    "\n",
    "degree_scores = [degree_centrality[a] for a in actions_for_plot]\n",
    "betweenness_scores = [betweenness_centrality[a] for a in actions_for_plot]\n",
    "pagerank_scores = [pagerank[a] * 10 for a in actions_for_plot]  # Scale for visibility\n",
    "\n",
    "x = np.arange(len(actions_for_plot))\n",
    "width = 0.25\n",
    "\n",
    "axes[1].barh(x - width, degree_scores, width, label='Degree', color='steelblue', alpha=0.7)\n",
    "axes[1].barh(x, betweenness_scores, width, label='Betweenness', color='coral', alpha=0.7)\n",
    "axes[1].barh(x + width, pagerank_scores, width, label='PageRank (√ó10)', color='green', alpha=0.7)\n",
    "\n",
    "axes[1].set_yticks(x)\n",
    "axes[1].set_yticklabels(actions_for_plot, fontsize=10)\n",
    "axes[1].set_xlabel('Centrality Score', fontsize=11)\n",
    "axes[1].set_title('Network Centrality Measures\\n(Top 10 Actions)', fontsize=12, fontweight='bold')\n",
    "axes[1].legend(fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/advanced_6_network_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/advanced_6_network_analysis.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-24",
   "metadata": {},
   "source": [
    "## Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ADVANCED ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    'synchrony_analysis': {\n",
    "        'mean_high_quality': float(np.mean(high_quality_sync)) if len(high_quality_sync) > 0 else None,\n",
    "        'mean_low_quality': float(np.mean(low_quality_sync)) if len(low_quality_sync) > 0 else None,\n",
    "        'significance': 'p < 0.05' if p_val < 0.05 else 'not significant'\n",
    "    },\n",
    "    'change_talk': {\n",
    "        'top_change_actions': [action for action, _ in change_talk_actions.most_common(10)],\n",
    "        'top_sustain_actions': [action for action, _ in sustain_talk_actions.most_common(10)],\n",
    "        'most_predictive_of_change': [action for action, _, _, _ in change_ratios[:5]]\n",
    "    },\n",
    "    'critical_moments': {\n",
    "        'breakthrough_count': int(df['is_breakthrough'].sum()),\n",
    "        'breakthrough_percentage': float(df['is_breakthrough'].sum() / len(df) * 100),\n",
    "        'stuck_points_detected': len(stuck_points)\n",
    "    },\n",
    "    'therapist_styles': {\n",
    "        'num_clusters': int(optimal_k),\n",
    "        'cluster_sizes': [int(sum(1 for p in therapist_profiles if p['cluster'] == i)) \n",
    "                         for i in range(optimal_k)],\n",
    "        'cluster_quality_percentages': [float(pct) for pct in cluster_quality]\n",
    "    },\n",
    "    'markov_chain': {\n",
    "        'top_transitions': [(from_a, to_a, float(prob)) \n",
    "                           for from_a, to_a, _, prob in transitions[:10]],\n",
    "        'stationary_distribution': {action: float(prob) \n",
    "                                   for action, prob in stationary_actions[:10]}\n",
    "    },\n",
    "    'network_analysis': {\n",
    "        'num_nodes': int(G.number_of_nodes()),\n",
    "        'num_edges': int(G.number_of_edges()),\n",
    "        'density': float(nx.density(G)),\n",
    "        'num_communities': len(communities),\n",
    "        'top_pagerank': {action: float(score) \n",
    "                        for action, score in sorted(pagerank.items(), \n",
    "                                                   key=lambda x: x[1], reverse=True)[:10]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open('output/analysis_AnnoMI/advanced_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Summary exported to: output/analysis_AnnoMI/advanced_analysis_summary.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ANALYSES COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated visualizations:\")\n",
    "print(\"   1. advanced_1_synchrony.png\")\n",
    "print(\"   2. advanced_2_change_talk.png\")\n",
    "print(\"   3. advanced_3_critical_moments.png\")\n",
    "print(\"   4. advanced_4_therapist_styles.png\")\n",
    "print(\"   5. advanced_5_markov_chain.png\")\n",
    "print(\"   6. advanced_6_network_analysis.png\")\n",
    "print(\"\\nData exports:\")\n",
    "print(\"   - advanced_analysis_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}