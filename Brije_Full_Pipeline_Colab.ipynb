{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Brije: Cognitive Action Detection - Full Pipeline (Google Colab)\n\nComplete pipeline for training **binary cognitive action probes across all layers** on Gemma 3 4B.\n\n**This notebook will:**\n1. ‚úÖ Clone the Brije repository\n2. ‚úÖ Install all dependencies\n3. ‚úÖ Capture activations from Gemma 3 4B layers 4-28 (~3-4 hours) using batch saving\n4. ‚úÖ Train 45 binary probes per layer (1,125 total probes) (~8-12 hours)\n5. ‚úÖ Compare performance across layers\n6. ‚úÖ Test with multi-probe inference\n7. ‚úÖ Download trained models to Google Drive\n\n**Requirements:**\n- Google Colab with A100 GPU (40GB VRAM recommended)\n- Runtime: ~12-16 hours total (can run in stages)\n\n**Dataset:** 31,500 cognitive action examples across 45 actions\n\n**Architecture:** One-vs-rest binary classification, 45 probes √ó 25 layers = 1,125 total probes"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU and Setup Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow on CPU.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone the repository\n",
    "repo_url = \"https://github.com/ChuloIva/brije.git\"\n",
    "repo_name = \"brije\"\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    print(\"üì• Cloning Brije repository...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "    print(\"üîÑ Pulling latest changes...\")\n",
    "    !cd {repo_name} && git pull\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\nprint(\"üì¶ Installing dependencies...\\n\")\n!pip install -q torch transformers h5py scikit-learn tqdm matplotlib seaborn\n\n# Clone and install nnsight\nnnsight_dir = \"third_party/nnsight\"\nnnsight_repo = \"https://github.com/ndif-team/nnsight\"\n\nprint(\"\\nüì¶ Setting up nnsight...\")\nif not os.path.exists(nnsight_dir) or not os.listdir(nnsight_dir):\n    print(\"   Cloning nnsight repository...\")\n    # Create third_party directory if it doesn't exist\n    os.makedirs(\"third_party\", exist_ok=True)\n    # Clone nnsight\n    !git clone {nnsight_repo} {nnsight_dir}\n    print(\"   ‚úÖ nnsight repository cloned\")\nelse:\n    print(\"   ‚úÖ nnsight repository already exists\")\n\n# Install nnsight\nprint(\"   Installing nnsight...\")\n!pip install -q -e {nnsight_dir}\n\nprint(\"\\n‚úÖ All dependencies installed!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "from google.colab import drive\ndrive.mount('/content/drive')\n\n# Create directories in Google Drive for outputs\ndrive_output_dir = '/content/drive/MyDrive/brije_outputs'\nos.makedirs(drive_output_dir, exist_ok=True)\nos.makedirs(f\"{drive_output_dir}/activations\", exist_ok=True)\nos.makedirs(f\"{drive_output_dir}/probes_binary\", exist_ok=True)\n\nprint(f\"‚úÖ Outputs will be saved to: {drive_output_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories in Google Drive for outputs\n",
    "drive_output_dir = '/content/drive/MyDrive/brije_outputs'\n",
    "os.makedirs(drive_output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/activations\", exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/probes\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Outputs will be saved to: {drive_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "import glob\n",
    "\n",
    "dataset_path = \"third_party/datagen/generated_data\"\n",
    "datasets = glob.glob(f\"{dataset_path}/*.jsonl\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AVAILABLE DATASETS\")\n",
    "print(\"=\"*60)\n",
    "for ds in datasets:\n",
    "    size = os.path.getsize(ds) / 1e6\n",
    "    print(f\"  {os.path.basename(ds)} ({size:.2f} MB)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the stratified combined dataset (31.5k examples)\n",
    "dataset_file = None\n",
    "for ds in datasets:\n",
    "    if 'stratified_combined' in ds or '31500' in ds:\n",
    "        dataset_file = ds\n",
    "        break\n",
    "\n",
    "if not dataset_file:\n",
    "    # Use any available dataset\n",
    "    dataset_file = datasets[0] if datasets else None\n",
    "\n",
    "if dataset_file:\n",
    "    print(f\"\\n‚úÖ Using dataset: {os.path.basename(dataset_file)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No dataset found! You may need to generate data first.\")\n",
    "    print(\"See: third_party/datagen/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Configure Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Configuration\nCONFIG = {\n    'model': 'google/gemma-3-4b-it',\n    'dataset': dataset_file,\n    'layer_start': 4,  # Start capturing from layer 4\n    'layer_end': 28,   # End at layer 28 (inclusive)\n    'probe_type': 'linear',  # 'linear' or 'multihead'\n    'batch_size': 16,  # Reduced for small datasets (~700 examples per action)\n    'epochs': 50,  # Increased with early stopping\n    'learning_rate': 0.0005,  # 5e-4, optimized for small datasets\n    'weight_decay': 0.001,  # 1e-3, stronger regularization for small data\n    'early_stopping_patience': 10,  # Stop if no improvement for 10 epochs\n    'use_scheduler': True,  # Cosine annealing LR scheduler\n    'device': 'auto',\n    'max_examples': None,  # None = use all examples, or set a number for quick test\n    'batch_save': True,  # Use memory-efficient batch saving\n    'batch_save_size': 1000,  # Batch size for saving activations\n}\n\n# Generate layer list\nCONFIG['layers_to_capture'] = list(range(CONFIG['layer_start'], CONFIG['layer_end'] + 1))\nnum_layers = len(CONFIG['layers_to_capture'])\ntotal_probes = num_layers * 45\n\nprint(\"=\"*60)\nprint(\"PIPELINE CONFIGURATION\")\nprint(\"=\"*60)\nfor key, value in CONFIG.items():\n    if key != 'layers_to_capture':  # Don't print the full list\n        print(f\"  {key:20s}: {value}\")\nprint(f\"  {'layers_to_capture':20s}: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({num_layers} layers)\")\nprint(f\"  {'total_probes':20s}: {total_probes} (45 per layer)\")\nprint(\"=\"*60)\nprint(\"\\nOptimized for small datasets (~700 examples per cognitive action):\")\nprint(\"  ‚Ä¢ Smaller batch size (16) for more gradient updates\")\nprint(\"  ‚Ä¢ More epochs (50) with early stopping (patience=10)\")\nprint(\"  ‚Ä¢ Lower learning rate (5e-4) with cosine annealing scheduler\")\nprint(\"  ‚Ä¢ Stronger regularization (weight_decay=1e-3)\")\nprint(\"=\"*60)\n\n# For quick testing (uncomment to test with smaller dataset and fewer layers)\n# CONFIG['max_examples'] = 1000\n# CONFIG['epochs'] = 20\n# CONFIG['layer_start'] = 20\n# CONFIG['layer_end'] = 22\n# CONFIG['layers_to_capture'] = list(range(CONFIG['layer_start'], CONFIG['layer_end'] + 1))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6Ô∏è‚É£ Step 1: Capture Activations from Layers 4-28 (~3-4 hours)\n\nThis extracts hidden states from Gemma 3 4B at 25 layers (4-28) using **memory-efficient batch saving**.\n\n**‚è∞ Expected time:** ~3-4 hours for full dataset (31.5k examples √ó 25 layers)\n\n**üíæ Memory:** ~12-16 GB VRAM peak usage (batch saving keeps memory low)\n\n**üí° Note:** Activations are cached per layer. If interrupted, you can resume from where it stopped."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import time\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"STEP 1: CAPTURING ACTIVATIONS FROM LAYERS 4-28 (BATCH SAVING)\")\nprint(\"=\"*60)\nprint(f\"Model: {CONFIG['model']}\")\nprint(f\"Layers: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({len(CONFIG['layers_to_capture'])} layers)\")\nprint(f\"Dataset: {os.path.basename(CONFIG['dataset'])}\")\nprint(f\"Batch saving: {CONFIG['batch_save']}\")\nprint(f\"Batch size: {CONFIG['batch_save_size']}\")\nprint(\"\\n‚è∞ This will take 3-4 hours. Progress will be displayed below.\")\nprint(\"üí° Each layer is saved separately. Restarting resumes from last completed layer!\\n\")\n\nstart_time = time.time()\n\n# Build command\ncmd = [\n    'python', 'src/probes/capture_activations.py',\n    '--dataset', CONFIG['dataset'],\n    '--output-dir', 'data/activations',\n    '--model', CONFIG['model'],\n    '--layers', *[str(l) for l in CONFIG['layers_to_capture']],\n    '--device', CONFIG['device'],\n    '--format', 'hdf5'\n]\n\n# Add batch saving flags\nif CONFIG['batch_save']:\n    cmd.extend(['--batch-save', '--batch-size', str(CONFIG['batch_save_size'])])\n\nif CONFIG['max_examples']:\n    cmd.extend(['--max-examples', str(CONFIG['max_examples'])])\n\n# Run capture\n!{' '.join(cmd)}\n\nelapsed = time.time() - start_time\nprint(f\"\\n‚úÖ Activation capture completed in {elapsed/3600:.2f} hours ({elapsed/60:.1f} minutes)\")\n\n# Copy to Google Drive for backup\nprint(\"\\nüì• Backing up activations to Google Drive...\")\n!cp -r data/activations/* {drive_output_dir}/activations/\nprint(\"‚úÖ Backup complete!\")\n\n# Show captured layers\nimport glob\nactivation_files = glob.glob('data/activations/layer_*_activations.h5')\nprint(f\"\\nüìä Captured {len(activation_files)} layer files:\")\nfor f in sorted(activation_files)[:5]:\n    print(f\"  ‚Ä¢ {os.path.basename(f)}\")\nif len(activation_files) > 5:\n    print(f\"  ... and {len(activation_files) - 5} more\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7Ô∏è‚É£ Step 2: Train Binary Probes for All Layers (~8-12 hours)\n\nTrain 45 binary probes per layer (1,125 total probes) using one-vs-rest strategy.\n\n**‚è∞ Expected time:** 8-12 hours for all layers\n- ~20-30 minutes per layer\n- 25 layers total\n\n**üéØ Expected performance:** AUC-ROC 0.85-0.95 per probe (varies by layer)\n\n**üí° Note:** Training happens sequentially per layer. Progress is saved after each layer completes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*60)\nprint(\"STEP 2: TRAINING 45 BINARY PROBES PER LAYER\")\nprint(\"=\"*60)\nprint(f\"Layers: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({len(CONFIG['layers_to_capture'])} layers)\")\nprint(f\"Probes per layer: 45\")\nprint(f\"Total probes: {len(CONFIG['layers_to_capture']) * 45}\")\nprint(f\"Probe type: {CONFIG['probe_type']}\")\nprint(f\"Epochs per probe: {CONFIG['epochs']} (max, with early stopping)\")\nprint(f\"Batch size: {CONFIG['batch_size']}\")\nprint(f\"Learning rate: {CONFIG['learning_rate']}\")\nprint(f\"Weight decay: {CONFIG['weight_decay']}\")\nprint(f\"Early stopping patience: {CONFIG['early_stopping_patience']}\")\nprint(\"\\n‚è∞ This will take 8-12 hours. Grab some coffee ‚òï\")\nprint(\"üí° Each layer's probes are saved separately. Early stopping will reduce actual training time!\\n\")\n\noverall_start = time.time()\nlayer_results = []\n\nfor layer_idx in CONFIG['layers_to_capture']:\n    layer_start = time.time()\n    \n    print(f\"\\n{'='*70}\")\n    print(f\"Training Layer {layer_idx} ({CONFIG['layers_to_capture'].index(layer_idx) + 1}/{len(CONFIG['layers_to_capture'])})\")\n    print(f\"{'='*70}\")\n    \n    # Build command\n    activation_file = f\"data/activations/layer_{layer_idx}_activations.h5\"\n    output_dir = f\"data/probes_binary/layer_{layer_idx}\"\n    \n    # Check if activations exist\n    if not os.path.exists(activation_file):\n        print(f\"‚ö†Ô∏è  Activation file not found: {activation_file}\")\n        print(f\"   Skipping layer {layer_idx}\")\n        continue\n    \n    cmd = [\n        'python', 'src/probes/train_binary_probes.py',\n        '--activations', activation_file,\n        '--output-dir', output_dir,\n        '--model-type', CONFIG['probe_type'],\n        '--batch-size', str(CONFIG['batch_size']),\n        '--epochs', str(CONFIG['epochs']),\n        '--lr', str(CONFIG['learning_rate']),\n        '--weight-decay', str(CONFIG['weight_decay']),\n        '--early-stopping-patience', str(CONFIG['early_stopping_patience']),\n        '--device', CONFIG['device']\n    ]\n    \n    # Add scheduler flag if disabled\n    if not CONFIG.get('use_scheduler', True):\n        cmd.append('--no-scheduler')\n    \n    # Run training\n    !{' '.join(cmd)}\n    \n    layer_elapsed = time.time() - layer_start\n    \n    # Load metrics for this layer\n    metrics_file = f\"{output_dir}/aggregate_metrics.json\"\n    if os.path.exists(metrics_file):\n        with open(metrics_file, 'r') as f:\n            metrics = json.load(f)\n        \n        layer_results.append({\n            'layer': layer_idx,\n            'avg_auc': metrics['average_auc_roc'],\n            'avg_f1': metrics['average_f1'],\n            'avg_accuracy': metrics['average_accuracy'],\n            'time_minutes': layer_elapsed / 60\n        })\n        \n        print(f\"\\n‚úÖ Layer {layer_idx} complete in {layer_elapsed/60:.1f} minutes\")\n        print(f\"   Avg AUC: {metrics['average_auc_roc']:.4f}, Avg F1: {metrics['average_f1']:.4f}\")\n    \n    # Backup to Google Drive after each layer\n    !cp -r {output_dir} {drive_output_dir}/probes_binary/\n\noverall_elapsed = time.time() - overall_start\nprint(f\"\\n{'='*70}\")\nprint(f\"‚úÖ ALL LAYERS COMPLETE!\")\nprint(f\"{'='*70}\")\nprint(f\"Total time: {overall_elapsed/3600:.2f} hours ({overall_elapsed/60:.1f} minutes)\")\nprint(f\"Trained {len(layer_results) * 45} probes across {len(layer_results)} layers\")\nprint(f\"\\nOutputs backed up to Google Drive: {drive_output_dir}/probes_binary/\")\n\n# Save layer summary\nsummary = {\n    'total_layers': len(layer_results),\n    'total_probes': len(layer_results) * 45,\n    'total_time_hours': overall_elapsed / 3600,\n    'layer_results': layer_results,\n    'config': {\n        'batch_size': CONFIG['batch_size'],\n        'epochs': CONFIG['epochs'],\n        'learning_rate': CONFIG['learning_rate'],\n        'weight_decay': CONFIG['weight_decay'],\n        'early_stopping_patience': CONFIG['early_stopping_patience'],\n        'use_scheduler': CONFIG.get('use_scheduler', True)\n    }\n}\n\nwith open('data/probes_binary/training_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\n\nprint(f\"\\nSummary saved to: data/probes_binary/training_summary.json\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8Ô∏è‚É£ View Training Results - Performance Across All Layers\n\nCompare binary probe performance across different layers"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "## 8Ô∏è‚É£ View Training Results - Overall Performance Across Layers\n\nCompare overall binary probe performance across different layers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 9Ô∏è‚É£ Step 3: Test Multi-Probe Inference\n\nRun all 45 binary probes from the **best performing layer** to detect cognitive actions."
  },
  {
   "cell_type": "markdown",
   "source": "## 8Ô∏è‚É£.5Ô∏è‚É£ Per-Action Layer Analysis - Find Best Layer for Each Cognitive Action\n\nAnalyze which layer performs best for EACH of the 45 cognitive actions",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import json\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\n\nprint(\"=\"*70)\nprint(\"PER-ACTION LAYER ANALYSIS\")\nprint(\"=\"*70)\nprint(\"\\nAnalyzing which layer is best for each cognitive action...\\n\")\n\n# Collect per-action metrics across all layers\naction_layer_performance = defaultdict(dict)  # {action_name: {layer: auc}}\n\nfor layer_idx in CONFIG['layers_to_capture']:\n    metrics_file = f'data/probes_binary/layer_{layer_idx}/aggregate_metrics.json'\n    if os.path.exists(metrics_file):\n        with open(metrics_file, 'r') as f:\n            metrics = json.load(f)\n        \n        # Extract per-action metrics\n        for action_metrics in metrics['per_action_metrics']:\n            action_name = action_metrics['action']\n            auc = action_metrics['auc_roc']\n            f1 = action_metrics['f1']\n            \n            action_layer_performance[action_name][layer_idx] = {\n                'auc': auc,\n                'f1': f1\n            }\n\n# Find best layer for each action\naction_best_layers = []\nfor action_name, layer_perfs in sorted(action_layer_performance.items()):\n    # Find layer with highest AUC\n    best_layer = max(layer_perfs.items(), key=lambda x: x[1]['auc'])\n    layer_idx, perf = best_layer\n    \n    # Get performance range\n    auc_scores = [p['auc'] for p in layer_perfs.values()]\n    auc_range = max(auc_scores) - min(auc_scores)\n    \n    action_best_layers.append({\n        'action': action_name,\n        'best_layer': layer_idx,\n        'best_auc': perf['auc'],\n        'best_f1': perf['f1'],\n        'auc_range': auc_range,\n        'worst_auc': min(auc_scores),\n        'layer_sensitivity': auc_range  # How much performance varies by layer\n    })\n\n# Convert to DataFrame for easier analysis\ndf = pd.DataFrame(action_best_layers)\n\nprint(f\"‚úÖ Analyzed {len(action_best_layers)} cognitive actions across {len(CONFIG['layers_to_capture'])} layers\\n\")\n\n# Show distribution of best layers\nprint(\"=\"*70)\nprint(\"BEST LAYER DISTRIBUTION\")\nprint(\"=\"*70)\nlayer_counts = df['best_layer'].value_counts().sort_index()\nprint(\"\\nHow many actions are best detected at each layer:\\n\")\nfor layer, count in layer_counts.items():\n    bar = \"‚ñà\" * count\n    print(f\"  Layer {layer:2d}: {count:2d} actions {bar}\")\n\n# Most common best layers\ntop_layers = layer_counts.head(5)\nprint(f\"\\nüèÜ Most effective layers:\")\nfor layer, count in top_layers.items():\n    pct = (count / len(action_best_layers)) * 100\n    print(f\"   Layer {layer}: {count} actions ({pct:.1f}%)\")\n\n# Show actions grouped by best layer\nprint(\"\\n\" + \"=\"*70)\nprint(\"ACTIONS GROUPED BY BEST LAYER\")\nprint(\"=\"*70)\nfor layer in sorted(df['best_layer'].unique())[:10]:  # Show first 10 layers\n    actions = df[df['best_layer'] == layer]['action'].tolist()\n    if actions:\n        print(f\"\\nLayer {layer} ({len(actions)} actions):\")\n        for action in actions[:5]:  # Show first 5 actions per layer\n            auc = df[df['action'] == action]['best_auc'].values[0]\n            print(f\"  ‚Ä¢ {action:30s} (AUC: {auc:.4f})\")\n        if len(actions) > 5:\n            print(f\"  ... and {len(actions) - 5} more\")\n\n# Show most layer-sensitive actions (vary a lot by layer)\nprint(\"\\n\" + \"=\"*70)\nprint(\"LAYER-SENSITIVE ACTIONS\")\nprint(\"=\"*70)\nprint(\"\\nActions where layer choice matters most:\\n\")\ndf_sorted = df.sort_values('layer_sensitivity', ascending=False)\nprint(f\"{'Action':<30} {'Best Layer':<12} {'Best AUC':<10} {'AUC Range':<10}\")\nprint(\"-\" * 70)\nfor _, row in df_sorted.head(10).iterrows():\n    print(f\"{row['action']:<30} Layer {row['best_layer']:<6} {row['best_auc']:.4f}     {row['auc_range']:.4f}\")\n\nprint(\"\\n‚Üí Large AUC range = layer choice is critical for this action\")\n\n# Show layer-robust actions (work well across all layers)\nprint(\"\\n\" + \"=\"*70)\nprint(\"LAYER-ROBUST ACTIONS\")\nprint(\"=\"*70)\nprint(\"\\nActions that work well across all layers:\\n\")\nprint(f\"{'Action':<30} {'Best Layer':<12} {'Best AUC':<10} {'AUC Range':<10}\")\nprint(\"-\" * 70)\nfor _, row in df_sorted.tail(10).iterrows():\n    print(f\"{row['action']:<30} Layer {row['best_layer']:<6} {row['best_auc']:.4f}     {row['auc_range']:.4f}\")\n\nprint(\"\\n‚Üí Small AUC range = works well regardless of layer\")\n\n# Save detailed results\nresults = {\n    'summary': {\n        'total_actions': len(action_best_layers),\n        'total_layers_tested': len(CONFIG['layers_to_capture']),\n        'most_common_best_layer': int(layer_counts.idxmax()),\n        'layer_distribution': {int(k): int(v) for k, v in layer_counts.items()}\n    },\n    'per_action_best_layers': action_best_layers\n}\n\nwith open('data/probes_binary/per_action_layer_analysis.json', 'w') as f:\n    json.dump(results, f, indent=2)\n\nprint(f\"\\n‚úÖ Detailed results saved to: data/probes_binary/per_action_layer_analysis.json\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 8Ô∏è‚É£.6Ô∏è‚É£ Visualize Per-Action Layer Performance\n\nHeatmap showing which layers are best for each cognitive action",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Create heatmap data: actions √ó layers\nactions = sorted(action_layer_performance.keys())\nlayers = sorted(CONFIG['layers_to_capture'])\n\n# Build matrix\nheatmap_data = []\nfor action in actions:\n    row = []\n    for layer in layers:\n        if layer in action_layer_performance[action]:\n            row.append(action_layer_performance[action][layer]['auc'])\n        else:\n            row.append(np.nan)\n    heatmap_data.append(row)\n\nheatmap_data = np.array(heatmap_data)\n\n# Create figure with multiple subplots\nfig, axes = plt.subplots(2, 2, figsize=(20, 16))\n\n# Plot 1: Full heatmap (all actions, all layers)\nax1 = axes[0, 0]\nim1 = ax1.imshow(heatmap_data, aspect='auto', cmap='RdYlGn', vmin=0.5, vmax=1.0)\nax1.set_xticks(range(0, len(layers), 2))\nax1.set_xticklabels([layers[i] for i in range(0, len(layers), 2)], fontsize=8)\nax1.set_yticks(range(len(actions)))\nax1.set_yticklabels(actions, fontsize=6)\nax1.set_xlabel('Layer', fontsize=10)\nax1.set_ylabel('Cognitive Action', fontsize=10)\nax1.set_title('Per-Action AUC-ROC Across All Layers', fontsize=12, fontweight='bold')\nplt.colorbar(im1, ax=ax1, label='AUC-ROC')\n\n# Mark best layer for each action with a star\nfor i, action in enumerate(actions):\n    best_layer_idx = df[df['action'] == action]['best_layer'].values[0]\n    best_layer_pos = layers.index(best_layer_idx)\n    ax1.plot(best_layer_pos, i, 'w*', markersize=4)\n\n# Plot 2: Best layer distribution\nax2 = axes[0, 1]\nlayer_counts = df['best_layer'].value_counts().sort_index()\nax2.bar(layer_counts.index, layer_counts.values, color='steelblue', alpha=0.7)\nax2.set_xlabel('Layer', fontsize=10)\nax2.set_ylabel('Number of Actions', fontsize=10)\nax2.set_title('Distribution of Best Layers\\n(How many actions peak at each layer)', fontsize=12, fontweight='bold')\nax2.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor layer, count in layer_counts.items():\n    ax2.text(layer, count + 0.2, str(count), ha='center', fontsize=8)\n\n# Plot 3: Layer sensitivity (how much does layer matter?)\nax3 = axes[1, 0]\ndf_sorted_sens = df.sort_values('layer_sensitivity', ascending=False)\ny_pos = np.arange(len(df_sorted_sens.head(20)))\nax3.barh(y_pos, df_sorted_sens.head(20)['layer_sensitivity'], color='coral', alpha=0.7)\nax3.set_yticks(y_pos)\nax3.set_yticklabels(df_sorted_sens.head(20)['action'], fontsize=8)\nax3.set_xlabel('AUC Range (Best - Worst)', fontsize=10)\nax3.set_title('Top 20 Layer-Sensitive Actions\\n(Layer choice matters most)', fontsize=12, fontweight='bold')\nax3.grid(True, alpha=0.3, axis='x')\nax3.invert_yaxis()\n\n# Plot 4: Average performance by layer group\nax4 = axes[1, 1]\nn = len(layers)\nearly_layers = layers[:n//3]\nmiddle_layers = layers[n//3:2*n//3]\nlate_layers = layers[2*n//3:]\n\ngroups = ['Early\\n(4-12)', 'Middle\\n(13-20)', 'Late\\n(21-28)']\nlayer_groups = [early_layers, middle_layers, late_layers]\n\n# Calculate average AUC for each group\ngroup_aucs = []\nfor layer_group in layer_groups:\n    aucs = []\n    for layer in layer_group:\n        if layer in all_layer_metrics_dict:  # Need to create this\n            aucs.append(all_layer_metrics_dict[layer]['avg_auc'])\n    group_aucs.append(np.mean(aucs) if aucs else 0)\n\n# Create dict for easy lookup\nall_layer_metrics_dict = {m['layer']: m for m in all_layer_metrics}\ngroup_aucs = []\nfor layer_group in layer_groups:\n    aucs = [all_layer_metrics_dict[layer]['avg_auc'] for layer in layer_group if layer in all_layer_metrics_dict]\n    group_aucs.append(np.mean(aucs) if aucs else 0)\n\nbars = ax4.bar(groups, group_aucs, color=['lightblue', 'lightgreen', 'lightsalmon'], alpha=0.7, edgecolor='black')\nax4.set_ylabel('Average AUC-ROC', fontsize=10)\nax4.set_title('Performance by Layer Group\\n(Early vs Middle vs Late)', fontsize=12, fontweight='bold')\nax4.set_ylim([min(group_aucs) - 0.02, max(group_aucs) + 0.02])\nax4.grid(True, alpha=0.3, axis='y')\n\n# Add value labels on bars\nfor bar, auc in zip(bars, group_aucs):\n    height = bar.get_height()\n    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n             f'{auc:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n\nplt.tight_layout()\nplt.savefig('data/probes_binary/per_action_layer_analysis.png', dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"‚úÖ Per-action layer analysis saved to: data/probes_binary/per_action_layer_analysis.png\")\n\n# Additional insight: Show examples of actions best at different layer stages\nprint(\"\\n\" + \"=\"*70)\nprint(\"LAYER STAGE EXAMPLES\")\nprint(\"=\"*70)\n\nearly_actions = df[df['best_layer'].isin(early_layers)].nlargest(5, 'best_auc')\nmiddle_actions = df[df['best_layer'].isin(middle_layers)].nlargest(5, 'best_auc')\nlate_actions = df[df['best_layer'].isin(late_layers)].nlargest(5, 'best_auc')\n\nprint(f\"\\nüìò Best actions detected in EARLY layers ({min(early_layers)}-{max(early_layers)}):\")\nfor _, row in early_actions.iterrows():\n    print(f\"   ‚Ä¢ {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n\nprint(f\"\\nüìó Best actions detected in MIDDLE layers ({min(middle_layers)}-{max(middle_layers)}):\")\nfor _, row in middle_actions.iterrows():\n    print(f\"   ‚Ä¢ {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n\nprint(f\"\\nüìï Best actions detected in LATE layers ({min(late_layers)}-{max(late_layers)}):\")\nfor _, row in late_actions.iterrows():\n    print(f\"   ‚Ä¢ {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n\nprint(\"\\n‚Üí This reveals which cognitive processes are captured at different network depths!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Determine best layer for inference\nif all_layer_metrics:\n    best_layer_idx = max(all_layer_metrics, key=lambda x: x['avg_auc'])['layer']\nelse:\n    best_layer_idx = CONFIG['layer_end']  # Default to last layer\n\nprint(f\"Using probes from Layer {best_layer_idx} (best performing layer)\\n\")\n\n# Test on sample texts\ntest_texts = [\n    \"After receiving feedback, she began reconsidering her initial approach to the problem.\",\n    \"He was analyzing the data to find patterns and correlations between variables.\",\n    \"They started generating creative ideas for solving the design challenge.\",\n    \"She was evaluating different strategies to determine the most effective one.\",\n    \"He tried to recall the specific details from the previous meeting.\"\n]\n\nprint(\"=\"*60)\nprint(\"MULTI-PROBE INFERENCE EXAMPLES\")\nprint(\"=\"*60)\nprint(f\"\\nRunning all 45 binary probes from Layer {best_layer_idx}...\\n\")\n\nfor i, text in enumerate(test_texts, 1):\n    print(f\"\\n{'='*60}\")\n    print(f\"Example {i}\")\n    print(f\"{'='*60}\")\n    print(f\"üìù Text: {text}\\n\")\n    \n    cmd = [\n        'python', 'src/probes/multi_probe_inference.py',\n        '--probes-dir', f'data/probes_binary/layer_{best_layer_idx}',\n        '--model', CONFIG['model'],\n        '--layer', str(best_layer_idx),\n        '--text', f'\"{text}\"',\n        '--top-k', '5',\n        '--threshold', '0.1'\n    ]\n    \n    !{' '.join(cmd)}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## üîü Visualize Performance Across Layers"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import matplotlib.pyplot as plt\nimport json\nimport numpy as np\n\n# Plot performance across layers\nif all_layer_metrics:\n    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n    \n    layers = [m['layer'] for m in all_layer_metrics]\n    auc_scores = [m['avg_auc'] for m in all_layer_metrics]\n    f1_scores = [m['avg_f1'] for m in all_layer_metrics]\n    acc_scores = [m['avg_accuracy'] for m in all_layer_metrics]\n    \n    # Plot 1: AUC-ROC across layers\n    axes[0].plot(layers, auc_scores, 'b-o', linewidth=2, markersize=6, label='AUC-ROC')\n    axes[0].axhline(y=np.mean(auc_scores), color='r', linestyle='--', alpha=0.5, label='Mean')\n    axes[0].set_xlabel('Layer', fontsize=12)\n    axes[0].set_ylabel('Average AUC-ROC', fontsize=12)\n    axes[0].set_title('Binary Probe Performance Across Layers (AUC-ROC)', fontsize=14, fontweight='bold')\n    axes[0].grid(True, alpha=0.3)\n    axes[0].legend()\n    axes[0].set_ylim([min(auc_scores) - 0.02, max(auc_scores) + 0.02])\n    \n    # Mark best layer\n    best_idx = np.argmax(auc_scores)\n    axes[0].annotate(f'Best: {layers[best_idx]}\\n{auc_scores[best_idx]:.4f}',\n                     xy=(layers[best_idx], auc_scores[best_idx]),\n                     xytext=(10, 10), textcoords='offset points',\n                     bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n    \n    # Plot 2: All metrics comparison\n    axes[1].plot(layers, auc_scores, 'b-o', label='AUC-ROC', linewidth=2, markersize=5)\n    axes[1].plot(layers, f1_scores, 'g-s', label='F1 Score', linewidth=2, markersize=5)\n    axes[1].plot(layers, acc_scores, 'r-^', label='Accuracy', linewidth=2, markersize=5)\n    axes[1].set_xlabel('Layer', fontsize=12)\n    axes[1].set_ylabel('Score', fontsize=12)\n    axes[1].set_title('All Metrics Across Layers', fontsize=14, fontweight='bold')\n    axes[1].grid(True, alpha=0.3)\n    axes[1].legend()\n    axes[1].set_ylim([0.5, 1.0])\n    \n    plt.tight_layout()\n    plt.savefig('data/probes_binary/layer_comparison.png', dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(\"‚úÖ Layer comparison plot saved to: data/probes_binary/layer_comparison.png\")\n    \n    # Additional analysis: Early vs Middle vs Late layers\n    n = len(layers)\n    early = all_layer_metrics[:n//3]\n    middle = all_layer_metrics[n//3:2*n//3]\n    late = all_layer_metrics[2*n//3:]\n    \n    print(\"\\nüìä Layer Group Analysis:\")\n    print(f\"   Early layers ({layers[0]}-{layers[n//3-1]}):  AUC = {np.mean([m['avg_auc'] for m in early]):.4f}\")\n    print(f\"   Middle layers ({layers[n//3]}-{layers[2*n//3-1]}): AUC = {np.mean([m['avg_auc'] for m in middle]):.4f}\")\n    print(f\"   Late layers ({layers[2*n//3]}-{layers[-1]}):   AUC = {np.mean([m['avg_auc'] for m in late]):.4f}\")\n    \nelse:\n    print(\"‚ö†Ô∏è  No metrics available for plotting\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1Ô∏è‚É£1Ô∏è‚É£ Download Trained Binary Probes\n\nDownload all 45 trained binary probes and metrics for local use."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from google.colab import files\nimport shutil\n\n# Create a zip file with all outputs\noutput_zip = 'brije_all_layers_binary_probes.zip'\n\nprint(\"üì¶ Creating download package...\")\nprint(\"   This may take a few minutes for 1,125 probe files...\")\n!cd data && zip -r ../{output_zip} probes_binary/ -q\n\nprint(f\"\\n‚úÖ Package created: {output_zip}\")\nprint(f\"Size: {os.path.getsize(output_zip) / 1e6:.2f} MB\")\n\n# Option to download best layer only\nprint(\"\\nüí° TIP: Download options:\")\nprint(\"   1. Full package (all layers) - see below\")\nprint(f\"   2. Best layer only - smaller download\")\n\n# Create best layer only zip\nbest_layer_zip = f'brije_layer_{best_layer_idx}_probes.zip'\n!cd data/probes_binary && zip -r ../../{best_layer_zip} layer_{best_layer_idx}/ -q\n\nprint(f\"\\nBest layer package: {best_layer_zip} ({os.path.getsize(best_layer_zip) / 1e6:.2f} MB)\")\n\n# Download best layer by default (faster)\nprint(\"\\nüì• Downloading best layer probes...\")\nfiles.download(best_layer_zip)\n\nprint(\"\\n‚úÖ Download complete!\")\nprint(\"\\nPackage contains:\")\nprint(f\"  ‚Ä¢ Layer {best_layer_idx} probes (45 binary probes)\")\nprint(\"  ‚Ä¢ Per-action metrics (metrics_*.json)\")\nprint(\"  ‚Ä¢ Aggregate performance summary\")\nprint(\"\\nTo download ALL layers (all 1,125 probes):\")\nprint(f\"  Uncomment the line below and run again:\")\nprint(f\"  # files.download('{output_zip}')\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=\"*60)\nprint(\"üéâ PIPELINE COMPLETE!\")\nprint(\"=\"*60)\nprint(\"\\n‚úÖ What was accomplished:\")\nprint(\"  1. Captured activations from Gemma 3 4B (layers 4-28)\")\nprint(\"  2. Trained 1,125 binary probes (45 per layer √ó 25 layers)\")\nprint(\"  3. Evaluated performance across all layers\")\nprint(\"  4. Identified best performing layer\")\nprint(\"  5. Saved all outputs to Google Drive\")\nprint(\"\\nüìÇ Outputs saved to:\")\nprint(f\"  ‚Ä¢ Local: {os.getcwd()}/data/\")\nprint(f\"  ‚Ä¢ Google Drive: {drive_output_dir}\")\n\nif all_layer_metrics:\n    best = max(all_layer_metrics, key=lambda x: x['avg_auc'])\n    print(f\"\\nüèÜ Best Layer: {best['layer']} (AUC: {best['avg_auc']:.4f})\")\n\nprint(\"\\nüöÄ Next Steps:\")\nprint(\"  1. Download trained probes (best layer or all layers)\")\nprint(\"  2. Use multi_probe_inference.py for predictions\")\nprint(\"  3. Compare performance across layers\")\nprint(\"  4. Experiment with different thresholds\")\n\nprint(\"\\nüí° Usage Example (Best Layer):\")\nprint(f\"  python src/probes/multi_probe_inference.py \\\\\")\nprint(f\"    --probes-dir data/probes_binary/layer_{best_layer_idx if all_layer_metrics else 27} \\\\\")\nprint(\"    --model google/gemma-3-4b-it \\\\\")\nprint(f\"    --layer {best_layer_idx if all_layer_metrics else 27} \\\\\")\nprint(\"    --text \\\"Your text here\\\" \\\\\")\nprint(\"    --top-k 5\")\n\nprint(\"\\nüìä Key Findings:\")\nif all_layer_metrics:\n    auc_scores = [m['avg_auc'] for m in all_layer_metrics]\n    print(f\"  ‚Ä¢ {len(all_layer_metrics)} layers trained successfully\")\n    print(f\"  ‚Ä¢ Average AUC across all layers: {np.mean(auc_scores):.4f}\")\n    print(f\"  ‚Ä¢ Performance range: {min(auc_scores):.4f} - {max(auc_scores):.4f}\")\n    print(f\"  ‚Ä¢ {sum(1 for auc in auc_scores if auc > 0.90)}/{len(auc_scores)} layers achieved AUC > 0.90\")\n\nprint(\"\\nüìö Files & Documentation:\")\nprint(\"  ‚Ä¢ training_summary.json - Overall training summary\")\nprint(\"  ‚Ä¢ layer_comparison.png - Performance visualization\")\nprint(\"  ‚Ä¢ aggregate_metrics.json (per layer) - Detailed metrics\")\nprint(\"  ‚Ä¢ README.md - Full documentation\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}