{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brije: Cognitive Action Detection - Full Pipeline (Google Colab)\n",
    "\n",
    "Complete pipeline for training **binary cognitive action probes across all layers** on Gemma 3 4B.\n",
    "\n",
    "**This notebook will:**\n",
    "1. ‚úÖ Clone the Brije repository\n",
    "2. ‚úÖ Install all dependencies\n",
    "3. ‚úÖ Capture activations from Gemma 3 4B layers 4-28 (~3-4 hours) using batch saving\n",
    "4. ‚úÖ Train 45 binary probes per layer (1,125 total probes) (~8-12 hours)\n",
    "5. ‚úÖ Compare performance across layers\n",
    "6. ‚úÖ Test with multi-probe inference\n",
    "7. ‚úÖ Download trained models to Google Drive\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with A100 GPU (40GB VRAM recommended)\n",
    "- Runtime: ~12-16 hours total (can run in stages)\n",
    "\n",
    "**Dataset:** 31,500 cognitive action examples across 45 actions\n",
    "\n",
    "**Architecture:** One-vs-rest binary classification, 45 probes √ó 25 layers = 1,125 total probes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Check GPU and Setup Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"GPU INFORMATION\")\n",
    "print(\"=\"*60)\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No GPU detected! This will be very slow on CPU.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Clone the repository\n",
    "repo_url = \"https://github.com/ChuloIva/brije.git\"\n",
    "repo_name = \"brije\"\n",
    "\n",
    "if not os.path.exists(repo_name):\n",
    "    print(\"üì• Cloning Brije repository...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"‚úÖ Repository cloned successfully!\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "    print(\"üîÑ Pulling latest changes...\")\n",
    "    !cd {repo_name} && git pull\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "print(\"üì¶ Installing dependencies...\\n\")\n",
    "!pip install -q torch transformers h5py scikit-learn tqdm matplotlib seaborn\n",
    "\n",
    "# Clone and install nnsight\n",
    "nnsight_dir = \"third_party/nnsight\"\n",
    "nnsight_repo = \"https://github.com/ndif-team/nnsight\"\n",
    "\n",
    "print(\"\\nüì¶ Setting up nnsight...\")\n",
    "if not os.path.exists(nnsight_dir) or not os.listdir(nnsight_dir):\n",
    "    print(\"   Cloning nnsight repository...\")\n",
    "    # Create third_party directory if it doesn't exist\n",
    "    os.makedirs(\"third_party\", exist_ok=True)\n",
    "    # Clone nnsight\n",
    "    !git clone {nnsight_repo} {nnsight_dir}\n",
    "    print(\"   ‚úÖ nnsight repository cloned\")\n",
    "else:\n",
    "    print(\"   ‚úÖ nnsight repository already exists\")\n",
    "\n",
    "# Install nnsight\n",
    "print(\"   Installing nnsight...\")\n",
    "!pip install -q -e {nnsight_dir}\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories in Google Drive for outputs\n",
    "drive_output_dir = '/content/drive/MyDrive/brije_outputs'\n",
    "os.makedirs(drive_output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/activations\", exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/probes_binary\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Outputs will be saved to: {drive_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create directories in Google Drive for outputs\n",
    "drive_output_dir = '/content/drive/MyDrive/brije_outputs'\n",
    "os.makedirs(drive_output_dir, exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/activations\", exist_ok=True)\n",
    "os.makedirs(f\"{drive_output_dir}/probes\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Outputs will be saved to: {drive_output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Verify Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if dataset exists\n",
    "import glob\n",
    "\n",
    "dataset_path = \"third_party/datagen/generated_data\"\n",
    "datasets = glob.glob(f\"{dataset_path}/*.jsonl\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"AVAILABLE DATASETS\")\n",
    "print(\"=\"*60)\n",
    "for ds in datasets:\n",
    "    size = os.path.getsize(ds) / 1e6\n",
    "    print(f\"  {os.path.basename(ds)} ({size:.2f} MB)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use the stratified combined dataset (31.5k examples)\n",
    "dataset_file = None\n",
    "for ds in datasets:\n",
    "    if 'stratified_combined' in ds or '31500' in ds:\n",
    "        dataset_file = ds\n",
    "        break\n",
    "\n",
    "if not dataset_file:\n",
    "    # Use any available dataset\n",
    "    dataset_file = datasets[0] if datasets else None\n",
    "\n",
    "if dataset_file:\n",
    "    print(f\"\\n‚úÖ Using dataset: {os.path.basename(dataset_file)}\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No dataset found! You may need to generate data first.\")\n",
    "    print(\"See: third_party/datagen/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Configure Pipeline Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'model': 'google/gemma-3-4b-it',\n",
    "    'dataset': dataset_file,\n",
    "    'layer_start': 4,  # Start capturing from layer 4\n",
    "    'layer_end': 28,   # End at layer 28 (inclusive)\n",
    "    'probe_type': 'linear',  # 'linear' or 'multihead'\n",
    "    'batch_size': 16,  # Reduced for small datasets (~700 examples per action)\n",
    "    'epochs': 50,  # Increased with early stopping\n",
    "    'learning_rate': 0.0005,  # 5e-4, optimized for small datasets\n",
    "    'weight_decay': 0.001,  # 1e-3, stronger regularization for small data\n",
    "    'early_stopping_patience': 10,  # Stop if no improvement for 10 epochs\n",
    "    'use_scheduler': True,  # Cosine annealing LR scheduler\n",
    "    'device': 'auto',\n",
    "    'max_examples': None,  # None = use all examples, or set a number for quick test\n",
    "    'batch_save': True,  # Use memory-efficient batch saving\n",
    "    'batch_save_size': 1000,  # Batch size for saving activations\n",
    "}\n",
    "\n",
    "# Generate layer list\n",
    "CONFIG['layers_to_capture'] = list(range(CONFIG['layer_start'], CONFIG['layer_end'] + 1))\n",
    "num_layers = len(CONFIG['layers_to_capture'])\n",
    "total_probes = num_layers * 45\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"PIPELINE CONFIGURATION\")\n",
    "print(\"=\"*60)\n",
    "for key, value in CONFIG.items():\n",
    "    if key != 'layers_to_capture':  # Don't print the full list\n",
    "        print(f\"  {key:20s}: {value}\")\n",
    "print(f\"  {'layers_to_capture':20s}: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({num_layers} layers)\")\n",
    "print(f\"  {'total_probes':20s}: {total_probes} (45 per layer)\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nOptimized for small datasets (~700 examples per cognitive action):\")\n",
    "print(\"  ‚Ä¢ Smaller batch size (16) for more gradient updates\")\n",
    "print(\"  ‚Ä¢ More epochs (50) with early stopping (patience=10)\")\n",
    "print(\"  ‚Ä¢ Lower learning rate (5e-4) with cosine annealing scheduler\")\n",
    "print(\"  ‚Ä¢ Stronger regularization (weight_decay=1e-3)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# For quick testing (uncomment to test with smaller dataset and fewer layers)\n",
    "# CONFIG['max_examples'] = 1000\n",
    "# CONFIG['epochs'] = 20\n",
    "# CONFIG['layer_start'] = 20\n",
    "# CONFIG['layer_end'] = 22\n",
    "# CONFIG['layers_to_capture'] = list(range(CONFIG['layer_start'], CONFIG['layer_end'] + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Step 1: Capture Activations from Layers 4-28 (~3-4 hours)\n",
    "\n",
    "This extracts hidden states from Gemma 3 4B at 25 layers (4-28) using **memory-efficient batch saving**.\n",
    "\n",
    "**‚è∞ Expected time:** ~3-4 hours for full dataset (31.5k examples √ó 25 layers)\n",
    "\n",
    "**üíæ Memory:** ~12-16 GB VRAM peak usage (batch saving keeps memory low)\n",
    "\n",
    "**üí° Note:** Activations are cached per layer. If interrupted, you can resume from where it stopped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 1: CAPTURING ACTIVATIONS FROM LAYERS 4-28 (BATCH SAVING)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {CONFIG['model']}\")\n",
    "print(f\"Layers: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({len(CONFIG['layers_to_capture'])} layers)\")\n",
    "print(f\"Dataset: {os.path.basename(CONFIG['dataset'])}\")\n",
    "print(f\"Batch saving: {CONFIG['batch_save']}\")\n",
    "print(f\"Batch size: {CONFIG['batch_save_size']}\")\n",
    "print(\"\\n‚è∞ This will take 3-4 hours. Progress will be displayed below.\")\n",
    "print(\"üí° Each layer is saved separately. Restarting resumes from last completed layer!\\n\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Build command\n",
    "cmd = [\n",
    "    'python', 'src/probes/capture_activations.py',\n",
    "    '--dataset', CONFIG['dataset'],\n",
    "    '--output-dir', 'data/activations',\n",
    "    '--model', CONFIG['model'],\n",
    "    '--layers', *[str(l) for l in CONFIG['layers_to_capture']],\n",
    "    '--device', CONFIG['device'],\n",
    "    '--format', 'hdf5'\n",
    "]\n",
    "\n",
    "# Add batch saving flags\n",
    "if CONFIG['batch_save']:\n",
    "    cmd.extend(['--batch-save', '--batch-size', str(CONFIG['batch_save_size'])])\n",
    "\n",
    "if CONFIG['max_examples']:\n",
    "    cmd.extend(['--max-examples', str(CONFIG['max_examples'])])\n",
    "\n",
    "# Run capture\n",
    "!{' '.join(cmd)}\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Activation capture completed in {elapsed/3600:.2f} hours ({elapsed/60:.1f} minutes)\")\n",
    "\n",
    "# Copy to Google Drive for backup\n",
    "print(\"\\nüì• Backing up activations to Google Drive...\")\n",
    "!cp -r data/activations/* {drive_output_dir}/activations/\n",
    "print(\"‚úÖ Backup complete!\")\n",
    "\n",
    "# Show captured layers\n",
    "import glob\n",
    "activation_files = glob.glob('data/activations/layer_*_activations.h5')\n",
    "print(f\"\\nüìä Captured {len(activation_files)} layer files:\")\n",
    "for f in sorted(activation_files)[:5]:\n",
    "    print(f\"  ‚Ä¢ {os.path.basename(f)}\")\n",
    "if len(activation_files) > 5:\n",
    "    print(f\"  ... and {len(activation_files) - 5} more\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Step 2: Train Binary Probes for All Layers (~8-12 hours)\n",
    "\n",
    "Train 45 binary probes per layer (1,125 total probes) using one-vs-rest strategy.\n",
    "\n",
    "**‚è∞ Expected time:** 8-12 hours for all layers\n",
    "- ~20-30 minutes per layer\n",
    "- 25 layers total\n",
    "\n",
    "**üéØ Expected performance:** AUC-ROC 0.85-0.95 per probe (varies by layer)\n",
    "\n",
    "**üí° Note:** Training happens sequentially per layer. Progress is saved after each layer completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STEP 2: TRAINING 45 BINARY PROBES PER LAYER\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Layers: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({len(CONFIG['layers_to_capture'])} layers)\")\n",
    "print(f\"Probes per layer: 45\")\n",
    "print(f\"Total probes: {len(CONFIG['layers_to_capture']) * 45}\")\n",
    "print(f\"Probe type: {CONFIG['probe_type']}\")\n",
    "print(f\"Epochs per probe: {CONFIG['epochs']} (max, with early stopping)\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Learning rate: {CONFIG['learning_rate']}\")\n",
    "print(f\"Weight decay: {CONFIG['weight_decay']}\")\n",
    "print(f\"Early stopping patience: {CONFIG['early_stopping_patience']}\")\n",
    "print(\"\\n‚è∞ This will take 8-12 hours. Grab some coffee ‚òï\")\n",
    "print(\"üí° Each layer's probes are saved separately. Early stopping will reduce actual training time!\\n\")\n",
    "\n",
    "overall_start = time.time()\n",
    "layer_results = []\n",
    "\n",
    "for layer_idx in CONFIG['layers_to_capture']:\n",
    "    layer_start = time.time()\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Training Layer {layer_idx} ({CONFIG['layers_to_capture'].index(layer_idx) + 1}/{len(CONFIG['layers_to_capture'])})\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    # Build command\n",
    "    activation_file = f\"data/activations/layer_{layer_idx}_activations.h5\"\n",
    "    output_dir = f\"data/probes_binary/layer_{layer_idx}\"\n",
    "    \n",
    "    # Check if activations exist\n",
    "    if not os.path.exists(activation_file):\n",
    "        print(f\"‚ö†Ô∏è  Activation file not found: {activation_file}\")\n",
    "        print(f\"   Skipping layer {layer_idx}\")\n",
    "        continue\n",
    "    \n",
    "    cmd = [\n",
    "        'python', 'src/probes/train_binary_probes.py',\n",
    "        '--activations', activation_file,\n",
    "        '--output-dir', output_dir,\n",
    "        '--model-type', CONFIG['probe_type'],\n",
    "        '--batch-size', str(CONFIG['batch_size']),\n",
    "        '--epochs', str(CONFIG['epochs']),\n",
    "        '--lr', str(CONFIG['learning_rate']),\n",
    "        '--weight-decay', str(CONFIG['weight_decay']),\n",
    "        '--early-stopping-patience', str(CONFIG['early_stopping_patience']),\n",
    "        '--device', CONFIG['device']\n",
    "    ]\n",
    "    \n",
    "    # Add scheduler flag if disabled\n",
    "    if not CONFIG.get('use_scheduler', True):\n",
    "        cmd.append('--no-scheduler')\n",
    "    \n",
    "    # Run training\n",
    "    !{' '.join(cmd)}\n",
    "    \n",
    "    layer_elapsed = time.time() - layer_start\n",
    "    \n",
    "    # Load metrics for this layer\n",
    "    metrics_file = f\"{output_dir}/aggregate_metrics.json\"\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        layer_results.append({\n",
    "            'layer': layer_idx,\n",
    "            'avg_auc': metrics['average_auc_roc'],\n",
    "            'avg_f1': metrics['average_f1'],\n",
    "            'avg_accuracy': metrics['average_accuracy'],\n",
    "            'time_minutes': layer_elapsed / 60\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n‚úÖ Layer {layer_idx} complete in {layer_elapsed/60:.1f} minutes\")\n",
    "        print(f\"   Avg AUC: {metrics['average_auc_roc']:.4f}, Avg F1: {metrics['average_f1']:.4f}\")\n",
    "    \n",
    "    # Backup to Google Drive after each layer\n",
    "    !cp -r {output_dir} {drive_output_dir}/probes_binary/\n",
    "\n",
    "overall_elapsed = time.time() - overall_start\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"‚úÖ ALL LAYERS COMPLETE!\")\n",
    "print(f\"{'='*70}\")\n",
    "print(f\"Total time: {overall_elapsed/3600:.2f} hours ({overall_elapsed/60:.1f} minutes)\")\n",
    "print(f\"Trained {len(layer_results) * 45} probes across {len(layer_results)} layers\")\n",
    "print(f\"\\nOutputs backed up to Google Drive: {drive_output_dir}/probes_binary/\")\n",
    "\n",
    "# Save layer summary\n",
    "summary = {\n",
    "    'total_layers': len(layer_results),\n",
    "    'total_probes': len(layer_results) * 45,\n",
    "    'total_time_hours': overall_elapsed / 3600,\n",
    "    'layer_results': layer_results,\n",
    "    'config': {\n",
    "        'batch_size': CONFIG['batch_size'],\n",
    "        'epochs': CONFIG['epochs'],\n",
    "        'learning_rate': CONFIG['learning_rate'],\n",
    "        'weight_decay': CONFIG['weight_decay'],\n",
    "        'early_stopping_patience': CONFIG['early_stopping_patience'],\n",
    "        'use_scheduler': CONFIG.get('use_scheduler', True)\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('data/probes_binary/training_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"\\nSummary saved to: data/probes_binary/training_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ View Training Results - Performance Across All Layers\n",
    "\n",
    "Compare binary probe performance across different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£ View Training Results - Overall Performance Across Layers\n",
    "\n",
    "Compare overall binary probe performance across different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9Ô∏è‚É£ Step 3: Test Multi-Probe Inference\n",
    "\n",
    "Run all 45 binary probes from the **best performing layer** to detect cognitive actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£.5Ô∏è‚É£ Per-Action Layer Analysis - Find Best Layer for Each Cognitive Action\n",
    "\n",
    "Analyze which layer performs best for EACH of the 45 cognitive actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"PER-ACTION LAYER ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nAnalyzing which layer is best for each cognitive action...\\n\")\n",
    "\n",
    "# Collect per-action metrics across all layers\n",
    "action_layer_performance = defaultdict(dict)  # {action_name: {layer: auc}}\n",
    "\n",
    "for layer_idx in CONFIG['layers_to_capture']:\n",
    "    metrics_file = f'data/probes_binary/layer_{layer_idx}/aggregate_metrics.json'\n",
    "    if os.path.exists(metrics_file):\n",
    "        with open(metrics_file, 'r') as f:\n",
    "            metrics = json.load(f)\n",
    "        \n",
    "        # Extract per-action metrics\n",
    "        for action_metrics in metrics['per_action_metrics']:\n",
    "            action_name = action_metrics['action']\n",
    "            auc = action_metrics['auc_roc']\n",
    "            f1 = action_metrics['f1']\n",
    "            \n",
    "            action_layer_performance[action_name][layer_idx] = {\n",
    "                'auc': auc,\n",
    "                'f1': f1\n",
    "            }\n",
    "\n",
    "# Find best layer for each action\n",
    "action_best_layers = []\n",
    "for action_name, layer_perfs in sorted(action_layer_performance.items()):\n",
    "    # Find layer with highest AUC\n",
    "    best_layer = max(layer_perfs.items(), key=lambda x: x[1]['auc'])\n",
    "    layer_idx, perf = best_layer\n",
    "    \n",
    "    # Get performance range\n",
    "    auc_scores = [p['auc'] for p in layer_perfs.values()]\n",
    "    auc_range = max(auc_scores) - min(auc_scores)\n",
    "    \n",
    "    action_best_layers.append({\n",
    "        'action': action_name,\n",
    "        'best_layer': layer_idx,\n",
    "        'best_auc': perf['auc'],\n",
    "        'best_f1': perf['f1'],\n",
    "        'auc_range': auc_range,\n",
    "        'worst_auc': min(auc_scores),\n",
    "        'layer_sensitivity': auc_range  # How much performance varies by layer\n",
    "    })\n",
    "\n",
    "# Convert to DataFrame for easier analysis\n",
    "df = pd.DataFrame(action_best_layers)\n",
    "\n",
    "print(f\"‚úÖ Analyzed {len(action_best_layers)} cognitive actions across {len(CONFIG['layers_to_capture'])} layers\\n\")\n",
    "\n",
    "# Show distribution of best layers\n",
    "print(\"=\"*70)\n",
    "print(\"BEST LAYER DISTRIBUTION\")\n",
    "print(\"=\"*70)\n",
    "layer_counts = df['best_layer'].value_counts().sort_index()\n",
    "print(\"\\nHow many actions are best detected at each layer:\\n\")\n",
    "for layer, count in layer_counts.items():\n",
    "    bar = \"‚ñà\" * count\n",
    "    print(f\"  Layer {layer:2d}: {count:2d} actions {bar}\")\n",
    "\n",
    "# Most common best layers\n",
    "top_layers = layer_counts.head(5)\n",
    "print(f\"\\nüèÜ Most effective layers:\")\n",
    "for layer, count in top_layers.items():\n",
    "    pct = (count / len(action_best_layers)) * 100\n",
    "    print(f\"   Layer {layer}: {count} actions ({pct:.1f}%)\")\n",
    "\n",
    "# Show actions grouped by best layer\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ACTIONS GROUPED BY BEST LAYER\")\n",
    "print(\"=\"*70)\n",
    "for layer in sorted(df['best_layer'].unique())[:10]:  # Show first 10 layers\n",
    "    actions = df[df['best_layer'] == layer]['action'].tolist()\n",
    "    if actions:\n",
    "        print(f\"\\nLayer {layer} ({len(actions)} actions):\")\n",
    "        for action in actions[:5]:  # Show first 5 actions per layer\n",
    "            auc = df[df['action'] == action]['best_auc'].values[0]\n",
    "            print(f\"  ‚Ä¢ {action:30s} (AUC: {auc:.4f})\")\n",
    "        if len(actions) > 5:\n",
    "            print(f\"  ... and {len(actions) - 5} more\")\n",
    "\n",
    "# Show most layer-sensitive actions (vary a lot by layer)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAYER-SENSITIVE ACTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nActions where layer choice matters most:\\n\")\n",
    "df_sorted = df.sort_values('layer_sensitivity', ascending=False)\n",
    "print(f\"{'Action':<30} {'Best Layer':<12} {'Best AUC':<10} {'AUC Range':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for _, row in df_sorted.head(10).iterrows():\n",
    "    print(f\"{row['action']:<30} Layer {row['best_layer']:<6} {row['best_auc']:.4f}     {row['auc_range']:.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí Large AUC range = layer choice is critical for this action\")\n",
    "\n",
    "# Show layer-robust actions (work well across all layers)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAYER-ROBUST ACTIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nActions that work well across all layers:\\n\")\n",
    "print(f\"{'Action':<30} {'Best Layer':<12} {'Best AUC':<10} {'AUC Range':<10}\")\n",
    "print(\"-\" * 70)\n",
    "for _, row in df_sorted.tail(10).iterrows():\n",
    "    print(f\"{row['action']:<30} Layer {row['best_layer']:<6} {row['best_auc']:.4f}     {row['auc_range']:.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí Small AUC range = works well regardless of layer\")\n",
    "\n",
    "# Save detailed results\n",
    "results = {\n",
    "    'summary': {\n",
    "        'total_actions': len(action_best_layers),\n",
    "        'total_layers_tested': len(CONFIG['layers_to_capture']),\n",
    "        'most_common_best_layer': int(layer_counts.idxmax()),\n",
    "        'layer_distribution': {int(k): int(v) for k, v in layer_counts.items()}\n",
    "    },\n",
    "    'per_action_best_layers': action_best_layers\n",
    "}\n",
    "\n",
    "with open('data/probes_binary/per_action_layer_analysis.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Detailed results saved to: data/probes_binary/per_action_layer_analysis.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8Ô∏è‚É£.6Ô∏è‚É£ Visualize Per-Action Layer Performance\n",
    "\n",
    "Heatmap showing which layers are best for each cognitive action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Create heatmap data: actions √ó layers\n",
    "actions = sorted(action_layer_performance.keys())\n",
    "layers = sorted(CONFIG['layers_to_capture'])\n",
    "\n",
    "# Build matrix\n",
    "heatmap_data = []\n",
    "for action in actions:\n",
    "    row = []\n",
    "    for layer in layers:\n",
    "        if layer in action_layer_performance[action]:\n",
    "            row.append(action_layer_performance[action][layer]['auc'])\n",
    "        else:\n",
    "            row.append(np.nan)\n",
    "    heatmap_data.append(row)\n",
    "\n",
    "heatmap_data = np.array(heatmap_data)\n",
    "\n",
    "# Create figure with multiple subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "\n",
    "# Plot 1: Full heatmap (all actions, all layers)\n",
    "ax1 = axes[0, 0]\n",
    "im1 = ax1.imshow(heatmap_data, aspect='auto', cmap='RdYlGn', vmin=0.5, vmax=1.0)\n",
    "ax1.set_xticks(range(0, len(layers), 2))\n",
    "ax1.set_xticklabels([layers[i] for i in range(0, len(layers), 2)], fontsize=8)\n",
    "ax1.set_yticks(range(len(actions)))\n",
    "ax1.set_yticklabels(actions, fontsize=6)\n",
    "ax1.set_xlabel('Layer', fontsize=10)\n",
    "ax1.set_ylabel('Cognitive Action', fontsize=10)\n",
    "ax1.set_title('Per-Action AUC-ROC Across All Layers', fontsize=12, fontweight='bold')\n",
    "plt.colorbar(im1, ax=ax1, label='AUC-ROC')\n",
    "\n",
    "# Mark best layer for each action with a star\n",
    "for i, action in enumerate(actions):\n",
    "    best_layer_idx = df[df['action'] == action]['best_layer'].values[0]\n",
    "    best_layer_pos = layers.index(best_layer_idx)\n",
    "    ax1.plot(best_layer_pos, i, 'w*', markersize=4)\n",
    "\n",
    "# Plot 2: Best layer distribution\n",
    "ax2 = axes[0, 1]\n",
    "layer_counts = df['best_layer'].value_counts().sort_index()\n",
    "ax2.bar(layer_counts.index, layer_counts.values, color='steelblue', alpha=0.7)\n",
    "ax2.set_xlabel('Layer', fontsize=10)\n",
    "ax2.set_ylabel('Number of Actions', fontsize=10)\n",
    "ax2.set_title('Distribution of Best Layers\\n(How many actions peak at each layer)', fontsize=12, fontweight='bold')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for layer, count in layer_counts.items():\n",
    "    ax2.text(layer, count + 0.2, str(count), ha='center', fontsize=8)\n",
    "\n",
    "# Plot 3: Layer sensitivity (how much does layer matter?)\n",
    "ax3 = axes[1, 0]\n",
    "df_sorted_sens = df.sort_values('layer_sensitivity', ascending=False)\n",
    "y_pos = np.arange(len(df_sorted_sens.head(20)))\n",
    "ax3.barh(y_pos, df_sorted_sens.head(20)['layer_sensitivity'], color='coral', alpha=0.7)\n",
    "ax3.set_yticks(y_pos)\n",
    "ax3.set_yticklabels(df_sorted_sens.head(20)['action'], fontsize=8)\n",
    "ax3.set_xlabel('AUC Range (Best - Worst)', fontsize=10)\n",
    "ax3.set_title('Top 20 Layer-Sensitive Actions\\n(Layer choice matters most)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3, axis='x')\n",
    "ax3.invert_yaxis()\n",
    "\n",
    "# Plot 4: Average performance by layer group\n",
    "ax4 = axes[1, 1]\n",
    "n = len(layers)\n",
    "early_layers = layers[:n//3]\n",
    "middle_layers = layers[n//3:2*n//3]\n",
    "late_layers = layers[2*n//3:]\n",
    "\n",
    "groups = ['Early\\n(4-12)', 'Middle\\n(13-20)', 'Late\\n(21-28)']\n",
    "layer_groups = [early_layers, middle_layers, late_layers]\n",
    "\n",
    "# Calculate average AUC for each group\n",
    "group_aucs = []\n",
    "for layer_group in layer_groups:\n",
    "    aucs = []\n",
    "    for layer in layer_group:\n",
    "        if layer in all_layer_metrics_dict:  # Need to create this\n",
    "            aucs.append(all_layer_metrics_dict[layer]['avg_auc'])\n",
    "    group_aucs.append(np.mean(aucs) if aucs else 0)\n",
    "\n",
    "# Create dict for easy lookup\n",
    "all_layer_metrics_dict = {m['layer']: m for m in all_layer_metrics}\n",
    "group_aucs = []\n",
    "for layer_group in layer_groups:\n",
    "    aucs = [all_layer_metrics_dict[layer]['avg_auc'] for layer in layer_group if layer in all_layer_metrics_dict]\n",
    "    group_aucs.append(np.mean(aucs) if aucs else 0)\n",
    "\n",
    "bars = ax4.bar(groups, group_aucs, color=['lightblue', 'lightgreen', 'lightsalmon'], alpha=0.7, edgecolor='black')\n",
    "ax4.set_ylabel('Average AUC-ROC', fontsize=10)\n",
    "ax4.set_title('Performance by Layer Group\\n(Early vs Middle vs Late)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylim([min(group_aucs) - 0.02, max(group_aucs) + 0.02])\n",
    "ax4.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, auc in zip(bars, group_aucs):\n",
    "    height = bar.get_height()\n",
    "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
    "             f'{auc:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data/probes_binary/per_action_layer_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Per-action layer analysis saved to: data/probes_binary/per_action_layer_analysis.png\")\n",
    "\n",
    "# Additional insight: Show examples of actions best at different layer stages\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"LAYER STAGE EXAMPLES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "early_actions = df[df['best_layer'].isin(early_layers)].nlargest(5, 'best_auc')\n",
    "middle_actions = df[df['best_layer'].isin(middle_layers)].nlargest(5, 'best_auc')\n",
    "late_actions = df[df['best_layer'].isin(late_layers)].nlargest(5, 'best_auc')\n",
    "\n",
    "print(f\"\\nüìò Best actions detected in EARLY layers ({min(early_layers)}-{max(early_layers)}):\")\n",
    "for _, row in early_actions.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìó Best actions detected in MIDDLE layers ({min(middle_layers)}-{max(middle_layers)}):\")\n",
    "for _, row in middle_actions.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n",
    "\n",
    "print(f\"\\nüìï Best actions detected in LATE layers ({min(late_layers)}-{max(late_layers)}):\")\n",
    "for _, row in late_actions.iterrows():\n",
    "    print(f\"   ‚Ä¢ {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n",
    "\n",
    "print(\"\\n‚Üí This reveals which cognitive processes are captured at different network depths!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine best layer for inference\n",
    "if all_layer_metrics:\n",
    "    best_layer_idx = max(all_layer_metrics, key=lambda x: x['avg_auc'])['layer']\n",
    "else:\n",
    "    best_layer_idx = CONFIG['layer_end']  # Default to last layer\n",
    "\n",
    "print(f\"Using probes from Layer {best_layer_idx} (best performing layer)\\n\")\n",
    "\n",
    "# Test on sample texts\n",
    "test_texts = [\n",
    "    \"After receiving feedback, she began reconsidering her initial approach to the problem.\",\n",
    "    \"He was analyzing the data to find patterns and correlations between variables.\",\n",
    "    \"They started generating creative ideas for solving the design challenge.\",\n",
    "    \"She was evaluating different strategies to determine the most effective one.\",\n",
    "    \"He tried to recall the specific details from the previous meeting.\"\n",
    "]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MULTI-PROBE INFERENCE EXAMPLES\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nRunning all 45 binary probes from Layer {best_layer_idx}...\\n\")\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Example {i}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"üìù Text: {text}\\n\")\n",
    "    \n",
    "    cmd = [\n",
    "        'python', 'src/probes/multi_probe_inference.py',\n",
    "        '--probes-dir', f'data/probes_binary/layer_{best_layer_idx}',\n",
    "        '--model', CONFIG['model'],\n",
    "        '--layer', str(best_layer_idx),\n",
    "        '--text', f'\"{text}\"',\n",
    "        '--top-k', '5',\n",
    "        '--threshold', '0.1'\n",
    "    ]\n",
    "    \n",
    "    !{' '.join(cmd)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîü Visualize Performance Across Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Plot performance across layers\n",
    "if all_layer_metrics:\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    layers = [m['layer'] for m in all_layer_metrics]\n",
    "    auc_scores = [m['avg_auc'] for m in all_layer_metrics]\n",
    "    f1_scores = [m['avg_f1'] for m in all_layer_metrics]\n",
    "    acc_scores = [m['avg_accuracy'] for m in all_layer_metrics]\n",
    "    \n",
    "    # Plot 1: AUC-ROC across layers\n",
    "    axes[0].plot(layers, auc_scores, 'b-o', linewidth=2, markersize=6, label='AUC-ROC')\n",
    "    axes[0].axhline(y=np.mean(auc_scores), color='r', linestyle='--', alpha=0.5, label='Mean')\n",
    "    axes[0].set_xlabel('Layer', fontsize=12)\n",
    "    axes[0].set_ylabel('Average AUC-ROC', fontsize=12)\n",
    "    axes[0].set_title('Binary Probe Performance Across Layers (AUC-ROC)', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylim([min(auc_scores) - 0.02, max(auc_scores) + 0.02])\n",
    "    \n",
    "    # Mark best layer\n",
    "    best_idx = np.argmax(auc_scores)\n",
    "    axes[0].annotate(f'Best: {layers[best_idx]}\\n{auc_scores[best_idx]:.4f}',\n",
    "                     xy=(layers[best_idx], auc_scores[best_idx]),\n",
    "                     xytext=(10, 10), textcoords='offset points',\n",
    "                     bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
    "                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Plot 2: All metrics comparison\n",
    "    axes[1].plot(layers, auc_scores, 'b-o', label='AUC-ROC', linewidth=2, markersize=5)\n",
    "    axes[1].plot(layers, f1_scores, 'g-s', label='F1 Score', linewidth=2, markersize=5)\n",
    "    axes[1].plot(layers, acc_scores, 'r-^', label='Accuracy', linewidth=2, markersize=5)\n",
    "    axes[1].set_xlabel('Layer', fontsize=12)\n",
    "    axes[1].set_ylabel('Score', fontsize=12)\n",
    "    axes[1].set_title('All Metrics Across Layers', fontsize=14, fontweight='bold')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_ylim([0.5, 1.0])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('data/probes_binary/layer_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Layer comparison plot saved to: data/probes_binary/layer_comparison.png\")\n",
    "    \n",
    "    # Additional analysis: Early vs Middle vs Late layers\n",
    "    n = len(layers)\n",
    "    early = all_layer_metrics[:n//3]\n",
    "    middle = all_layer_metrics[n//3:2*n//3]\n",
    "    late = all_layer_metrics[2*n//3:]\n",
    "    \n",
    "    print(\"\\nüìä Layer Group Analysis:\")\n",
    "    print(f\"   Early layers ({layers[0]}-{layers[n//3-1]}):  AUC = {np.mean([m['avg_auc'] for m in early]):.4f}\")\n",
    "    print(f\"   Middle layers ({layers[n//3]}-{layers[2*n//3-1]}): AUC = {np.mean([m['avg_auc'] for m in middle]):.4f}\")\n",
    "    print(f\"   Late layers ({layers[2*n//3]}-{layers[-1]}):   AUC = {np.mean([m['avg_auc'] for m in late]):.4f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No metrics available for plotting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£1Ô∏è‚É£ Download Trained Binary Probes\n",
    "\n",
    "Download all 45 trained binary probes and metrics for local use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create a zip file with all outputs\n",
    "output_zip = 'brije_all_layers_binary_probes.zip'\n",
    "\n",
    "print(\"üì¶ Creating download package...\")\n",
    "print(\"   This may take a few minutes for 1,125 probe files...\")\n",
    "!cd data && zip -r ../{output_zip} probes_binary/ -q\n",
    "\n",
    "print(f\"\\n‚úÖ Package created: {output_zip}\")\n",
    "print(f\"Size: {os.path.getsize(output_zip) / 1e6:.2f} MB\")\n",
    "\n",
    "# Option to download best layer only\n",
    "print(\"\\nüí° TIP: Download options:\")\n",
    "print(\"   1. Full package (all layers) - see below\")\n",
    "print(f\"   2. Best layer only - smaller download\")\n",
    "\n",
    "# Create best layer only zip\n",
    "best_layer_zip = f'brije_layer_{best_layer_idx}_probes.zip'\n",
    "!cd data/probes_binary && zip -r ../../{best_layer_zip} layer_{best_layer_idx}/ -q\n",
    "\n",
    "print(f\"\\nBest layer package: {best_layer_zip} ({os.path.getsize(best_layer_zip) / 1e6:.2f} MB)\")\n",
    "\n",
    "# Download best layer by default (faster)\n",
    "print(\"\\nüì• Downloading best layer probes...\")\n",
    "files.download(best_layer_zip)\n",
    "\n",
    "print(\"\\n‚úÖ Download complete!\")\n",
    "print(\"\\nPackage contains:\")\n",
    "print(f\"  ‚Ä¢ Layer {best_layer_idx} probes (45 binary probes)\")\n",
    "print(\"  ‚Ä¢ Per-action metrics (metrics_*.json)\")\n",
    "print(\"  ‚Ä¢ Aggregate performance summary\")\n",
    "print(\"\\nTo download ALL layers (all 1,125 probes):\")\n",
    "print(f\"  Uncomment the line below and run again:\")\n",
    "print(f\"  # files.download('{output_zip}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£2Ô∏è‚É£ Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üéâ PIPELINE COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚úÖ What was accomplished:\")\n",
    "print(\"  1. Captured activations from Gemma 3 4B (layers 4-28)\")\n",
    "print(\"  2. Trained 1,125 binary probes (45 per layer √ó 25 layers)\")\n",
    "print(\"  3. Evaluated performance across all layers\")\n",
    "print(\"  4. Identified best performing layer\")\n",
    "print(\"  5. Saved all outputs to Google Drive\")\n",
    "print(\"\\nüìÇ Outputs saved to:\")\n",
    "print(f\"  ‚Ä¢ Local: {os.getcwd()}/data/\")\n",
    "print(f\"  ‚Ä¢ Google Drive: {drive_output_dir}\")\n",
    "\n",
    "if all_layer_metrics:\n",
    "    best = max(all_layer_metrics, key=lambda x: x['avg_auc'])\n",
    "    print(f\"\\nüèÜ Best Layer: {best['layer']} (AUC: {best['avg_auc']:.4f})\")\n",
    "\n",
    "print(\"\\nüöÄ Next Steps:\")\n",
    "print(\"  1. Download trained probes (best layer or all layers)\")\n",
    "print(\"  2. Use multi_probe_inference.py for predictions\")\n",
    "print(\"  3. Compare performance across layers\")\n",
    "print(\"  4. Experiment with different thresholds\")\n",
    "\n",
    "print(\"\\nüí° Usage Example (Best Layer):\")\n",
    "print(f\"  python src/probes/multi_probe_inference.py \\\\\")\n",
    "print(f\"    --probes-dir data/probes_binary/layer_{best_layer_idx if all_layer_metrics else 27} \\\\\")\n",
    "print(\"    --model google/gemma-3-4b-it \\\\\")\n",
    "print(f\"    --layer {best_layer_idx if all_layer_metrics else 27} \\\\\")\n",
    "print(\"    --text \\\"Your text here\\\" \\\\\")\n",
    "print(\"    --top-k 5\")\n",
    "\n",
    "print(\"\\nüìä Key Findings:\")\n",
    "if all_layer_metrics:\n",
    "    auc_scores = [m['avg_auc'] for m in all_layer_metrics]\n",
    "    print(f\"  ‚Ä¢ {len(all_layer_metrics)} layers trained successfully\")\n",
    "    print(f\"  ‚Ä¢ Average AUC across all layers: {np.mean(auc_scores):.4f}\")\n",
    "    print(f\"  ‚Ä¢ Performance range: {min(auc_scores):.4f} - {max(auc_scores):.4f}\")\n",
    "    print(f\"  ‚Ä¢ {sum(1 for auc in auc_scores if auc > 0.90)}/{len(auc_scores)} layers achieved AUC > 0.90\")\n",
    "\n",
    "print(\"\\nüìö Files & Documentation:\")\n",
    "print(\"  ‚Ä¢ training_summary.json - Overall training summary\")\n",
    "print(\"  ‚Ä¢ layer_comparison.png - Performance visualization\")\n",
    "print(\"  ‚Ä¢ aggregate_metrics.json (per layer) - Detailed metrics\")\n",
    "print(\"  ‚Ä¢ README.md - Full documentation\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
