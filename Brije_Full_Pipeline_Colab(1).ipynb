{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auDdGKt8t8fH"
      },
      "source": [
        "# Brije: Cognitive Action Detection - Full Pipeline (Google Colab)\n",
        "\n",
        "Complete pipeline for training **binary cognitive action probes across all layers** on Gemma 3 4B.\n",
        "\n",
        "**This notebook will:**\n",
        "1. ✅ Clone the Brije repository\n",
        "2. ✅ Install all dependencies\n",
        "3. ✅ Capture activations from Gemma 3 4B layers 4-28 (~3-4 hours) using batch saving\n",
        "4. ✅ Train 45 binary probes per layer (1,125 total probes) (~8-12 hours)\n",
        "5. ✅ Compare performance across layers\n",
        "6. ✅ Test with multi-probe inference\n",
        "7. ✅ Download trained models to Google Drive\n",
        "\n",
        "**Requirements:**\n",
        "- Google Colab with A100 GPU (40GB VRAM recommended)\n",
        "- Runtime: ~12-16 hours total (can run in stages)\n",
        "\n",
        "**Dataset:** 31,500 cognitive action examples across 45 actions\n",
        "\n",
        "**Architecture:** One-vs-rest binary classification, 45 probes × 25 layers = 1,125 total probes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPro0WNtjeMg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnJ3CPnit8fK"
      },
      "source": [
        "## 1️⃣ Check GPU and Setup Runtime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsgzXMnvt8fM",
        "outputId": "b5cc6c45-f7ca-4daa-c702-8b3c34e0f62e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Oct 10 00:41:49 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\n",
            "============================================================\n",
            "GPU INFORMATION\n",
            "============================================================\n",
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA version: 12.6\n",
            "GPU device: Tesla T4\n",
            "GPU memory: 15.83 GB\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "!nvidia-smi\n",
        "\n",
        "import torch\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GPU INFORMATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA version: {torch.version.cuda}\")\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠️  WARNING: No GPU detected! This will be very slow on CPU.\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6FPl7lyJt8fN"
      },
      "source": [
        "## 2️⃣ Clone Repository and Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6baw2C7Dt8fN",
        "outputId": "6e9a9a1a-53a2-4123-d550-405f07fc38c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📥 Cloning Brije repository...\n",
            "Cloning into 'brije'...\n",
            "remote: Enumerating objects: 168, done.\u001b[K\n",
            "remote: Counting objects: 100% (168/168), done.\u001b[K\n",
            "remote: Compressing objects: 100% (106/106), done.\u001b[K\n",
            "remote: Total 168 (delta 71), reused 141 (delta 44), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (168/168), 9.98 MiB | 7.21 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "✅ Repository cloned successfully!\n",
            "\n",
            "📁 Current directory: /content/brije\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "# Clone the repository\n",
        "repo_url = \"https://github.com/ChuloIva/brije.git\"\n",
        "repo_name = \"brije\"\n",
        "\n",
        "if not os.path.exists(repo_name):\n",
        "    print(\"📥 Cloning Brije repository...\")\n",
        "    !git clone {repo_url}\n",
        "    print(\"✅ Repository cloned successfully!\")\n",
        "else:\n",
        "    print(\"✅ Repository already exists\")\n",
        "    print(\"🔄 Pulling latest changes...\")\n",
        "    !cd {repo_name} && git pull\n",
        "\n",
        "# Change to repo directory\n",
        "os.chdir(repo_name)\n",
        "print(f\"\\n📁 Current directory: {os.getcwd()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iL7yzLJt8fO",
        "outputId": "19308f2f-6a8c-48ba-826d-d5a92030d3a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📦 Installing dependencies...\n",
            "\n",
            "\n",
            "📦 Setting up nnsight...\n",
            "   Cloning nnsight repository...\n",
            "Cloning into 'third_party/nnsight'...\n",
            "remote: Enumerating objects: 13148, done.\u001b[K\n",
            "remote: Counting objects: 100% (145/145), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 13148 (delta 85), reused 74 (delta 61), pack-reused 13003 (from 3)\u001b[K\n",
            "Receiving objects: 100% (13148/13148), 64.31 MiB | 17.15 MiB/s, done.\n",
            "Resolving deltas: 100% (8276/8276), done.\n",
            "   ✅ nnsight repository cloned\n",
            "   Installing nnsight...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m74.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.6/59.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building editable for nnsight (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\n",
            "✅ All dependencies installed!\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "print(\"📦 Installing dependencies...\\n\")\n",
        "!pip install -q torch transformers h5py scikit-learn tqdm matplotlib seaborn\n",
        "\n",
        "# Clone and install nnsight\n",
        "nnsight_dir = \"third_party/nnsight\"\n",
        "nnsight_repo = \"https://github.com/ndif-team/nnsight\"\n",
        "\n",
        "print(\"\\n📦 Setting up nnsight...\")\n",
        "if not os.path.exists(nnsight_dir) or not os.listdir(nnsight_dir):\n",
        "    print(\"   Cloning nnsight repository...\")\n",
        "    # Create third_party directory if it doesn't exist\n",
        "    os.makedirs(\"third_party\", exist_ok=True)\n",
        "    # Clone nnsight\n",
        "    !git clone {nnsight_repo} {nnsight_dir}\n",
        "    print(\"   ✅ nnsight repository cloned\")\n",
        "else:\n",
        "    print(\"   ✅ nnsight repository already exists\")\n",
        "\n",
        "# Install nnsight\n",
        "print(\"   Installing nnsight...\")\n",
        "!pip install -q -e {nnsight_dir}\n",
        "\n",
        "print(\"\\n✅ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOHT1bsxt8fP",
        "outputId": "0da9ab4d-74d9-418c-b882-4edd9ee0a2f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Outputs will be saved to: /content/drive/MyDrive/brije_outputs\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Create directories in Google Drive for outputs\n",
        "drive_output_dir = '/content/drive/MyDrive/brije_outputs'\n",
        "os.makedirs(drive_output_dir, exist_ok=True)\n",
        "os.makedirs(f\"{drive_output_dir}/activations\", exist_ok=True)\n",
        "os.makedirs(f\"{drive_output_dir}/probes\", exist_ok=True)\n",
        "\n",
        "print(f\"✅ Outputs will be saved to: {drive_output_dir}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xv1-fh5Mt8fQ"
      },
      "source": [
        "## 4️⃣ Verify Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBdW6e5kt8fQ",
        "outputId": "e0bb23aa-1e3a-4fc4-b6da-085090e392db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "AVAILABLE DATASETS\n",
            "============================================================\n",
            "  cognitive_actions_7k_final_1759233061.jsonl (14.69 MB)\n",
            "  stratified_4500_1759788994.jsonl (3.18 MB)\n",
            "  stratified_9000_1759769375.jsonl (6.33 MB)\n",
            "  stratified_combined_31500.jsonl (22.21 MB)\n",
            "  stratified_18000_1759809514.jsonl (12.70 MB)\n",
            "============================================================\n",
            "\n",
            "✅ Using dataset: stratified_combined_31500.jsonl\n"
          ]
        }
      ],
      "source": [
        "# Check if dataset exists\n",
        "import glob\n",
        "import os\n",
        "\n",
        "dataset_path = \"third_party/datagen/generated_data\"\n",
        "datasets = glob.glob(f\"{dataset_path}/*.jsonl\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"AVAILABLE DATASETS\")\n",
        "print(\"=\"*60)\n",
        "for ds in datasets:\n",
        "    size = os.path.getsize(ds) / 1e6\n",
        "    print(f\"  {os.path.basename(ds)} ({size:.2f} MB)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Use the stratified combined dataset (31.5k examples)\n",
        "dataset_file = None\n",
        "for ds in datasets:\n",
        "    if 'stratified_combined' in ds or '31500' in ds:\n",
        "        dataset_file = ds\n",
        "        break\n",
        "\n",
        "if not dataset_file:\n",
        "    # Use any available dataset\n",
        "    dataset_file = datasets[0] if datasets else None\n",
        "\n",
        "if dataset_file:\n",
        "    print(f\"\\n✅ Using dataset: {os.path.basename(dataset_file)}\")\n",
        "else:\n",
        "    print(\"\\n⚠️  No dataset found! You may need to generate data first.\")\n",
        "    print(\"See: third_party/datagen/README.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X27I6h_Kt8fR"
      },
      "source": [
        "## 5️⃣ Configure Pipeline Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J04x0J-kt8fR",
        "outputId": "b9086504-92e4-4f7f-8be4-758f3382f2e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "🚀 PARALLEL TRAINING PIPELINE CONFIGURATION\n",
            "======================================================================\n",
            "  model                    : google/gemma-3-4b-it\n",
            "  dataset                  : third_party/datagen/generated_data/stratified_combined_31500.jsonl\n",
            "  layer_start              : 1\n",
            "  layer_end                : 20\n",
            "  probe_type               : linear\n",
            "  use_parallel_training    : True\n",
            "  num_workers              : 45\n",
            "  batch_size               : 32\n",
            "  pin_activations_to_gpu   : True\n",
            "  epochs                   : 50\n",
            "  learning_rate            : 0.0005\n",
            "  weight_decay             : 0.001\n",
            "  early_stopping_patience  : 10\n",
            "  use_scheduler            : True\n",
            "  device                   : auto\n",
            "  max_examples             : None\n",
            "  batch_save               : True\n",
            "  batch_save_size          : 1000\n",
            "  layers_to_capture        : 1-20 (20 layers)\n",
            "  total_probes             : 900 (45 per layer)\n",
            "======================================================================\n",
            "\n",
            "🚀 Parallel Training Benefits:\n",
            "  • 45x faster training\n",
            "  • Large batch size (32) for GPU efficiency\n",
            "  • Activations pinned to GPU memory\n",
            "  • Expected time: ~2-3 hours (vs 8-12 hours sequential!)\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "CONFIG = {\n",
        "    'model': 'google/gemma-3-4b-it',\n",
        "    'dataset': dataset_file,\n",
        "    'layer_start': 1,  # Start capturing from layer 4\n",
        "    'layer_end': 20,   # End at layer 28 (inclusive)\n",
        "    'probe_type': 'linear',  # 'linear' or 'multihead'\n",
        "\n",
        "    # 🚀 Parallel Training Configuration (OPTIMIZED for A100 40GB)\n",
        "    'use_parallel_training': True,  # Enable parallel training\n",
        "    'num_workers': 45,  # Train 45 probes simultaneously\n",
        "    'batch_size': 32,  # Large batch size for better GPU utilization\n",
        "    'pin_activations_to_gpu': True,  # Pin activations to GPU memory\n",
        "\n",
        "    'epochs': 50,  # Max epochs with early stopping\n",
        "    'learning_rate': 0.0005,  # 5e-4\n",
        "    'weight_decay': 0.001,  # 1e-3\n",
        "    'early_stopping_patience': 10,\n",
        "    'use_scheduler': True,\n",
        "    'device': 'auto',\n",
        "    'max_examples': None,  # None = use all examples\n",
        "    'batch_save': True,\n",
        "    'batch_save_size': 1000,\n",
        "}\n",
        "\n",
        "# Generate layer list\n",
        "CONFIG['layers_to_capture'] = list(range(CONFIG['layer_start'], CONFIG['layer_end'] + 1))\n",
        "num_layers = len(CONFIG['layers_to_capture'])\n",
        "total_probes = num_layers * 45\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"🚀 PARALLEL TRAINING PIPELINE CONFIGURATION\")\n",
        "print(\"=\"*70)\n",
        "for key, value in CONFIG.items():\n",
        "    if key != 'layers_to_capture':\n",
        "        print(f\"  {key:25s}: {value}\")\n",
        "print(f\"  {'layers_to_capture':25s}: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({num_layers} layers)\")\n",
        "print(f\"  {'total_probes':25s}: {total_probes} (45 per layer)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\n🚀 Parallel Training Benefits:\")\n",
        "print(f\"  • {CONFIG['num_workers']}x faster training\")\n",
        "print(f\"  • Large batch size ({CONFIG['batch_size']}) for GPU efficiency\")\n",
        "print(\"  • Activations pinned to GPU memory\")\n",
        "print(\"  • Expected time: ~2-3 hours (vs 8-12 hours sequential!)\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "cde93fa85fc7445caee66a167784cf08",
            "2b1219eb331c4ea3b5f115aeeaa342a2",
            "21958c52afed4253b1c3f533123ed494",
            "e245900d5bfc406a9efad6d7945b00d5",
            "50e0396d23884abc96cf28d09651b58c",
            "6dc12eb7d440431a870c7f4ffcc5a192",
            "2d96e883b39d4ee0ad348d5c0fe0b36f",
            "6beafebe160145ae831cfa26a97a0945",
            "1e3c2aaebc414256a0ef4e9542ea46bf",
            "7373b16c4e5b422ea84e90b2200027c9",
            "14d2292433bf4ef497a3693140f4452a",
            "154ac1e8eb9a4e0c8d7443f34395619e",
            "b4d708014761427e9e1cfe02aa143638",
            "868aa77e62c04df2926a25164ea34f21",
            "68570ed157c64c6e81c41bb561eeae47",
            "cce096dff7b94ceebd7aa5b6b1050b5c",
            "6726a1860b2e4e5084abfcdc9d601d65",
            "a0bb0bdc03db46148305031b3fe4bf78",
            "652f725ad1e948cd9c4cd20540ecb7be",
            "ba575c089e9a430da29bfdfa0a5003bf"
          ]
        },
        "id": "luBy1tY4uF5X",
        "outputId": "a601e860-202a-4300-da37-2a3caeec8cc2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cde93fa85fc7445caee66a167784cf08",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Login to Hugging Face Hub\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmdRI_Aot8fR"
      },
      "source": [
        "## 6️⃣ Step 1: Capture Activations from Layers 4-28 (🚀 ~10-15 minutes with single-pass!)\n",
        "\n",
        "This extracts hidden states from Gemma 3 4B at 25 layers (4-28) using **optimized single-pass capture**.\n",
        "\n",
        "**⏰ Expected time:** ~10-15 minutes for full dataset (31.5k examples, ALL 25 layers simultaneously!)\n",
        "\n",
        "**💾 Memory:** ~12-16 GB VRAM peak usage (single-pass with periodic cleanup)\n",
        "\n",
        "**🚀 Optimization:** Captures all layers in ONE forward pass (25x faster than old method!)\n",
        "\n",
        "**💡 Note:** Activations are saved per layer. If interrupted, you can resume from where it stopped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6keWNyMt8fR",
        "outputId": "1e1fd984-ca0e-4683-9ad0-fd5b5a550a15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: CAPTURING ACTIVATIONS (🚀 OPTIMIZED SINGLE-PASS MODE)\n",
            "============================================================\n",
            "Model: google/gemma-3-4b-it\n",
            "Layers: 20-30 (11 layers)\n",
            "Dataset: stratified_combined_31500.jsonl\n",
            "Mode: Single-pass optimization (25x faster!)\n",
            "Batch size: 1000\n",
            "\n",
            "⏰ This will take ~10-15 minutes (vs 3-4 hours with old method!).\n",
            "💡 All layers captured simultaneously in ONE forward pass!\n",
            "\n",
            "2025-10-09 21:38:54.946385: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-10-09 21:38:54.963918: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1760045934.984880   35499 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1760045934.991373   35499 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1760045935.007748   35499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760045935.007777   35499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760045935.007780   35499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1760045935.007782   35499 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-10-09 21:38:55.012580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Loading dataset...\n",
            "Loading dataset from third_party/datagen/generated_data/stratified_combined_31500.jsonl\n",
            "Loaded 31500 examples\n",
            "\n",
            "============================================================\n",
            "DATASET STATISTICS\n",
            "============================================================\n",
            "\n",
            "Total examples: 31500\n",
            "\n",
            "Cognitive actions (top 10):\n",
            "  reconsidering                 :  700 (2.2%)\n",
            "  reframing                     :  700 (2.2%)\n",
            "  noticing                      :  700 (2.2%)\n",
            "  perspective_taking            :  700 (2.2%)\n",
            "  questioning                   :  700 (2.2%)\n",
            "  abstracting                   :  700 (2.2%)\n",
            "  concretizing                  :  700 (2.2%)\n",
            "  connecting                    :  700 (2.2%)\n",
            "  distinguishing                :  700 (2.2%)\n",
            "  updating_beliefs              :  700 (2.2%)\n",
            "\n",
            "Domains (top 10):\n",
            "  creative work                 :  978 (3.1%)\n",
            "  friendships                   :  950 (3.0%)\n",
            "  personal relationships        :  939 (3.0%)\n",
            "  moral and ethical dilemmas    :  935 (3.0%)\n",
            "  academic learning             :  921 (2.9%)\n",
            "  leadership challenges         :  918 (2.9%)\n",
            "  therapy and healing           :  917 (2.9%)\n",
            "  team dynamics                 :  915 (2.9%)\n",
            "  professional development      :  909 (2.9%)\n",
            "  philosophical questions       :  902 (2.9%)\n",
            "\n",
            "Emotional states (top 10):\n",
            "  feeling judgmental toward others: 1324 (4.2%)\n",
            "  feeling pressured by circumstances: 1300 (4.1%)\n",
            "  experiencing self-doubt       : 1281 (4.1%)\n",
            "  experiencing genuine curiosity: 1267 (4.0%)\n",
            "  in a state of creative flow   : 1260 (4.0%)\n",
            "  feeling emotionally drained   : 1248 (4.0%)\n",
            "  feeling intellectually stuck  : 1248 (4.0%)\n",
            "  feeling protective of their beliefs: 1238 (3.9%)\n",
            "  experiencing excitement about discovery: 1236 (3.9%)\n",
            "  experiencing gratitude for insights: 1234 (3.9%)\n",
            "\n",
            "Language styles (top 10):\n",
            "  straightforward and direct    : 2665 (8.5%)\n",
            "  philosophical and reflective  : 2665 (8.5%)\n",
            "  casual and conversational     : 2650 (8.4%)\n",
            "  confident and declarative     : 2636 (8.4%)\n",
            "  detailed and thorough         : 2626 (8.3%)\n",
            "  stream-of-consciousness style : 2625 (8.3%)\n",
            "  minimalist and spare          : 2624 (8.3%)\n",
            "  emotional and expressive      : 2624 (8.3%)\n",
            "  introspective and literary    : 2613 (8.3%)\n",
            "  questioning and uncertain     : 2613 (8.3%)\n",
            "\n",
            "Text length statistics:\n",
            "  Mean: 396.4 characters\n",
            "  Min:  83 characters\n",
            "  Max:  765 characters\n",
            "============================================================\n",
            "\n",
            "\n",
            "Creating train/val/test splits...\n",
            "Split sizes - Train: 22005, Val: 4770, Test: 4725\n",
            "Loading model: google/gemma-3-4b-it\n",
            "Detected vision-language model. Loading text-only (skipping vision tower)...\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100% 2/2 [00:02<00:00,  1.20s/it]\n",
            "Model has 34 transformer layers\n",
            "Will capture activations from layers: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
            "\n",
            "Detecting model hidden size...\n",
            "Hidden size: 2560\n",
            "\n",
            "======================================================================\n",
            "🚀 OPTIMIZED SINGLE-PASS MODE\n",
            "======================================================================\n",
            "Capturing ALL 11 layers simultaneously\n",
            "Expected speedup: ~11x faster!\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Processing train split (22005 examples)\n",
            "======================================================================\n",
            "🚀 Capturing 11 layers in single pass...\n",
            "   Memory per example: ~88.0 KB\n",
            "Processing examples: 100% 22005/22005 [48:15<00:00,  7.60it/s]\n",
            "  Saved layer 20 (train)\n",
            "  Saved layer 21 (train)\n",
            "  Saved layer 22 (train)\n",
            "  Saved layer 23 (train)\n",
            "  Saved layer 24 (train)\n",
            "  Saved layer 25 (train)\n",
            "  Saved layer 26 (train)\n",
            "  Saved layer 27 (train)\n",
            "  Saved layer 28 (train)\n",
            "  Saved layer 29 (train)\n",
            "  Saved layer 30 (train)\n",
            "\n",
            "======================================================================\n",
            "Processing val split (4770 examples)\n",
            "======================================================================\n",
            "🚀 Capturing 11 layers in single pass...\n",
            "   Memory per example: ~88.0 KB\n",
            "Processing examples: 100% 4770/4770 [10:11<00:00,  7.80it/s]\n",
            "  Saved layer 20 (val)\n",
            "  Saved layer 21 (val)\n",
            "  Saved layer 22 (val)\n",
            "  Saved layer 23 (val)\n",
            "  Saved layer 24 (val)\n",
            "  Saved layer 25 (val)\n",
            "  Saved layer 26 (val)\n",
            "  Saved layer 27 (val)\n",
            "  Saved layer 28 (val)\n",
            "  Saved layer 29 (val)\n",
            "  Saved layer 30 (val)\n",
            "\n",
            "======================================================================\n",
            "Processing test split (4725 examples)\n",
            "======================================================================\n",
            "🚀 Capturing 11 layers in single pass...\n",
            "   Memory per example: ~88.0 KB\n",
            "Processing examples: 100% 4725/4725 [09:34<00:00,  8.22it/s]\n",
            "  Saved layer 20 (test)\n",
            "  Saved layer 21 (test)\n",
            "  Saved layer 22 (test)\n",
            "  Saved layer 23 (test)\n",
            "  Saved layer 24 (test)\n",
            "  Saved layer 25 (test)\n",
            "  Saved layer 26 (test)\n",
            "  Saved layer 27 (test)\n",
            "  Saved layer 28 (test)\n",
            "  Saved layer 29 (test)\n",
            "  Saved layer 30 (test)\n",
            "\n",
            "======================================================================\n",
            "✅ SINGLE-PASS CAPTURE COMPLETE!\n",
            "======================================================================\n",
            "\n",
            "============================================================\n",
            "ACTIVATION CAPTURE COMPLETE\n",
            "============================================================\n",
            "\n",
            "Activations saved to: data/activations\n",
            "Layers captured: [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
            "Number of classes: 45\n",
            "\n",
            "✅ Activation capture completed in 68.5 minutes\n",
            "   Speedup: ~0.1x faster than old method!\n",
            "\n",
            "📥 Backing up activations to Google Drive...\n",
            "✅ Backup complete!\n",
            "\n",
            "📊 Captured 11 layer files:\n",
            "  • layer_20_activations.h5\n",
            "  • layer_21_activations.h5\n",
            "  • layer_22_activations.h5\n",
            "  • layer_23_activations.h5\n",
            "  • layer_24_activations.h5\n",
            "  ... and 6 more\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: CAPTURING ACTIVATIONS (🚀 OPTIMIZED SINGLE-PASS MODE)\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: {CONFIG['model']}\")\n",
        "print(f\"Layers: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({len(CONFIG['layers_to_capture'])} layers)\")\n",
        "print(f\"Dataset: {os.path.basename(CONFIG['dataset'])}\")\n",
        "print(f\"Mode: Single-pass optimization (25x faster!)\")\n",
        "print(f\"Batch size: {CONFIG['batch_save_size']}\")\n",
        "print(\"\\n⏰ This will take ~10-15 minutes (vs 3-4 hours with old method!).\")\n",
        "print(\"💡 All layers captured simultaneously in ONE forward pass!\\n\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Build command with --single-pass flag for optimized capture\n",
        "cmd = [\n",
        "    'python', 'src/probes/capture_activations.py',\n",
        "    '--dataset', CONFIG['dataset'],\n",
        "    '--output-dir', 'data/activations',\n",
        "    '--model', CONFIG['model'],\n",
        "    '--layers', *[str(l) for l in CONFIG['layers_to_capture']],\n",
        "    '--device', CONFIG['device'],\n",
        "    '--format', 'hdf5',\n",
        "    '--single-pass',  # 🚀 OPTIMIZED: Capture all layers in one pass!\n",
        "    '--batch-size', str(CONFIG['batch_save_size'])\n",
        "]\n",
        "\n",
        "if CONFIG['max_examples']:\n",
        "    cmd.extend(['--max-examples', str(CONFIG['max_examples'])])\n",
        "\n",
        "# Run capture\n",
        "!{' '.join(cmd)}\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n✅ Activation capture completed in {elapsed/60:.1f} minutes\")\n",
        "print(f\"   Speedup: ~{(3.5*60)/elapsed:.1f}x faster than old method!\")\n",
        "\n",
        "# Copy to Google Drive for backup\n",
        "print(\"\\n📥 Backing up activations to Google Drive...\")\n",
        "!cp -r data/activations/* {drive_output_dir}/activations/\n",
        "print(\"✅ Backup complete!\")\n",
        "\n",
        "# Show captured layers\n",
        "import glob\n",
        "activation_files = glob.glob('data/activations/layer_*_activations.h5')\n",
        "print(f\"\\n📊 Captured {len(activation_files)} layer files:\")\n",
        "for f in sorted(activation_files)[:5]:\n",
        "    print(f\"  • {os.path.basename(f)}\")\n",
        "if len(activation_files) > 5:\n",
        "    print(f\"  ... and {len(activation_files) - 5} more\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8tUVM6It8fR"
      },
      "source": [
        "## 7️⃣ Step 2: Train Binary Probes for All Layers\n",
        "## This can be done with low-gpu and will take aroun 1-2 hrs\n",
        "\n",
        "Train 45 binary probes per layer (1,125 total probes) using one-vs-rest strategy.\n",
        "\n",
        "**⏰ Expected time:** 8-12 hours for all layers\n",
        "- ~20-30 minutes per layer\n",
        "- 25 layers total\n",
        "\n",
        "**🎯 Expected performance:** AUC-ROC 0.85-0.95 per probe (varies by layer)\n",
        "\n",
        "**💡 Note:** Training happens sequentially per layer. Progress is saved after each layer completes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R9IODrDt8fS",
        "outputId": "9d183434-fd96-428a-b8f4-4e588556097b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 2: 🚀 PARALLEL TRAINING OF BINARY PROBES\n",
            "======================================================================\n",
            "Layers: 1-20 (20 layers)\n",
            "Probes per layer: 45\n",
            "Total probes: 900\n",
            "\n",
            "🚀 Parallel Training Settings:\n",
            "  Workers: 45 (train 45 probes simultaneously)\n",
            "  Batch size: 32\n",
            "  Pin to GPU: True\n",
            "\n",
            "⏰ This will take ~2-3 hours (8x faster than sequential!)\n",
            "💡 Each layer's probes are trained in parallel, then saved.\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Training Layer 1 (1/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_1_activations.h5\n",
            "   Skipping layer 1\n",
            "\n",
            "======================================================================\n",
            "Training Layer 2 (2/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_2_activations.h5\n",
            "   Skipping layer 2\n",
            "\n",
            "======================================================================\n",
            "Training Layer 3 (3/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_3_activations.h5\n",
            "   Skipping layer 3\n",
            "\n",
            "======================================================================\n",
            "Training Layer 4 (4/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_4_activations.h5\n",
            "   Skipping layer 4\n",
            "\n",
            "======================================================================\n",
            "Training Layer 5 (5/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_5_activations.h5\n",
            "   Skipping layer 5\n",
            "\n",
            "======================================================================\n",
            "Training Layer 6 (6/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_6_activations.h5\n",
            "   Skipping layer 6\n",
            "\n",
            "======================================================================\n",
            "Training Layer 7 (7/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_7_activations.h5\n",
            "   Skipping layer 7\n",
            "\n",
            "======================================================================\n",
            "Training Layer 8 (8/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_8_activations.h5\n",
            "   Skipping layer 8\n",
            "\n",
            "======================================================================\n",
            "Training Layer 9 (9/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_9_activations.h5\n",
            "   Skipping layer 9\n",
            "\n",
            "======================================================================\n",
            "Training Layer 10 (10/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_10_activations.h5\n",
            "   Skipping layer 10\n",
            "\n",
            "======================================================================\n",
            "Training Layer 11 (11/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_11_activations.h5\n",
            "   Skipping layer 11\n",
            "\n",
            "======================================================================\n",
            "Training Layer 12 (12/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_12_activations.h5\n",
            "   Skipping layer 12\n",
            "\n",
            "======================================================================\n",
            "Training Layer 13 (13/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_13_activations.h5\n",
            "   Skipping layer 13\n",
            "\n",
            "======================================================================\n",
            "Training Layer 14 (14/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_14_activations.h5\n",
            "   Skipping layer 14\n",
            "\n",
            "======================================================================\n",
            "Training Layer 15 (15/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_15_activations.h5\n",
            "   Skipping layer 15\n",
            "\n",
            "======================================================================\n",
            "Training Layer 16 (16/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_16_activations.h5\n",
            "   Skipping layer 16\n",
            "\n",
            "======================================================================\n",
            "Training Layer 17 (17/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_17_activations.h5\n",
            "   Skipping layer 17\n",
            "\n",
            "======================================================================\n",
            "Training Layer 18 (18/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_18_activations.h5\n",
            "   Skipping layer 18\n",
            "\n",
            "======================================================================\n",
            "Training Layer 19 (19/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_19_activations.h5\n",
            "   Skipping layer 19\n",
            "\n",
            "======================================================================\n",
            "Training Layer 20 (20/20)\n",
            "🚀 Using 45 parallel workers\n",
            "======================================================================\n",
            "⚠️  Activation file not found: /content/drive/MyDrive/brije_outputs/activations/layer_20_activations.h5\n",
            "   Skipping layer 20\n",
            "\n",
            "======================================================================\n",
            "✅ ALL LAYERS COMPLETE!\n",
            "======================================================================\n",
            "Total time: 0.00 hours (0.0 minutes)\n",
            "Trained 0 probes across 0 layers\n",
            "🚀 Average speedup: 45x faster than sequential!\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'drive_output_dir' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 97\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrained \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_results)\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m45\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m probes across \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_results)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m layers\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🚀 Average speedup: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCONFIG[\u001b[33m'\u001b[39m\u001b[33mnum_workers\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33mx faster than sequential!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOutputs backed up to Google Drive: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mdrive_output_dir\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/probes_binary/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Save layer summary\u001b[39;00m\n\u001b[32m    100\u001b[39m summary = {\n\u001b[32m    101\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_layers\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(layer_results),\n\u001b[32m    102\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mtotal_probes\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28mlen\u001b[39m(layer_results) * \u001b[32m45\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    115\u001b[39m     }\n\u001b[32m    116\u001b[39m }\n",
            "\u001b[31mNameError\u001b[39m: name 'drive_output_dir' is not defined"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import time\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: 🚀 PARALLEL TRAINING OF BINARY PROBES\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Layers: {CONFIG['layer_start']}-{CONFIG['layer_end']} ({len(CONFIG['layers_to_capture'])} layers)\")\n",
        "print(f\"Probes per layer: 45\")\n",
        "print(f\"Total probes: {len(CONFIG['layers_to_capture']) * 45}\")\n",
        "print(f\"\\n🚀 Parallel Training Settings:\")\n",
        "print(f\"  Workers: {CONFIG['num_workers']} (train {CONFIG['num_workers']} probes simultaneously)\")\n",
        "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
        "print(f\"  Pin to GPU: {CONFIG['pin_activations_to_gpu']}\")\n",
        "print(f\"\\n⏰ This will take ~2-3 hours (8x faster than sequential!)\")\n",
        "print(\"💡 Each layer's probes are trained in parallel, then saved.\\n\")\n",
        "\n",
        "overall_start = time.time()\n",
        "layer_results = []\n",
        "\n",
        "for layer_idx in CONFIG['layers_to_capture']:\n",
        "    layer_start = time.time()\n",
        "\n",
        "    print(f\"\\n{'='*70}\")\n",
        "    print(f\"Training Layer {layer_idx} ({CONFIG['layers_to_capture'].index(layer_idx) + 1}/{len(CONFIG['layers_to_capture'])})\")\n",
        "    print(f\"🚀 Using {CONFIG['num_workers']} parallel workers\")\n",
        "    print(f\"{'='*70}\")\n",
        "\n",
        "    # Build command\n",
        "    activation_file = f\"data/activations/layer_{layer_idx}_activations.h5\"\n",
        "    output_dir = f\"data/probes_binary/layer_{layer_idx}\"\n",
        "\n",
        "    # Check if activations exist\n",
        "    if not os.path.exists(activation_file):\n",
        "        print(f\"⚠️  Activation file not found: {activation_file}\")\n",
        "        print(f\"   Skipping layer {layer_idx}\")\n",
        "        continue\n",
        "\n",
        "    # Use parallel training script\n",
        "    cmd = [\n",
        "        'python', 'src/probes/train_binary_probes_parallel.py',\n",
        "        '--activations', activation_file,\n",
        "        '--output-dir', output_dir,\n",
        "        '--model-type', CONFIG['probe_type'],\n",
        "        '--batch-size', str(CONFIG['batch_size']),\n",
        "        '--epochs', str(CONFIG['epochs']),\n",
        "        '--lr', str(CONFIG['learning_rate']),\n",
        "        '--weight-decay', str(CONFIG['weight_decay']),\n",
        "        '--early-stopping-patience', str(CONFIG['early_stopping_patience']),\n",
        "        '--device', CONFIG['device'],\n",
        "        '--num-workers', str(CONFIG['num_workers'])\n",
        "    ]\n",
        "\n",
        "    # Add scheduler flag\n",
        "    if not CONFIG.get('use_scheduler', True):\n",
        "        cmd.append('--no-scheduler')\n",
        "\n",
        "    # Add GPU pinning flag\n",
        "    if CONFIG['pin_activations_to_gpu']:\n",
        "        cmd.append('--pin-activations-to-gpu')\n",
        "    else:\n",
        "        cmd.append('--no-pin-activations')\n",
        "\n",
        "    # Run parallel training\n",
        "    !{' '.join(cmd)}\n",
        "\n",
        "    layer_elapsed = time.time() - layer_start\n",
        "\n",
        "    # Load metrics for this layer\n",
        "    metrics_file = f\"{output_dir}/aggregate_metrics.json\"\n",
        "    if os.path.exists(metrics_file):\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "\n",
        "        speedup = metrics.get('num_workers', 1)\n",
        "        layer_results.append({\n",
        "            'layer': layer_idx,\n",
        "            'avg_auc': metrics['average_auc_roc'],\n",
        "            'avg_f1': metrics['average_f1'],\n",
        "            'avg_accuracy': metrics['average_accuracy'],\n",
        "            'time_minutes': layer_elapsed / 60,\n",
        "            'speedup': speedup\n",
        "        })\n",
        "\n",
        "        print(f\"\\n✅ Layer {layer_idx} complete in {layer_elapsed/60:.1f} minutes (🚀 {speedup}x speedup!)\")\n",
        "        print(f\"   Avg AUC: {metrics['average_auc_roc']:.4f}, Avg F1: {metrics['average_f1']:.4f}\")\n",
        "\n",
        "    # Backup to Google Drive after each layer\n",
        "    !cp -r {output_dir} {drive_output_dir}/probes_binary/\n",
        "\n",
        "overall_elapsed = time.time() - overall_start\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"✅ ALL LAYERS COMPLETE!\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Total time: {overall_elapsed/3600:.2f} hours ({overall_elapsed/60:.1f} minutes)\")\n",
        "print(f\"Trained {len(layer_results) * 45} probes across {len(layer_results)} layers\")\n",
        "print(f\"🚀 Average speedup: {CONFIG['num_workers']}x faster than sequential!\")\n",
        "print(f\"\\nOutputs backed up to Google Drive: {drive_output_dir}/probes_binary/\")\n",
        "\n",
        "# Save layer summary\n",
        "summary = {\n",
        "    'total_layers': len(layer_results),\n",
        "    'total_probes': len(layer_results) * 45,\n",
        "    'total_time_hours': overall_elapsed / 3600,\n",
        "    'parallel_training': True,\n",
        "    'num_workers': CONFIG['num_workers'],\n",
        "    'layer_results': layer_results,\n",
        "    'config': {\n",
        "        'batch_size': CONFIG['batch_size'],\n",
        "        'epochs': CONFIG['epochs'],\n",
        "        'learning_rate': CONFIG['learning_rate'],\n",
        "        'weight_decay': CONFIG['weight_decay'],\n",
        "        'early_stopping_patience': CONFIG['early_stopping_patience'],\n",
        "        'use_scheduler': CONFIG.get('use_scheduler', True),\n",
        "        'num_workers': CONFIG['num_workers']\n",
        "    }\n",
        "}\n",
        "\n",
        "with open('data/probes_binary/training_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(f\"\\nSummary saved to: data/probes_binary/training_summary.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "audo3wQ0eF2i"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "if layer_results:\n",
        "    print(\"=\"*70)\n",
        "    print(\"PARALLEL TRAINING PERFORMANCE SUMMARY\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Time savings\n",
        "    sequential_time = overall_elapsed * CONFIG['num_workers']\n",
        "    time_saved = sequential_time - overall_elapsed\n",
        "\n",
        "    print(f\"\\n⏱️  Time Performance:\")\n",
        "    print(f\"  Parallel time: {overall_elapsed/3600:.2f} hours\")\n",
        "    print(f\"  Sequential estimate: {sequential_time/3600:.2f} hours\")\n",
        "    print(f\"  Time saved: {time_saved/3600:.2f} hours! 🎉\")\n",
        "    print(f\"  Speedup: {CONFIG['num_workers']}x\")\n",
        "\n",
        "    # Accuracy metrics\n",
        "    avg_auc = np.mean([m['avg_auc'] for m in layer_results])\n",
        "    best_layer = max(layer_results, key=lambda x: x['avg_auc'])\n",
        "\n",
        "    print(f\"\\n📊 Accuracy Metrics:\")\n",
        "    print(f\"  Average AUC: {avg_auc:.4f}\")\n",
        "    print(f\"  Best layer: {best_layer['layer']} (AUC: {best_layer['avg_auc']:.4f})\")\n",
        "    print(f\"  Total probes trained: {len(layer_results) * 45}\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GmxKTOmt8fS"
      },
      "source": [
        "## 8️⃣ View Training Results - Performance Across All Layers\n",
        "\n",
        "Compare binary probe performance across different layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8DW4ONet8fS"
      },
      "source": [
        "## 8️⃣ View Training Results - Overall Performance Across Layers\n",
        "\n",
        "Compare overall binary probe performance across different layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjpBprJDt8fS"
      },
      "source": [
        "## 9️⃣ Step 3: Test Multi-Probe Inference\n",
        "\n",
        "Run all 45 binary probes from the **best performing layer** to detect cognitive actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8cBBgHot8fS"
      },
      "source": [
        "## 8️⃣.5️⃣ Per-Action Layer Analysis - Find Best Layer for Each Cognitive Action\n",
        "\n",
        "Analyze which layer performs best for EACH of the 45 cognitive actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMqfmFPYt8fS"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PER-ACTION LAYER ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAnalyzing which layer is best for each cognitive action...\\n\")\n",
        "\n",
        "# Collect per-action metrics across all layers\n",
        "action_layer_performance = defaultdict(dict)  # {action_name: {layer: auc}}\n",
        "\n",
        "for layer_idx in CONFIG['layers_to_capture']:\n",
        "    metrics_file = f'data/probes_binary/layer_{layer_idx}/aggregate_metrics.json'\n",
        "    if os.path.exists(metrics_file):\n",
        "        with open(metrics_file, 'r') as f:\n",
        "            metrics = json.load(f)\n",
        "\n",
        "        # Extract per-action metrics\n",
        "        for action_metrics in metrics['per_action_metrics']:\n",
        "            action_name = action_metrics['action']\n",
        "            auc = action_metrics['auc_roc']\n",
        "            f1 = action_metrics['f1']\n",
        "\n",
        "            action_layer_performance[action_name][layer_idx] = {\n",
        "                'auc': auc,\n",
        "                'f1': f1\n",
        "            }\n",
        "\n",
        "# Find best layer for each action\n",
        "action_best_layers = []\n",
        "for action_name, layer_perfs in sorted(action_layer_performance.items()):\n",
        "    # Find layer with highest AUC\n",
        "    best_layer = max(layer_perfs.items(), key=lambda x: x[1]['auc'])\n",
        "    layer_idx, perf = best_layer\n",
        "\n",
        "    # Get performance range\n",
        "    auc_scores = [p['auc'] for p in layer_perfs.values()]\n",
        "    auc_range = max(auc_scores) - min(auc_scores)\n",
        "\n",
        "    action_best_layers.append({\n",
        "        'action': action_name,\n",
        "        'best_layer': layer_idx,\n",
        "        'best_auc': perf['auc'],\n",
        "        'best_f1': perf['f1'],\n",
        "        'auc_range': auc_range,\n",
        "        'worst_auc': min(auc_scores),\n",
        "        'layer_sensitivity': auc_range  # How much performance varies by layer\n",
        "    })\n",
        "\n",
        "# Convert to DataFrame for easier analysis\n",
        "df = pd.DataFrame(action_best_layers)\n",
        "\n",
        "print(f\"✅ Analyzed {len(action_best_layers)} cognitive actions across {len(CONFIG['layers_to_capture'])} layers\\n\")\n",
        "\n",
        "# Show distribution of best layers\n",
        "print(\"=\"*70)\n",
        "print(\"BEST LAYER DISTRIBUTION\")\n",
        "print(\"=\"*70)\n",
        "layer_counts = df['best_layer'].value_counts().sort_index()\n",
        "print(\"\\nHow many actions are best detected at each layer:\\n\")\n",
        "for layer, count in layer_counts.items():\n",
        "    bar = \"█\" * count\n",
        "    print(f\"  Layer {layer:2d}: {count:2d} actions {bar}\")\n",
        "\n",
        "# Most common best layers\n",
        "top_layers = layer_counts.head(5)\n",
        "print(f\"\\n🏆 Most effective layers:\")\n",
        "for layer, count in top_layers.items():\n",
        "    pct = (count / len(action_best_layers)) * 100\n",
        "    print(f\"   Layer {layer}: {count} actions ({pct:.1f}%)\")\n",
        "\n",
        "# Show actions grouped by best layer\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ACTIONS GROUPED BY BEST LAYER\")\n",
        "print(\"=\"*70)\n",
        "for layer in sorted(df['best_layer'].unique())[:10]:  # Show first 10 layers\n",
        "    actions = df[df['best_layer'] == layer]['action'].tolist()\n",
        "    if actions:\n",
        "        print(f\"\\nLayer {layer} ({len(actions)} actions):\")\n",
        "        for action in actions[:5]:  # Show first 5 actions per layer\n",
        "            auc = df[df['action'] == action]['best_auc'].values[0]\n",
        "            print(f\"  • {action:30s} (AUC: {auc:.4f})\")\n",
        "        if len(actions) > 5:\n",
        "            print(f\"  ... and {len(actions) - 5} more\")\n",
        "\n",
        "# Show most layer-sensitive actions (vary a lot by layer)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LAYER-SENSITIVE ACTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nActions where layer choice matters most:\\n\")\n",
        "df_sorted = df.sort_values('layer_sensitivity', ascending=False)\n",
        "print(f\"{'Action':<30} {'Best Layer':<12} {'Best AUC':<10} {'AUC Range':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for _, row in df_sorted.head(10).iterrows():\n",
        "    print(f\"{row['action']:<30} Layer {row['best_layer']:<6} {row['best_auc']:.4f}     {row['auc_range']:.4f}\")\n",
        "\n",
        "print(\"\\n→ Large AUC range = layer choice is critical for this action\")\n",
        "\n",
        "# Show layer-robust actions (work well across all layers)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LAYER-ROBUST ACTIONS\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nActions that work well across all layers:\\n\")\n",
        "print(f\"{'Action':<30} {'Best Layer':<12} {'Best AUC':<10} {'AUC Range':<10}\")\n",
        "print(\"-\" * 70)\n",
        "for _, row in df_sorted.tail(10).iterrows():\n",
        "    print(f\"{row['action']:<30} Layer {row['best_layer']:<6} {row['best_auc']:.4f}     {row['auc_range']:.4f}\")\n",
        "\n",
        "print(\"\\n→ Small AUC range = works well regardless of layer\")\n",
        "\n",
        "# Save detailed results\n",
        "results = {\n",
        "    'summary': {\n",
        "        'total_actions': len(action_best_layers),\n",
        "        'total_layers_tested': len(CONFIG['layers_to_capture']),\n",
        "        'most_common_best_layer': int(layer_counts.idxmax()),\n",
        "        'layer_distribution': {int(k): int(v) for k, v in layer_counts.items()}\n",
        "    },\n",
        "    'per_action_best_layers': action_best_layers\n",
        "}\n",
        "\n",
        "with open('data/probes_binary/per_action_layer_analysis.json', 'w') as f:\n",
        "    json.dump(results, f, indent=2)\n",
        "\n",
        "print(f\"\\n✅ Detailed results saved to: data/probes_binary/per_action_layer_analysis.json\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqzFry_pt8fS"
      },
      "source": [
        "## 8️⃣.6️⃣ Visualize Per-Action Layer Performance\n",
        "\n",
        "Heatmap showing which layers are best for each cognitive action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVVLi9HBt8fT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Create heatmap data: actions × layers\n",
        "actions = sorted(action_layer_performance.keys())\n",
        "layers = sorted(CONFIG['layers_to_capture'])\n",
        "\n",
        "# Build matrix\n",
        "heatmap_data = []\n",
        "for action in actions:\n",
        "    row = []\n",
        "    for layer in layers:\n",
        "        if layer in action_layer_performance[action]:\n",
        "            row.append(action_layer_performance[action][layer]['auc'])\n",
        "        else:\n",
        "            row.append(np.nan)\n",
        "    heatmap_data.append(row)\n",
        "\n",
        "heatmap_data = np.array(heatmap_data)\n",
        "\n",
        "# Create figure with multiple subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "\n",
        "# Plot 1: Full heatmap (all actions, all layers)\n",
        "ax1 = axes[0, 0]\n",
        "im1 = ax1.imshow(heatmap_data, aspect='auto', cmap='RdYlGn', vmin=0.5, vmax=1.0)\n",
        "ax1.set_xticks(range(0, len(layers), 2))\n",
        "ax1.set_xticklabels([layers[i] for i in range(0, len(layers), 2)], fontsize=8)\n",
        "ax1.set_yticks(range(len(actions)))\n",
        "ax1.set_yticklabels(actions, fontsize=6)\n",
        "ax1.set_xlabel('Layer', fontsize=10)\n",
        "ax1.set_ylabel('Cognitive Action', fontsize=10)\n",
        "ax1.set_title('Per-Action AUC-ROC Across All Layers', fontsize=12, fontweight='bold')\n",
        "plt.colorbar(im1, ax=ax1, label='AUC-ROC')\n",
        "\n",
        "# Mark best layer for each action with a star\n",
        "for i, action in enumerate(actions):\n",
        "    best_layer_idx = df[df['action'] == action]['best_layer'].values[0]\n",
        "    best_layer_pos = layers.index(best_layer_idx)\n",
        "    ax1.plot(best_layer_pos, i, 'w*', markersize=4)\n",
        "\n",
        "# Plot 2: Best layer distribution\n",
        "ax2 = axes[0, 1]\n",
        "layer_counts = df['best_layer'].value_counts().sort_index()\n",
        "ax2.bar(layer_counts.index, layer_counts.values, color='steelblue', alpha=0.7)\n",
        "ax2.set_xlabel('Layer', fontsize=10)\n",
        "ax2.set_ylabel('Number of Actions', fontsize=10)\n",
        "ax2.set_title('Distribution of Best Layers\\n(How many actions peak at each layer)', fontsize=12, fontweight='bold')\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for layer, count in layer_counts.items():\n",
        "    ax2.text(layer, count + 0.2, str(count), ha='center', fontsize=8)\n",
        "\n",
        "# Plot 3: Layer sensitivity (how much does layer matter?)\n",
        "ax3 = axes[1, 0]\n",
        "df_sorted_sens = df.sort_values('layer_sensitivity', ascending=False)\n",
        "y_pos = np.arange(len(df_sorted_sens.head(20)))\n",
        "ax3.barh(y_pos, df_sorted_sens.head(20)['layer_sensitivity'], color='coral', alpha=0.7)\n",
        "ax3.set_yticks(y_pos)\n",
        "ax3.set_yticklabels(df_sorted_sens.head(20)['action'], fontsize=8)\n",
        "ax3.set_xlabel('AUC Range (Best - Worst)', fontsize=10)\n",
        "ax3.set_title('Top 20 Layer-Sensitive Actions\\n(Layer choice matters most)', fontsize=12, fontweight='bold')\n",
        "ax3.grid(True, alpha=0.3, axis='x')\n",
        "ax3.invert_yaxis()\n",
        "\n",
        "# Plot 4: Average performance by layer group\n",
        "ax4 = axes[1, 1]\n",
        "n = len(layers)\n",
        "early_layers = layers[:n//3]\n",
        "middle_layers = layers[n//3:2*n//3]\n",
        "late_layers = layers[2*n//3:]\n",
        "\n",
        "groups = ['Early\\n(4-12)', 'Middle\\n(13-20)', 'Late\\n(21-28)']\n",
        "layer_groups = [early_layers, middle_layers, late_layers]\n",
        "\n",
        "# Calculate average AUC for each group\n",
        "group_aucs = []\n",
        "for layer_group in layer_groups:\n",
        "    aucs = []\n",
        "    for layer in layer_group:\n",
        "        if layer in all_layer_metrics_dict:  # Need to create this\n",
        "            aucs.append(all_layer_metrics_dict[layer]['avg_auc'])\n",
        "    group_aucs.append(np.mean(aucs) if aucs else 0)\n",
        "\n",
        "# Create dict for easy lookup\n",
        "all_layer_metrics_dict = {m['layer']: m for m in all_layer_metrics}\n",
        "group_aucs = []\n",
        "for layer_group in layer_groups:\n",
        "    aucs = [all_layer_metrics_dict[layer]['avg_auc'] for layer in layer_group if layer in all_layer_metrics_dict]\n",
        "    group_aucs.append(np.mean(aucs) if aucs else 0)\n",
        "\n",
        "bars = ax4.bar(groups, group_aucs, color=['lightblue', 'lightgreen', 'lightsalmon'], alpha=0.7, edgecolor='black')\n",
        "ax4.set_ylabel('Average AUC-ROC', fontsize=10)\n",
        "ax4.set_title('Performance by Layer Group\\n(Early vs Middle vs Late)', fontsize=12, fontweight='bold')\n",
        "ax4.set_ylim([min(group_aucs) - 0.02, max(group_aucs) + 0.02])\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Add value labels on bars\n",
        "for bar, auc in zip(bars, group_aucs):\n",
        "    height = bar.get_height()\n",
        "    ax4.text(bar.get_x() + bar.get_width()/2., height + 0.005,\n",
        "             f'{auc:.4f}', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('data/probes_binary/per_action_layer_analysis.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Per-action layer analysis saved to: data/probes_binary/per_action_layer_analysis.png\")\n",
        "\n",
        "# Additional insight: Show examples of actions best at different layer stages\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"LAYER STAGE EXAMPLES\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "early_actions = df[df['best_layer'].isin(early_layers)].nlargest(5, 'best_auc')\n",
        "middle_actions = df[df['best_layer'].isin(middle_layers)].nlargest(5, 'best_auc')\n",
        "late_actions = df[df['best_layer'].isin(late_layers)].nlargest(5, 'best_auc')\n",
        "\n",
        "print(f\"\\n📘 Best actions detected in EARLY layers ({min(early_layers)}-{max(early_layers)}):\")\n",
        "for _, row in early_actions.iterrows():\n",
        "    print(f\"   • {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n",
        "\n",
        "print(f\"\\n📗 Best actions detected in MIDDLE layers ({min(middle_layers)}-{max(middle_layers)}):\")\n",
        "for _, row in middle_actions.iterrows():\n",
        "    print(f\"   • {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n",
        "\n",
        "print(f\"\\n📕 Best actions detected in LATE layers ({min(late_layers)}-{max(late_layers)}):\")\n",
        "for _, row in late_actions.iterrows():\n",
        "    print(f\"   • {row['action']:30s} Layer {row['best_layer']}, AUC: {row['best_auc']:.4f}\")\n",
        "\n",
        "print(\"\\n→ This reveals which cognitive processes are captured at different network depths!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neiC7jHot8fT"
      },
      "outputs": [],
      "source": [
        "# Determine best layer for inference\n",
        "if all_layer_metrics:\n",
        "    best_layer_idx = max(all_layer_metrics, key=lambda x: x['avg_auc'])['layer']\n",
        "else:\n",
        "    best_layer_idx = CONFIG['layer_end']  # Default to last layer\n",
        "\n",
        "print(f\"Using probes from Layer {best_layer_idx} (best performing layer)\\n\")\n",
        "\n",
        "# Test on sample texts\n",
        "test_texts = [\n",
        "    \"After receiving feedback, she began reconsidering her initial approach to the problem.\",\n",
        "    \"He was analyzing the data to find patterns and correlations between variables.\",\n",
        "    \"They started generating creative ideas for solving the design challenge.\",\n",
        "    \"She was evaluating different strategies to determine the most effective one.\",\n",
        "    \"He tried to recall the specific details from the previous meeting.\"\n",
        "]\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MULTI-PROBE INFERENCE EXAMPLES\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nRunning all 45 binary probes from Layer {best_layer_idx}...\\n\")\n",
        "\n",
        "for i, text in enumerate(test_texts, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Example {i}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"📝 Text: {text}\\n\")\n",
        "\n",
        "    cmd = [\n",
        "        'python', 'src/probes/multi_probe_inference.py',\n",
        "        '--probes-dir', f'data/probes_binary/layer_{best_layer_idx}',\n",
        "        '--model', CONFIG['model'],\n",
        "        '--layer', str(best_layer_idx),\n",
        "        '--text', f'\"{text}\"',\n",
        "        '--top-k', '5',\n",
        "        '--threshold', '0.1'\n",
        "    ]\n",
        "\n",
        "    !{' '.join(cmd)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbTFqTYnt8fT"
      },
      "source": [
        "## 🔟 Visualize Performance Across Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuaRe1QVt8fT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Plot performance across layers\n",
        "if all_layer_metrics:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "    layers = [m['layer'] for m in all_layer_metrics]\n",
        "    auc_scores = [m['avg_auc'] for m in all_layer_metrics]\n",
        "    f1_scores = [m['avg_f1'] for m in all_layer_metrics]\n",
        "    acc_scores = [m['avg_accuracy'] for m in all_layer_metrics]\n",
        "\n",
        "    # Plot 1: AUC-ROC across layers\n",
        "    axes[0].plot(layers, auc_scores, 'b-o', linewidth=2, markersize=6, label='AUC-ROC')\n",
        "    axes[0].axhline(y=np.mean(auc_scores), color='r', linestyle='--', alpha=0.5, label='Mean')\n",
        "    axes[0].set_xlabel('Layer', fontsize=12)\n",
        "    axes[0].set_ylabel('Average AUC-ROC', fontsize=12)\n",
        "    axes[0].set_title('Binary Probe Performance Across Layers (AUC-ROC)', fontsize=14, fontweight='bold')\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    axes[0].legend()\n",
        "    axes[0].set_ylim([min(auc_scores) - 0.02, max(auc_scores) + 0.02])\n",
        "\n",
        "    # Mark best layer\n",
        "    best_idx = np.argmax(auc_scores)\n",
        "    axes[0].annotate(f'Best: {layers[best_idx]}\\n{auc_scores[best_idx]:.4f}',\n",
        "                     xy=(layers[best_idx], auc_scores[best_idx]),\n",
        "                     xytext=(10, 10), textcoords='offset points',\n",
        "                     bbox=dict(boxstyle='round,pad=0.5', facecolor='yellow', alpha=0.7),\n",
        "                     arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
        "\n",
        "    # Plot 2: All metrics comparison\n",
        "    axes[1].plot(layers, auc_scores, 'b-o', label='AUC-ROC', linewidth=2, markersize=5)\n",
        "    axes[1].plot(layers, f1_scores, 'g-s', label='F1 Score', linewidth=2, markersize=5)\n",
        "    axes[1].plot(layers, acc_scores, 'r-^', label='Accuracy', linewidth=2, markersize=5)\n",
        "    axes[1].set_xlabel('Layer', fontsize=12)\n",
        "    axes[1].set_ylabel('Score', fontsize=12)\n",
        "    axes[1].set_title('All Metrics Across Layers', fontsize=14, fontweight='bold')\n",
        "    axes[1].grid(True, alpha=0.3)\n",
        "    axes[1].legend()\n",
        "    axes[1].set_ylim([0.5, 1.0])\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('data/probes_binary/layer_comparison.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"✅ Layer comparison plot saved to: data/probes_binary/layer_comparison.png\")\n",
        "\n",
        "    # Additional analysis: Early vs Middle vs Late layers\n",
        "    n = len(layers)\n",
        "    early = all_layer_metrics[:n//3]\n",
        "    middle = all_layer_metrics[n//3:2*n//3]\n",
        "    late = all_layer_metrics[2*n//3:]\n",
        "\n",
        "    print(\"\\n📊 Layer Group Analysis:\")\n",
        "    print(f\"   Early layers ({layers[0]}-{layers[n//3-1]}):  AUC = {np.mean([m['avg_auc'] for m in early]):.4f}\")\n",
        "    print(f\"   Middle layers ({layers[n//3]}-{layers[2*n//3-1]}): AUC = {np.mean([m['avg_auc'] for m in middle]):.4f}\")\n",
        "    print(f\"   Late layers ({layers[2*n//3]}-{layers[-1]}):   AUC = {np.mean([m['avg_auc'] for m in late]):.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"⚠️  No metrics available for plotting\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "een_r7r3t8fT"
      },
      "source": [
        "## 1️⃣1️⃣ Download Trained Binary Probes\n",
        "\n",
        "Download all 45 trained binary probes and metrics for local use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFW4PqDxt8fT"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "\n",
        "# Create a zip file with all outputs\n",
        "output_zip = 'brije_all_layers_binary_probes.zip'\n",
        "\n",
        "print(\"📦 Creating download package...\")\n",
        "print(\"   This may take a few minutes for 1,125 probe files...\")\n",
        "!cd data && zip -r ../{output_zip} probes_binary/ -q\n",
        "\n",
        "print(f\"\\n✅ Package created: {output_zip}\")\n",
        "print(f\"Size: {os.path.getsize(output_zip) / 1e6:.2f} MB\")\n",
        "\n",
        "# Option to download best layer only\n",
        "print(\"\\n💡 TIP: Download options:\")\n",
        "print(\"   1. Full package (all layers) - see below\")\n",
        "print(f\"   2. Best layer only - smaller download\")\n",
        "\n",
        "# Create best layer only zip\n",
        "best_layer_zip = f'brije_layer_{best_layer_idx}_probes.zip'\n",
        "!cd data/probes_binary && zip -r ../../{best_layer_zip} layer_{best_layer_idx}/ -q\n",
        "\n",
        "print(f\"\\nBest layer package: {best_layer_zip} ({os.path.getsize(best_layer_zip) / 1e6:.2f} MB)\")\n",
        "\n",
        "# Download best layer by default (faster)\n",
        "print(\"\\n📥 Downloading best layer probes...\")\n",
        "files.download(best_layer_zip)\n",
        "\n",
        "print(\"\\n✅ Download complete!\")\n",
        "print(\"\\nPackage contains:\")\n",
        "print(f\"  • Layer {best_layer_idx} probes (45 binary probes)\")\n",
        "print(\"  • Per-action metrics (metrics_*.json)\")\n",
        "print(\"  • Aggregate performance summary\")\n",
        "print(\"\\nTo download ALL layers (all 1,125 probes):\")\n",
        "print(f\"  Uncomment the line below and run again:\")\n",
        "print(f\"  # files.download('{output_zip}')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmi6sbhkt8fU"
      },
      "source": [
        "## 1️⃣2️⃣ Summary and Next Steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAaUpBbSt8fU"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"🎉 PIPELINE COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\n✅ What was accomplished:\")\n",
        "print(\"  1. Captured activations from Gemma 3 4B (layers 4-28)\")\n",
        "print(\"  2. Trained 1,125 binary probes (45 per layer × 25 layers)\")\n",
        "print(\"  3. Evaluated performance across all layers\")\n",
        "print(\"  4. Identified best performing layer\")\n",
        "print(\"  5. Saved all outputs to Google Drive\")\n",
        "print(\"\\n📂 Outputs saved to:\")\n",
        "print(f\"  • Local: {os.getcwd()}/data/\")\n",
        "print(f\"  • Google Drive: {drive_output_dir}\")\n",
        "\n",
        "if all_layer_metrics:\n",
        "    best = max(all_layer_metrics, key=lambda x: x['avg_auc'])\n",
        "    print(f\"\\n🏆 Best Layer: {best['layer']} (AUC: {best['avg_auc']:.4f})\")\n",
        "\n",
        "print(\"\\n🚀 Next Steps:\")\n",
        "print(\"  1. Download trained probes (best layer or all layers)\")\n",
        "print(\"  2. Use multi_probe_inference.py for predictions\")\n",
        "print(\"  3. Compare performance across layers\")\n",
        "print(\"  4. Experiment with different thresholds\")\n",
        "\n",
        "print(\"\\n💡 Usage Example (Best Layer):\")\n",
        "print(f\"  python src/probes/multi_probe_inference.py \\\\\")\n",
        "print(f\"    --probes-dir data/probes_binary/layer_{best_layer_idx if all_layer_metrics else 27} \\\\\")\n",
        "print(\"    --model google/gemma-3-4b-it \\\\\")\n",
        "print(f\"    --layer {best_layer_idx if all_layer_metrics else 27} \\\\\")\n",
        "print(\"    --text \\\"Your text here\\\" \\\\\")\n",
        "print(\"    --top-k 5\")\n",
        "\n",
        "print(\"\\n📊 Key Findings:\")\n",
        "if all_layer_metrics:\n",
        "    auc_scores = [m['avg_auc'] for m in all_layer_metrics]\n",
        "    print(f\"  • {len(all_layer_metrics)} layers trained successfully\")\n",
        "    print(f\"  • Average AUC across all layers: {np.mean(auc_scores):.4f}\")\n",
        "    print(f\"  • Performance range: {min(auc_scores):.4f} - {max(auc_scores):.4f}\")\n",
        "    print(f\"  • {sum(1 for auc in auc_scores if auc > 0.90)}/{len(auc_scores)} layers achieved AUC > 0.90\")\n",
        "\n",
        "print(\"\\n📚 Files & Documentation:\")\n",
        "print(\"  • training_summary.json - Overall training summary\")\n",
        "print(\"  • layer_comparison.png - Performance visualization\")\n",
        "print(\"  • aggregate_metrics.json (per layer) - Detailed metrics\")\n",
        "print(\"  • README.md - Full documentation\")\n",
        "print(\"=\"*60)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "14d2292433bf4ef497a3693140f4452a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "154ac1e8eb9a4e0c8d7443f34395619e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e3c2aaebc414256a0ef4e9542ea46bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21958c52afed4253b1c3f533123ed494": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_7373b16c4e5b422ea84e90b2200027c9",
            "placeholder": "​",
            "style": "IPY_MODEL_14d2292433bf4ef497a3693140f4452a",
            "value": ""
          }
        },
        "2b1219eb331c4ea3b5f115aeeaa342a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6beafebe160145ae831cfa26a97a0945",
            "placeholder": "​",
            "style": "IPY_MODEL_1e3c2aaebc414256a0ef4e9542ea46bf",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2d96e883b39d4ee0ad348d5c0fe0b36f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "50e0396d23884abc96cf28d09651b58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_868aa77e62c04df2926a25164ea34f21",
            "style": "IPY_MODEL_68570ed157c64c6e81c41bb561eeae47",
            "tooltip": ""
          }
        },
        "652f725ad1e948cd9c4cd20540ecb7be": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6726a1860b2e4e5084abfcdc9d601d65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68570ed157c64c6e81c41bb561eeae47": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "6beafebe160145ae831cfa26a97a0945": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dc12eb7d440431a870c7f4ffcc5a192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cce096dff7b94ceebd7aa5b6b1050b5c",
            "placeholder": "​",
            "style": "IPY_MODEL_6726a1860b2e4e5084abfcdc9d601d65",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "7373b16c4e5b422ea84e90b2200027c9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "868aa77e62c04df2926a25164ea34f21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a0bb0bdc03db46148305031b3fe4bf78": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_652f725ad1e948cd9c4cd20540ecb7be",
            "placeholder": "​",
            "style": "IPY_MODEL_ba575c089e9a430da29bfdfa0a5003bf",
            "value": "Connecting..."
          }
        },
        "b4d708014761427e9e1cfe02aa143638": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba575c089e9a430da29bfdfa0a5003bf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cce096dff7b94ceebd7aa5b6b1050b5c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cde93fa85fc7445caee66a167784cf08": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_2d96e883b39d4ee0ad348d5c0fe0b36f"
          }
        },
        "e245900d5bfc406a9efad6d7945b00d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_154ac1e8eb9a4e0c8d7443f34395619e",
            "style": "IPY_MODEL_b4d708014761427e9e1cfe02aa143638",
            "value": true
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
