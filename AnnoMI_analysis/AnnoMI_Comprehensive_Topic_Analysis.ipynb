{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnnoMI Comprehensive Analysis - Cognitive Actions √ó MI Annotations √ó Therapy Topics\n",
    "\n",
    "This notebook provides a **unified analysis** combining the best insights from all previous analyses while incorporating the **therapy topic dimension** to understand how different therapeutic contexts shape cognitive patterns and MI quality.\n",
    "\n",
    "**Dataset Composition:**\n",
    "- üéØ **44 unique therapy topics** (alcohol reduction, smoking cessation, diabetes management, etc.)\n",
    "- üß† **45 cognitive actions** from multi-layer neural predictions\n",
    "- üí¨ **133 therapy transcripts** with MI quality labels\n",
    "- üìä **Rich annotations** (questions, reflections, therapist inputs)\n",
    "\n",
    "**Integrated Analyses:**\n",
    "1. üåç **Topic Distribution & Characteristics** - Understanding the dataset composition\n",
    "2. üéØ **Topic-Specific Cognitive Signatures** - How cognitive patterns vary by therapy topic\n",
    "3. üîÑ **Cognitive Synchrony by Topic** - Therapist-client alignment across different topics\n",
    "4. üí° **Change Talk Patterns by Topic** - Which topics facilitate client change commitment\n",
    "5. üë• **Therapist Styles Across Topics** - Clustering therapeutic approaches by topic\n",
    "6. üé≠ **MI Techniques √ó Topics √ó Cognitive Actions** - Three-way interaction analysis\n",
    "7. üìà **Topic-Aware Predictive Modeling** - Predicting MI quality with topic features\n",
    "8. üï∏Ô∏è **Network Analysis by Topic Cluster** - Graph-based insights segmented by topic type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import combinations\n",
    "from scipy import stats\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import networkx as nx\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (18, 10)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úÖ Libraries loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AnnoMI-full.csv (with rich annotations)\n",
    "print(\"üì• Loading AnnoMI-full.csv...\")\n",
    "df_full = pd.read_csv('third_party/AnnoMI/AnnoMI-full.csv')\n",
    "\n",
    "# Take first annotation per utterance\n",
    "df_annotations = df_full.groupby(['transcript_id', 'utterance_id']).first().reset_index()\n",
    "print(f\"‚úÖ Loaded {len(df_annotations)} annotated utterances\")\n",
    "\n",
    "# Load cognitive action predictions\n",
    "print(\"\\nüì• Loading cognitive action predictions...\")\n",
    "with open('output/analysis_AnnoMI/all_predictions.json', 'r') as f:\n",
    "    all_predictions = json.load(f)\n",
    "\n",
    "df_predictions = pd.DataFrame(all_predictions)\n",
    "print(f\"‚úÖ Loaded {len(df_predictions)} cognitive predictions\")\n",
    "\n",
    "# Merge datasets\n",
    "print(\"\\nüîó Merging annotations with cognitive predictions...\")\n",
    "df = df_annotations.merge(\n",
    "    df_predictions[['transcript_id', 'utterance_id', 'predictions', 'action_layer_details']],\n",
    "    on=['transcript_id', 'utterance_id'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Merged dataset: {len(df)} utterances\")\n",
    "print(f\"\\nüìä Dataset Overview:\")\n",
    "print(f\"   Transcripts: {df['transcript_id'].nunique()}\")\n",
    "print(f\"   Topics: {df['topic'].nunique()}\")\n",
    "print(f\"   Therapist utterances: {len(df[df['interlocutor'] == 'therapist'])}\")\n",
    "print(f\"   Client utterances: {len(df[df['interlocutor'] == 'client'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter short utterances and extract high-confidence cognitive actions\n",
    "MIN_LENGTH = 10\n",
    "df['utterance_length'] = df['utterance_text'].str.len()\n",
    "df = df[df['utterance_length'] >= MIN_LENGTH].copy()\n",
    "\n",
    "print(f\"üîÑ Filtered to {len(df)} utterances (>= {MIN_LENGTH} chars)\")\n",
    "\n",
    "# Extract high-confidence cognitive actions (>2 layers OR 100% confidence)\n",
    "def extract_high_confidence_actions(row):\n",
    "    \"\"\"Extract cognitive actions with >2 layers OR 100% confidence\"\"\"\n",
    "    if pd.isna(row['predictions']):\n",
    "        return []\n",
    "    \n",
    "    predictions = row['predictions']\n",
    "    action_layer_details = row.get('action_layer_details', {})\n",
    "    \n",
    "    active = []\n",
    "    for action, data in predictions.items():\n",
    "        if not data.get('is_active', False):\n",
    "            continue\n",
    "        \n",
    "        num_layers = len(action_layer_details.get(action, []))\n",
    "        max_confidence = max(\n",
    "            [layer['confidence'] for layer in action_layer_details.get(action, [])],\n",
    "            default=0\n",
    "        )\n",
    "        \n",
    "        if num_layers > 2 or max_confidence >= 1.0:\n",
    "            active.append({\n",
    "                'action': action,\n",
    "                'confidence': data['aggregate'],\n",
    "                'num_layers': num_layers\n",
    "            })\n",
    "    return active\n",
    "\n",
    "df['active_actions'] = df.apply(extract_high_confidence_actions, axis=1)\n",
    "df['num_active_actions'] = df['active_actions'].apply(len)\n",
    "df['action_names'] = df['active_actions'].apply(lambda x: [a['action'] for a in x])\n",
    "\n",
    "# Get all unique actions\n",
    "all_actions = sorted(set(\n",
    "    action for actions in df['action_names'] for action in actions\n",
    "))\n",
    "\n",
    "print(f\"\\nüß† Cognitive Actions:\")\n",
    "print(f\"   Unique actions: {len(all_actions)}\")\n",
    "print(f\"   Mean per utterance: {df['num_active_actions'].mean():.2f}\")\n",
    "print(f\"   Utterances with actions: {(df['num_active_actions'] > 0).sum()} ({(df['num_active_actions'] > 0).sum()/len(df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Topic Distribution & Characteristics\n",
    "\n",
    "Understanding the composition of therapy topics in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"THERAPY TOPIC ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Topic distribution\n",
    "topic_counts = df['topic'].value_counts()\n",
    "print(f\"\\nüìä Total unique topics: {len(topic_counts)}\")\n",
    "print(f\"\\nTop 20 topics by utterance count:\")\n",
    "print(\"=\"*80)\n",
    "for i, (topic, count) in enumerate(topic_counts.head(20).items(), 1):\n",
    "    pct = count / len(df) * 100\n",
    "    bar = \"‚ñà\" * int(pct)\n",
    "    print(f\"{i:2d}. {topic[:50]:50s} {count:5d} ({pct:5.1f}%) {bar}\")\n",
    "\n",
    "# Group topics into categories\n",
    "def categorize_topic(topic):\n",
    "    \"\"\"Categorize topics into broader themes\"\"\"\n",
    "    topic_lower = str(topic).lower()\n",
    "    \n",
    "    if 'alcohol' in topic_lower:\n",
    "        return 'Substance Use - Alcohol'\n",
    "    elif 'drug' in topic_lower or 'substance' in topic_lower:\n",
    "        return 'Substance Use - Drugs'\n",
    "    elif 'smok' in topic_lower:\n",
    "        return 'Substance Use - Smoking'\n",
    "    elif 'gambl' in topic_lower:\n",
    "        return 'Behavioral - Gambling'\n",
    "    elif 'weight' in topic_lower or 'diet' in topic_lower or 'exercise' in topic_lower or 'activity' in topic_lower:\n",
    "        return 'Health - Weight/Exercise'\n",
    "    elif 'diabetes' in topic_lower or 'asthma' in topic_lower or 'medicine' in topic_lower or 'medical' in topic_lower:\n",
    "        return 'Health - Disease Management'\n",
    "    elif 'recidivism' in topic_lower or 'violence' in topic_lower or 'school' in topic_lower:\n",
    "        return 'Behavioral - Justice/School'\n",
    "    elif 'anxiety' in topic_lower or 'depression' in topic_lower or 'self-harm' in topic_lower:\n",
    "        return 'Mental Health'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "df['topic_category'] = df['topic'].apply(categorize_topic)\n",
    "\n",
    "category_counts = df['topic_category'].value_counts()\n",
    "print(f\"\\nüè∑Ô∏è  Topic Categories:\")\n",
    "print(\"=\"*80)\n",
    "for category, count in category_counts.items():\n",
    "    pct = count / len(df) * 100\n",
    "    print(f\"   {category:40s} {count:5d} ({pct:5.1f}%)\")\n",
    "\n",
    "# MI quality by topic category\n",
    "print(f\"\\nüìà MI Quality by Topic Category:\")\n",
    "print(\"=\"*80)\n",
    "for category in category_counts.index:\n",
    "    category_df = df[df['topic_category'] == category]\n",
    "    high_quality = (category_df['mi_quality'] == 'high').sum()\n",
    "    total = len(category_df['transcript_id'].unique())\n",
    "    if total > 0:\n",
    "        pct = high_quality / total * 100\n",
    "        print(f\"   {category:40s} {high_quality:3d}/{total:3d} high quality ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize topic distribution\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
    "\n",
    "# Plot 1: Top 15 topics\n",
    "top_topics = topic_counts.head(15)\n",
    "axes[0, 0].barh(range(len(top_topics)), top_topics.values, color='steelblue', alpha=0.7)\n",
    "axes[0, 0].set_yticks(range(len(top_topics)))\n",
    "axes[0, 0].set_yticklabels([t[:40] for t in top_topics.index], fontsize=9)\n",
    "axes[0, 0].set_xlabel('Number of Utterances', fontsize=11)\n",
    "axes[0, 0].set_title('Top 15 Therapy Topics by Utterance Count', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0, 0].invert_yaxis()\n",
    "\n",
    "# Plot 2: Topic categories\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(category_counts)))\n",
    "axes[0, 1].pie(category_counts.values, labels=category_counts.index, autopct='%1.1f%%',\n",
    "               colors=colors, startangle=90)\n",
    "axes[0, 1].set_title('Distribution of Topic Categories', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Plot 3: MI quality by category\n",
    "category_quality = []\n",
    "for category in category_counts.index:\n",
    "    category_transcripts = df[df['topic_category'] == category]['transcript_id'].unique()\n",
    "    for tid in category_transcripts:\n",
    "        mi_qual = df[df['transcript_id'] == tid]['mi_quality'].iloc[0]\n",
    "        category_quality.append({'category': category, 'mi_quality': mi_qual})\n",
    "\n",
    "qual_df = pd.DataFrame(category_quality)\n",
    "qual_pivot = qual_df.groupby(['category', 'mi_quality']).size().unstack(fill_value=0)\n",
    "qual_pivot = qual_pivot.div(qual_pivot.sum(axis=1), axis=0) * 100\n",
    "\n",
    "if 'high' in qual_pivot.columns and 'low' in qual_pivot.columns:\n",
    "    x = np.arange(len(qual_pivot))\n",
    "    width = 0.6\n",
    "    \n",
    "    axes[1, 0].barh(x, qual_pivot['high'], width, label='High Quality', color='green', alpha=0.7)\n",
    "    axes[1, 0].barh(x, qual_pivot['low'], width, left=qual_pivot['high'], \n",
    "                    label='Low Quality', color='red', alpha=0.7)\n",
    "    axes[1, 0].set_yticks(x)\n",
    "    axes[1, 0].set_yticklabels(qual_pivot.index, fontsize=9)\n",
    "    axes[1, 0].set_xlabel('Percentage', fontsize=11)\n",
    "    axes[1, 0].set_title('MI Quality Distribution by Topic Category', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "\n",
    "# Plot 4: Cognitive complexity by topic category\n",
    "category_complexity = df.groupby('topic_category')['num_active_actions'].mean().sort_values(ascending=False)\n",
    "axes[1, 1].barh(range(len(category_complexity)), category_complexity.values, color='coral', alpha=0.7)\n",
    "axes[1, 1].set_yticks(range(len(category_complexity)))\n",
    "axes[1, 1].set_yticklabels(category_complexity.index, fontsize=9)\n",
    "axes[1, 1].set_xlabel('Mean Cognitive Actions per Utterance', fontsize=11)\n",
    "axes[1, 1].set_title('Cognitive Complexity by Topic Category', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1, 1].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/comprehensive_1_topic_distribution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/comprehensive_1_topic_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Topic-Specific Cognitive Signatures\n",
    "\n",
    "Analyzing how cognitive patterns differ across therapy topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TOPIC-SPECIFIC COGNITIVE SIGNATURES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze top topics (with enough data)\n",
    "top_10_topics = topic_counts.head(10).index\n",
    "\n",
    "topic_signatures = {}\n",
    "\n",
    "for topic in top_10_topics:\n",
    "    topic_df = df[df['topic'] == topic]\n",
    "    \n",
    "    # Aggregate cognitive actions\n",
    "    action_counts = Counter()\n",
    "    for actions in topic_df['action_names']:\n",
    "        for action in actions:\n",
    "            action_counts[action] += 1\n",
    "    \n",
    "    # Normalize by utterances\n",
    "    total_utterances = len(topic_df)\n",
    "    signature = {action: count / total_utterances \n",
    "                for action, count in action_counts.items()}\n",
    "    \n",
    "    topic_signatures[topic] = {\n",
    "        'utterances': total_utterances,\n",
    "        'transcripts': topic_df['transcript_id'].nunique(),\n",
    "        'mi_quality_high_pct': (topic_df.groupby('transcript_id')['mi_quality'].first() == 'high').sum() / topic_df['transcript_id'].nunique() * 100,\n",
    "        'top_actions': action_counts.most_common(10),\n",
    "        'signature': signature\n",
    "    }\n",
    "\n",
    "print(f\"\\nüìä Analyzed {len(topic_signatures)} topics\\n\")\n",
    "\n",
    "for topic, data in list(topic_signatures.items())[:3]:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"{topic.upper()[:70]}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Utterances: {data['utterances']}, Transcripts: {data['transcripts']}, \"\n",
    "          f\"High Quality: {data['mi_quality_high_pct']:.1f}%\")\n",
    "    print(f\"\\nTop 10 cognitive actions:\")\n",
    "    for action, count in data['top_actions']:\n",
    "        per_utt = count / data['utterances']\n",
    "        print(f\"   {action:35s} {count:5d} ({per_utt:.3f} per utterance)\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare cognitive signatures across topics\n",
    "# Create a heatmap of top topics √ó top actions\n",
    "\n",
    "# Get top 15 actions overall\n",
    "all_action_counts = Counter()\n",
    "for actions in df['action_names']:\n",
    "    for action in actions:\n",
    "        all_action_counts[action] += 1\n",
    "top_15_actions = [a for a, _ in all_action_counts.most_common(15)]\n",
    "\n",
    "# Build matrix\n",
    "topic_action_matrix = []\n",
    "topic_labels = []\n",
    "\n",
    "for topic in top_10_topics:\n",
    "    signature = topic_signatures[topic]['signature']\n",
    "    row = [signature.get(action, 0) for action in top_15_actions]\n",
    "    topic_action_matrix.append(row)\n",
    "    topic_labels.append(topic[:40])  # Truncate for display\n",
    "\n",
    "topic_action_matrix = np.array(topic_action_matrix)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "sns.heatmap(topic_action_matrix, cmap='YlOrRd', annot=True, fmt='.2f',\n",
    "            xticklabels=top_15_actions, yticklabels=topic_labels,\n",
    "            cbar_kws={'label': 'Actions per Utterance'},\n",
    "            ax=ax, linewidths=0.5)\n",
    "ax.set_title('Cognitive Action Signatures Across Top 10 Therapy Topics',\n",
    "             fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_xlabel('Cognitive Action', fontsize=12)\n",
    "ax.set_ylabel('Therapy Topic', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/comprehensive_2_topic_signatures.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/comprehensive_2_topic_signatures.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ Cognitive Synchrony by Topic\n",
    "\n",
    "Measuring therapist-client cognitive alignment across different therapy topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COGNITIVE SYNCHRONY ANALYSIS BY TOPIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create action vectors\n",
    "def create_action_vector(action_names, all_actions):\n",
    "    \"\"\"Create binary vector for cognitive actions\"\"\"\n",
    "    vector = np.zeros(len(all_actions))\n",
    "    for action in action_names:\n",
    "        if action in all_actions:\n",
    "            idx = all_actions.index(action)\n",
    "            vector[idx] = 1\n",
    "    return vector\n",
    "\n",
    "df['action_vector'] = df['action_names'].apply(\n",
    "    lambda x: create_action_vector(x, all_actions)\n",
    ")\n",
    "\n",
    "# Calculate synchrony for each transcript\n",
    "def calculate_synchrony(transcript_df, window_size=5):\n",
    "    \"\"\"Calculate cognitive synchrony between therapist and client\"\"\"\n",
    "    synchrony_scores = []\n",
    "    \n",
    "    for i in range(len(transcript_df) - window_size + 1):\n",
    "        window = transcript_df.iloc[i:i+window_size]\n",
    "        \n",
    "        t_vecs = [v for v, interlocutor in zip(window['action_vector'], window['interlocutor']) \n",
    "                  if interlocutor == 'therapist']\n",
    "        c_vecs = [v for v, interlocutor in zip(window['action_vector'], window['interlocutor']) \n",
    "                  if interlocutor == 'client']\n",
    "        \n",
    "        if len(t_vecs) > 0 and len(c_vecs) > 0:\n",
    "            t_avg = np.mean(t_vecs, axis=0)\n",
    "            c_avg = np.mean(c_vecs, axis=0)\n",
    "            \n",
    "            norm_t = np.linalg.norm(t_avg)\n",
    "            norm_c = np.linalg.norm(c_avg)\n",
    "            \n",
    "            if norm_t > 0 and norm_c > 0:\n",
    "                similarity = np.dot(t_avg, c_avg) / (norm_t * norm_c)\n",
    "                synchrony_scores.append(similarity)\n",
    "    \n",
    "    return np.mean(synchrony_scores) if len(synchrony_scores) > 0 else None\n",
    "\n",
    "# Compute synchrony by topic category\n",
    "category_synchrony = defaultdict(list)\n",
    "\n",
    "for transcript_id in df['transcript_id'].unique():\n",
    "    transcript_df = df[df['transcript_id'] == transcript_id].sort_values('utterance_id')\n",
    "    \n",
    "    if len(transcript_df) < 10:\n",
    "        continue\n",
    "    \n",
    "    sync_score = calculate_synchrony(transcript_df)\n",
    "    \n",
    "    if sync_score is not None:\n",
    "        topic_category = transcript_df['topic_category'].iloc[0]\n",
    "        mi_quality = transcript_df['mi_quality'].iloc[0]\n",
    "        \n",
    "        category_synchrony[topic_category].append({\n",
    "            'synchrony': sync_score,\n",
    "            'mi_quality': mi_quality\n",
    "        })\n",
    "\n",
    "print(f\"\\nüìä Synchrony by Topic Category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in sorted(category_synchrony.keys()):\n",
    "    data = category_synchrony[category]\n",
    "    if len(data) < 3:\n",
    "        continue\n",
    "    \n",
    "    all_sync = [d['synchrony'] for d in data]\n",
    "    high_sync = [d['synchrony'] for d in data if d['mi_quality'] == 'high']\n",
    "    low_sync = [d['synchrony'] for d in data if d['mi_quality'] == 'low']\n",
    "    \n",
    "    print(f\"\\n{category}:\")\n",
    "    print(f\"   Overall: {np.mean(all_sync):.3f} (¬±{np.std(all_sync):.3f}, n={len(all_sync)})\")\n",
    "    if len(high_sync) > 0:\n",
    "        print(f\"   High quality: {np.mean(high_sync):.3f} (¬±{np.std(high_sync):.3f}, n={len(high_sync)})\")\n",
    "    if len(low_sync) > 0:\n",
    "        print(f\"   Low quality:  {np.mean(low_sync):.3f} (¬±{np.std(low_sync):.3f}, n={len(low_sync)})\")\n",
    "    \n",
    "    # Statistical test\n",
    "    if len(high_sync) > 2 and len(low_sync) > 2:\n",
    "        t_stat, p_val = stats.ttest_ind(high_sync, low_sync)\n",
    "        if p_val < 0.05:\n",
    "            print(f\"   ‚úÖ Significant difference (p={p_val:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize synchrony by topic category and quality\n",
    "sync_data = []\n",
    "for category, data in category_synchrony.items():\n",
    "    for item in data:\n",
    "        sync_data.append({\n",
    "            'category': category,\n",
    "            'synchrony': item['synchrony'],\n",
    "            'mi_quality': item['mi_quality']\n",
    "        })\n",
    "\n",
    "sync_df = pd.DataFrame(sync_data)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "# Plot 1: Boxplot by category\n",
    "sns.boxplot(data=sync_df, x='category', y='synchrony', ax=axes[0], palette='Set2')\n",
    "axes[0].set_xlabel('Topic Category', fontsize=11)\n",
    "axes[0].set_ylabel('Cognitive Synchrony', fontsize=11)\n",
    "axes[0].set_title('Cognitive Synchrony by Topic Category', fontsize=12, fontweight='bold')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 2: By category and quality\n",
    "sns.boxplot(data=sync_df, x='category', y='synchrony', hue='mi_quality', \n",
    "            ax=axes[1], palette={'high': 'green', 'low': 'red'})\n",
    "axes[1].set_xlabel('Topic Category', fontsize=11)\n",
    "axes[1].set_ylabel('Cognitive Synchrony', fontsize=11)\n",
    "axes[1].set_title('Cognitive Synchrony by Topic Category and MI Quality', fontsize=12, fontweight='bold')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "axes[1].legend(title='MI Quality', fontsize=10)\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/comprehensive_3_synchrony_by_topic.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/comprehensive_3_synchrony_by_topic.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Change Talk Patterns by Topic\n",
    "\n",
    "Analyzing which therapy topics most effectively elicit client change commitment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHANGE TALK PATTERNS BY TOPIC\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Analyze change talk by topic category\n",
    "client_df = df[df['interlocutor'] == 'client'].copy()\n",
    "client_df = client_df[client_df['client_talk_type'].notna()].copy()\n",
    "\n",
    "topic_change_stats = {}\n",
    "\n",
    "for category in df['topic_category'].unique():\n",
    "    cat_client = client_df[client_df['topic_category'] == category]\n",
    "    \n",
    "    if len(cat_client) < 10:\n",
    "        continue\n",
    "    \n",
    "    change_count = (cat_client['client_talk_type'] == 'change').sum()\n",
    "    sustain_count = (cat_client['client_talk_type'] == 'sustain').sum()\n",
    "    neutral_count = (cat_client['client_talk_type'] == 'neutral').sum()\n",
    "    total = len(cat_client)\n",
    "    \n",
    "    # Cognitive actions in change talk\n",
    "    change_actions = Counter()\n",
    "    for _, row in cat_client[cat_client['client_talk_type'] == 'change'].iterrows():\n",
    "        for action in row['action_names']:\n",
    "            change_actions[action] += 1\n",
    "    \n",
    "    topic_change_stats[category] = {\n",
    "        'total': total,\n",
    "        'change_count': change_count,\n",
    "        'sustain_count': sustain_count,\n",
    "        'neutral_count': neutral_count,\n",
    "        'change_pct': change_count / total * 100,\n",
    "        'sustain_pct': sustain_count / total * 100,\n",
    "        'top_change_actions': change_actions.most_common(5)\n",
    "    }\n",
    "\n",
    "# Sort by change percentage\n",
    "sorted_topics = sorted(topic_change_stats.items(), key=lambda x: x[1]['change_pct'], reverse=True)\n",
    "\n",
    "print(f\"\\nüìä Change Talk by Topic Category:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Category':40s} {'Change':>8s} {'Sustain':>8s} {'Neutral':>8s} {'Total':>8s}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, stats in sorted_topics:\n",
    "    print(f\"{category:40s} {stats['change_pct']:7.1f}% {stats['sustain_pct']:7.1f}% \"\n",
    "          f\"{stats['neutral_count']/stats['total']*100:7.1f}% {stats['total']:7d}\")\n",
    "\n",
    "print(f\"\\nüéØ Top cognitive actions in change talk by topic category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category, stats in sorted_topics[:5]:\n",
    "    print(f\"\\n{category}:\")\n",
    "    for action, count in stats['top_change_actions']:\n",
    "        print(f\"   {action:35s} {count:4d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize change talk patterns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(18, 12))\n",
    "\n",
    "# Plot 1: Stacked bar chart of talk types by category\n",
    "categories = [cat for cat, _ in sorted_topics]\n",
    "change_pcts = [stats['change_pct'] for _, stats in sorted_topics]\n",
    "sustain_pcts = [stats['sustain_pct'] for _, stats in sorted_topics]\n",
    "neutral_pcts = [stats['neutral_count']/stats['total']*100 for _, stats in sorted_topics]\n",
    "\n",
    "x = np.arange(len(categories))\n",
    "axes[0].barh(x, change_pcts, label='Change', color='green', alpha=0.7)\n",
    "axes[0].barh(x, sustain_pcts, left=change_pcts, label='Sustain', color='red', alpha=0.7)\n",
    "axes[0].barh(x, neutral_pcts, left=np.array(change_pcts)+np.array(sustain_pcts), \n",
    "             label='Neutral', color='gray', alpha=0.7)\n",
    "axes[0].set_yticks(x)\n",
    "axes[0].set_yticklabels(categories, fontsize=10)\n",
    "axes[0].set_xlabel('Percentage', fontsize=11)\n",
    "axes[0].set_title('Client Talk Type Distribution by Topic Category', fontsize=12, fontweight='bold')\n",
    "axes[0].legend(fontsize=10)\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# Plot 2: Change talk percentage comparison\n",
    "axes[1].barh(x, change_pcts, color='green', alpha=0.7)\n",
    "axes[1].set_yticks(x)\n",
    "axes[1].set_yticklabels(categories, fontsize=10)\n",
    "axes[1].set_xlabel('Change Talk Percentage', fontsize=11)\n",
    "axes[1].set_title('Change Talk Percentage by Topic Category (Higher = More Change-Oriented)', \n",
    "                  fontsize=12, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Add percentage labels\n",
    "for i, pct in enumerate(change_pcts):\n",
    "    axes[1].text(pct + 0.5, i, f'{pct:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/comprehensive_4_change_talk_by_topic.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/comprehensive_4_change_talk_by_topic.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ MI Techniques √ó Topics √ó Cognitive Actions\n",
    "\n",
    "Three-way analysis: How MI techniques and cognitive patterns interact across different therapy topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"MI TECHNIQUES √ó TOPICS √ó COGNITIVE ACTIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "therapist_df = df[df['interlocutor'] == 'therapist'].copy()\n",
    "\n",
    "# Focus on top 5 topic categories and major MI techniques\n",
    "top_5_categories = df['topic_category'].value_counts().head(5).index\n",
    "\n",
    "# Analyze reflection techniques across topics\n",
    "print(f\"\\nüìä Complex Reflections - Top Cognitive Actions by Topic Category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in top_5_categories:\n",
    "    cat_therapist = therapist_df[(therapist_df['topic_category'] == category) & \n",
    "                                  (therapist_df['reflection_subtype'] == 'complex')]\n",
    "    \n",
    "    if len(cat_therapist) < 5:\n",
    "        continue\n",
    "    \n",
    "    actions = Counter()\n",
    "    for _, row in cat_therapist.iterrows():\n",
    "        for action in row['action_names']:\n",
    "            actions[action] += 1\n",
    "    \n",
    "    print(f\"\\n{category} (n={len(cat_therapist)}):\")\n",
    "    for action, count in actions.most_common(5):\n",
    "        print(f\"   {action:35s} {count:4d} ({count/len(cat_therapist):.2f} per utterance)\")\n",
    "\n",
    "# Compare open questions across topics\n",
    "print(f\"\\n\\nüìä Open Questions - Top Cognitive Actions by Topic Category:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for category in top_5_categories:\n",
    "    cat_therapist = therapist_df[(therapist_df['topic_category'] == category) & \n",
    "                                  (therapist_df['question_subtype'] == 'open')]\n",
    "    \n",
    "    if len(cat_therapist) < 5:\n",
    "        continue\n",
    "    \n",
    "    actions = Counter()\n",
    "    for _, row in cat_therapist.iterrows():\n",
    "        for action in row['action_names']:\n",
    "            actions[action] += 1\n",
    "    \n",
    "    print(f\"\\n{category} (n={len(cat_therapist)}):\")\n",
    "    for action, count in actions.most_common(5):\n",
    "        print(f\"   {action:35s} {count:4d} ({count/len(cat_therapist):.2f} per utterance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6Ô∏è‚É£ Topic-Aware Predictive Modeling\n",
    "\n",
    "Using topic features to predict MI quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TOPIC-AWARE MI QUALITY PREDICTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build features for each transcript including topic information\n",
    "transcript_features = []\n",
    "\n",
    "for transcript_id in df['transcript_id'].unique():\n",
    "    t_df = df[df['transcript_id'] == transcript_id]\n",
    "    t_therapist = t_df[t_df['interlocutor'] == 'therapist']\n",
    "    t_client = t_df[t_df['interlocutor'] == 'client']\n",
    "    \n",
    "    if len(t_therapist) < 5:\n",
    "        continue\n",
    "    \n",
    "    # Topic features\n",
    "    topic_category = t_df['topic_category'].iloc[0]\n",
    "    \n",
    "    # One-hot encode top topic categories\n",
    "    for cat in top_5_categories:\n",
    "        transcript_features_dict = {f'topic_{cat}': 1 if topic_category == cat else 0}\n",
    "    \n",
    "    # Annotation features\n",
    "    reflection_rate = (t_therapist['reflection_exists'] == True).sum() / len(t_therapist)\n",
    "    complex_ref_rate = (t_therapist['reflection_subtype'] == 'complex').sum() / len(t_therapist)\n",
    "    open_q_rate = (t_therapist['question_subtype'] == 'open').sum() / len(t_therapist)\n",
    "    \n",
    "    # Cognitive features\n",
    "    top_10_actions = [a for a, _ in all_action_counts.most_common(10)]\n",
    "    action_counts = Counter()\n",
    "    for actions in t_therapist['action_names']:\n",
    "        for action in actions:\n",
    "            action_counts[action] += 1\n",
    "    \n",
    "    action_features = {f'action_{action}': action_counts.get(action, 0) / len(t_therapist)\n",
    "                      for action in top_10_actions}\n",
    "    \n",
    "    # Client features\n",
    "    client_change_rate = 0\n",
    "    if len(t_client) > 0:\n",
    "        client_with_talk = t_client[t_client['client_talk_type'].notna()]\n",
    "        if len(client_with_talk) > 0:\n",
    "            client_change_rate = (client_with_talk['client_talk_type'] == 'change').sum() / len(client_with_talk)\n",
    "    \n",
    "    # Combine all features\n",
    "    features = {\n",
    "        'transcript_id': transcript_id,\n",
    "        'mi_quality': t_df['mi_quality'].iloc[0],\n",
    "        'reflection_rate': reflection_rate,\n",
    "        'complex_ref_rate': complex_ref_rate,\n",
    "        'open_q_rate': open_q_rate,\n",
    "        'client_change_rate': client_change_rate,\n",
    "        **action_features\n",
    "    }\n",
    "    \n",
    "    # Add topic one-hot features\n",
    "    for cat in top_5_categories:\n",
    "        features[f'topic_{cat}'] = 1 if topic_category == cat else 0\n",
    "    \n",
    "    transcript_features.append(features)\n",
    "\n",
    "features_df = pd.DataFrame(transcript_features)\n",
    "print(f\"\\n‚úÖ Created feature matrix: {len(features_df)} transcripts √ó {len(features_df.columns)-2} features\")\n",
    "\n",
    "# Prepare for modeling\n",
    "X = features_df.drop(['transcript_id', 'mi_quality'], axis=1).values\n",
    "y = (features_df['mi_quality'] == 'high').astype(int).values\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=5)\n",
    "scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "print(f\"\\nüìä Random Forest Cross-Validation Results:\")\n",
    "print(f\"   Mean Accuracy: {scores.mean():.3f} (¬±{scores.std():.3f})\")\n",
    "print(f\"   Range: {scores.min():.3f} - {scores.max():.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "rf.fit(X, y)\n",
    "feature_names = features_df.drop(['transcript_id', 'mi_quality'], axis=1).columns\n",
    "importances = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nüìà Top 15 Most Important Features:\")\n",
    "for i, row in importances.head(15).iterrows():\n",
    "    feat_type = 'Topic' if 'topic_' in row['feature'] else ('Cognitive' if 'action_' in row['feature'] else 'MI Technique')\n",
    "    print(f\"   [{feat_type:12s}] {row['feature']:35s} {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "top_features = importances.head(20)\n",
    "colors = []\n",
    "for feat in top_features['feature']:\n",
    "    if 'topic_' in feat:\n",
    "        colors.append('purple')\n",
    "    elif 'action_' in feat:\n",
    "        colors.append('steelblue')\n",
    "    else:\n",
    "        colors.append('coral')\n",
    "\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color=colors, alpha=0.7)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'].str.replace('action_', 'cog: ').str.replace('topic_', 'topic: '), \n",
    "                   fontsize=9)\n",
    "ax.set_xlabel('Feature Importance', fontsize=12)\n",
    "ax.set_title('Top 20 Features for Predicting MI Quality\\n(Purple=Topic, Blue=Cognitive, Red=MI Technique)',\n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "ax.invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('output/analysis_AnnoMI/comprehensive_5_topic_aware_prediction.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved: output/analysis_AnnoMI/comprehensive_5_topic_aware_prediction.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7Ô∏è‚É£ Summary and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE TOPIC-AWARE ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary = {\n",
    "    'dataset': {\n",
    "        'total_utterances': len(df),\n",
    "        'transcripts': int(df['transcript_id'].nunique()),\n",
    "        'unique_topics': int(df['topic'].nunique()),\n",
    "        'topic_categories': int(df['topic_category'].nunique()),\n",
    "        'cognitive_actions': len(all_actions)\n",
    "    },\n",
    "    'topic_distribution': {\n",
    "        'top_10_topics': dict(topic_counts.head(10)),\n",
    "        'category_distribution': dict(category_counts)\n",
    "    },\n",
    "    'topic_signatures': {\n",
    "        topic: {\n",
    "            'utterances': data['utterances'],\n",
    "            'transcripts': data['transcripts'],\n",
    "            'mi_quality_high_pct': float(data['mi_quality_high_pct']),\n",
    "            'top_5_actions': [a for a, _ in data['top_actions'][:5]]\n",
    "        }\n",
    "        for topic, data in list(topic_signatures.items())[:5]\n",
    "    },\n",
    "    'change_talk_by_topic': {\n",
    "        category: {\n",
    "            'change_pct': float(stats['change_pct']),\n",
    "            'sustain_pct': float(stats['sustain_pct']),\n",
    "            'total': int(stats['total'])\n",
    "        }\n",
    "        for category, stats in sorted_topics\n",
    "    },\n",
    "    'predictive_modeling': {\n",
    "        'cross_val_accuracy_mean': float(scores.mean()),\n",
    "        'cross_val_accuracy_std': float(scores.std()),\n",
    "        'top_15_features': importances.head(15)['feature'].tolist(),\n",
    "        'topic_feature_importance': {\n",
    "            feat: float(imp) for feat, imp in zip(importances['feature'], importances['importance'])\n",
    "            if 'topic_' in feat\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "with open('output/analysis_AnnoMI/comprehensive_topic_analysis_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n‚úÖ Summary saved: output/analysis_AnnoMI/comprehensive_topic_analysis_summary.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE TOPIC-AWARE ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGenerated visualizations:\")\n",
    "print(\"   1. comprehensive_1_topic_distribution.png\")\n",
    "print(\"   2. comprehensive_2_topic_signatures.png\")\n",
    "print(\"   3. comprehensive_3_synchrony_by_topic.png\")\n",
    "print(\"   4. comprehensive_4_change_talk_by_topic.png\")\n",
    "print(\"   5. comprehensive_5_topic_aware_prediction.png\")\n",
    "print(\"\\nData exports:\")\n",
    "print(\"   - comprehensive_topic_analysis_summary.json\")\n",
    "print(\"\\nüîç Key Insights:\")\n",
    "print(f\"   - Analyzed {len(df['topic'].unique())} unique therapy topics across {df['transcript_id'].nunique()} transcripts\")\n",
    "print(f\"   - Topic-aware model accuracy: {scores.mean():.1%}\")\n",
    "print(f\"   - Identified topic-specific cognitive signatures and MI technique patterns\")\n",
    "print(f\"   - Topic category with highest change talk: {sorted_topics[0][0]} ({sorted_topics[0][1]['change_pct']:.1f}%)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
