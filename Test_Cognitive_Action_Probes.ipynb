{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cognitive Action Probe Testing\n",
    "\n",
    "Test your trained cognitive action probes on text.\n",
    "\n",
    "**Key Feature**: Automatically loads the best-performing layer for each action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD GPU detected - configuring ROCm environment variables\n",
      "  HSA_OVERRIDE_GFX_VERSION: 11.0.0\n",
      "  PYTORCH_ROCM_ARCH: gfx1100\n",
      "  Note: Using gfx1100 kernels for gfx1101 GPU\n"
     ]
    }
   ],
   "source": [
    "# IMPORTANT: Set AMD GPU environment variables BEFORE importing torch\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "def detect_amd_gpu():\n",
    "    try:\n",
    "        result = subprocess.run(['lspci'], capture_output=True, text=True, timeout=2)\n",
    "        return 'Advanced Micro Devices' in result.stdout\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if detect_amd_gpu():\n",
    "    print(\"AMD GPU detected - configuring ROCm environment variables\")\n",
    "    # PyTorch is compiled for gfx1100, not gfx1101\n",
    "    # Override to use gfx1100 kernels for gfx1101 GPU (RX 7700/7800 XT)\n",
    "    os.environ[\"HSA_OVERRIDE_GFX_VERSION\"] = \"11.0.0\"\n",
    "    os.environ[\"HIP_VISIBLE_DEVICES\"] = \"0\"\n",
    "    os.environ[\"AMD_SERIALIZE_KERNEL\"] = \"3\"\n",
    "    os.environ[\"TORCH_USE_HIP_DSA\"] = \"1\"\n",
    "    os.environ[\"PYTORCH_ROCM_ARCH\"] = \"gfx1100\"  # Use gfx1100 kernels (closest match)\n",
    "    os.environ[\"PYTORCH_HIP_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "    os.environ[\"HIP_LAUNCH_BLOCKING\"] = \"1\"  # Synchronous execution for better errors\n",
    "    print(f\"  HSA_OVERRIDE_GFX_VERSION: {os.environ['HSA_OVERRIDE_GFX_VERSION']}\")\n",
    "    print(f\"  PYTORCH_ROCM_ARCH: {os.environ['PYTORCH_ROCM_ARCH']}\")\n",
    "    print(\"  Note: Using gfx1100 kernels for gfx1101 GPU\")\n",
    "else:\n",
    "    print(\"No AMD GPU detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMD GPU detected - configuring ROCm environment variables\n",
      "  HSA_OVERRIDE_GFX_VERSION: 11.0.0\n",
      "  PYTORCH_ROCM_ARCH: gfx1100\n",
      "  TORCH_USE_HIP_DSA: 1\n",
      "  HIP_LAUNCH_BLOCKING: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koalacrown/Desktop/Code/Projects/brije/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "# Add src/probes to path\n",
    "sys.path.insert(0, str(Path.cwd() / 'src' / 'probes'))\n",
    "\n",
    "from best_multi_probe_inference import BestMultiProbeInferenceEngine\n",
    "from best_probe_loader import print_performance_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. View Probe Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "PROBE PERFORMANCE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Top 45 Best Performing Probes (by AUC):\n",
      "----------------------------------------------------------------------\n",
      " 1. suspending_judgment                 Layer 21  AUC: 0.9995  F1: 0.9347\n",
      " 2. accepting                           Layer 22  AUC: 0.9971  F1: 0.8601\n",
      " 3. zooming_in                          Layer 23  AUC: 0.9961  F1: 0.8421\n",
      " 4. analogical_thinking                 Layer 22  AUC: 0.9957  F1: 0.8466\n",
      " 5. counterfactual_reasoning            Layer 22  AUC: 0.9937  F1: 0.8912\n",
      " 6. zooming_out                         Layer 23  AUC: 0.9934  F1: 0.9200\n",
      " 7. divergent_thinking                  Layer 21  AUC: 0.9897  F1: 0.8168\n",
      " 8. attentional_deployment              Layer 21  AUC: 0.9892  F1: 0.7475\n",
      " 9. concretizing                        Layer 22  AUC: 0.9882  F1: 0.7957\n",
      "10. perspective_taking                  Layer 21  AUC: 0.9846  F1: 0.6480\n",
      "11. distinguishing                      Layer 21  AUC: 0.9821  F1: 0.6947\n",
      "12. hypothesis_generation               Layer 25  AUC: 0.9764  F1: 0.5464\n",
      "13. convergent_thinking                 Layer 22  AUC: 0.9742  F1: 0.6235\n",
      "14. abstracting                         Layer 21  AUC: 0.9727  F1: 0.6030\n",
      "15. emotional_reappraisal               Layer 22  AUC: 0.9720  F1: 0.5813\n",
      "16. emotion_valuing                     Layer 22  AUC: 0.9715  F1: 0.7609\n",
      "17. remembering                         Layer 22  AUC: 0.9695  F1: 0.6734\n",
      "18. connecting                          Layer 21  AUC: 0.9692  F1: 0.6286\n",
      "19. reconsidering                       Layer 24  AUC: 0.9678  F1: 0.4884\n",
      "20. meta_awareness                      Layer 21  AUC: 0.9673  F1: 0.5600\n",
      "21. situation_selection                 Layer 23  AUC: 0.9646  F1: 0.6474\n",
      "22. emotion_facilitation                Layer 22  AUC: 0.9638  F1: 0.6917\n",
      "23. pattern_recognition                 Layer 22  AUC: 0.9606  F1: 0.4371\n",
      "24. response_modulation                 Layer 24  AUC: 0.9576  F1: 0.5287\n",
      "25. questioning                         Layer 26  AUC: 0.9550  F1: 0.4536\n",
      "26. reframing                           Layer 24  AUC: 0.9526  F1: 0.4773\n",
      "27. emotion_organizing                  Layer 26  AUC: 0.9517  F1: 0.6633\n",
      "28. emotion_receiving                   Layer 22  AUC: 0.9515  F1: 0.3902\n",
      "29. analyzing                           Layer 23  AUC: 0.9501  F1: 0.6018\n",
      "30. emotion_management                  Layer 21  AUC: 0.9492  F1: 0.2815\n",
      "31. self_questioning                    Layer 22  AUC: 0.9485  F1: 0.3625\n",
      "32. metacognitive_monitoring            Layer 21  AUC: 0.9438  F1: 0.5667\n",
      "33. situation_modification              Layer 23  AUC: 0.9425  F1: 0.4796\n",
      "34. updating_beliefs                    Layer 25  AUC: 0.9348  F1: 0.3546\n",
      "35. noticing                            Layer 23  AUC: 0.9322  F1: 0.4372\n",
      "36. emotion_perception                  Layer 22  AUC: 0.9292  F1: 0.4382\n",
      "37. metacognitive_regulation            Layer 22  AUC: 0.9288  F1: 0.2836\n",
      "38. emotion_characterizing              Layer 24  AUC: 0.9266  F1: 0.3875\n",
      "39. creating                            Layer 21  AUC: 0.9194  F1: 0.3380\n",
      "40. applying                            Layer 22  AUC: 0.8997  F1: 0.2897\n",
      "41. evaluating                          Layer 23  AUC: 0.8913  F1: 0.3152\n",
      "42. cognition_awareness                 Layer 22  AUC: 0.8822  F1: 0.1765\n",
      "43. understanding                       Layer 22  AUC: 0.8680  F1: 0.1920\n",
      "44. emotion_understanding               Layer 23  AUC: 0.8672  F1: 0.2236\n",
      "45. emotion_responding                  Layer 26  AUC: 0.8278  F1: 0.0333\n",
      "\n",
      "Bottom 45 Performers (by AUC):\n",
      "----------------------------------------------------------------------\n",
      " 1. emotion_responding                  Layer 26  AUC: 0.8278  F1: 0.0333\n",
      " 2. emotion_understanding               Layer 23  AUC: 0.8672  F1: 0.2236\n",
      " 3. understanding                       Layer 22  AUC: 0.8680  F1: 0.1920\n",
      " 4. cognition_awareness                 Layer 22  AUC: 0.8822  F1: 0.1765\n",
      " 5. evaluating                          Layer 23  AUC: 0.8913  F1: 0.3152\n",
      " 6. applying                            Layer 22  AUC: 0.8997  F1: 0.2897\n",
      " 7. creating                            Layer 21  AUC: 0.9194  F1: 0.3380\n",
      " 8. emotion_characterizing              Layer 24  AUC: 0.9266  F1: 0.3875\n",
      " 9. metacognitive_regulation            Layer 22  AUC: 0.9288  F1: 0.2836\n",
      "10. emotion_perception                  Layer 22  AUC: 0.9292  F1: 0.4382\n",
      "11. noticing                            Layer 23  AUC: 0.9322  F1: 0.4372\n",
      "12. updating_beliefs                    Layer 25  AUC: 0.9348  F1: 0.3546\n",
      "13. situation_modification              Layer 23  AUC: 0.9425  F1: 0.4796\n",
      "14. metacognitive_monitoring            Layer 21  AUC: 0.9438  F1: 0.5667\n",
      "15. self_questioning                    Layer 22  AUC: 0.9485  F1: 0.3625\n",
      "16. emotion_management                  Layer 21  AUC: 0.9492  F1: 0.2815\n",
      "17. analyzing                           Layer 23  AUC: 0.9501  F1: 0.6018\n",
      "18. emotion_receiving                   Layer 22  AUC: 0.9515  F1: 0.3902\n",
      "19. emotion_organizing                  Layer 26  AUC: 0.9517  F1: 0.6633\n",
      "20. reframing                           Layer 24  AUC: 0.9526  F1: 0.4773\n",
      "21. questioning                         Layer 26  AUC: 0.9550  F1: 0.4536\n",
      "22. response_modulation                 Layer 24  AUC: 0.9576  F1: 0.5287\n",
      "23. pattern_recognition                 Layer 22  AUC: 0.9606  F1: 0.4371\n",
      "24. emotion_facilitation                Layer 22  AUC: 0.9638  F1: 0.6917\n",
      "25. situation_selection                 Layer 23  AUC: 0.9646  F1: 0.6474\n",
      "26. meta_awareness                      Layer 21  AUC: 0.9673  F1: 0.5600\n",
      "27. reconsidering                       Layer 24  AUC: 0.9678  F1: 0.4884\n",
      "28. connecting                          Layer 21  AUC: 0.9692  F1: 0.6286\n",
      "29. remembering                         Layer 22  AUC: 0.9695  F1: 0.6734\n",
      "30. emotion_valuing                     Layer 22  AUC: 0.9715  F1: 0.7609\n",
      "31. emotional_reappraisal               Layer 22  AUC: 0.9720  F1: 0.5813\n",
      "32. abstracting                         Layer 21  AUC: 0.9727  F1: 0.6030\n",
      "33. convergent_thinking                 Layer 22  AUC: 0.9742  F1: 0.6235\n",
      "34. hypothesis_generation               Layer 25  AUC: 0.9764  F1: 0.5464\n",
      "35. distinguishing                      Layer 21  AUC: 0.9821  F1: 0.6947\n",
      "36. perspective_taking                  Layer 21  AUC: 0.9846  F1: 0.6480\n",
      "37. concretizing                        Layer 22  AUC: 0.9882  F1: 0.7957\n",
      "38. attentional_deployment              Layer 21  AUC: 0.9892  F1: 0.7475\n",
      "39. divergent_thinking                  Layer 21  AUC: 0.9897  F1: 0.8168\n",
      "40. zooming_out                         Layer 23  AUC: 0.9934  F1: 0.9200\n",
      "41. counterfactual_reasoning            Layer 22  AUC: 0.9937  F1: 0.8912\n",
      "42. analogical_thinking                 Layer 22  AUC: 0.9957  F1: 0.8466\n",
      "43. zooming_in                          Layer 23  AUC: 0.9961  F1: 0.8421\n",
      "44. accepting                           Layer 22  AUC: 0.9971  F1: 0.8601\n",
      "45. suspending_judgment                 Layer 21  AUC: 0.9995  F1: 0.9347\n",
      "\n",
      "Most Layer-Sensitive Actions (vary most across layers):\n",
      "----------------------------------------------------------------------\n",
      " 1. analyzing                           Sensitivity: 0.4263  Best Layer: 23\n",
      " 2. meta_awareness                      Sensitivity: 0.3347  Best Layer: 21\n",
      " 3. emotional_reappraisal               Sensitivity: 0.2920  Best Layer: 22\n",
      " 4. evaluating                          Sensitivity: 0.2635  Best Layer: 23\n",
      " 5. situation_modification              Sensitivity: 0.2504  Best Layer: 23\n",
      " 6. emotion_facilitation                Sensitivity: 0.2497  Best Layer: 22\n",
      " 7. hypothesis_generation               Sensitivity: 0.2449  Best Layer: 25\n",
      " 8. updating_beliefs                    Sensitivity: 0.2418  Best Layer: 25\n",
      " 9. reframing                           Sensitivity: 0.2290  Best Layer: 24\n",
      "10. emotion_management                  Sensitivity: 0.2172  Best Layer: 21\n",
      "11. applying                            Sensitivity: 0.2051  Best Layer: 22\n",
      "12. noticing                            Sensitivity: 0.1946  Best Layer: 23\n",
      "13. creating                            Sensitivity: 0.1784  Best Layer: 21\n",
      "14. situation_selection                 Sensitivity: 0.1672  Best Layer: 23\n",
      "15. remembering                         Sensitivity: 0.1665  Best Layer: 22\n",
      "16. concretizing                        Sensitivity: 0.1644  Best Layer: 22\n",
      "17. analogical_thinking                 Sensitivity: 0.1627  Best Layer: 22\n",
      "18. emotion_responding                  Sensitivity: 0.1568  Best Layer: 26\n",
      "19. metacognitive_monitoring            Sensitivity: 0.1426  Best Layer: 21\n",
      "20. emotion_organizing                  Sensitivity: 0.1396  Best Layer: 26\n",
      "21. attentional_deployment              Sensitivity: 0.1378  Best Layer: 21\n",
      "22. questioning                         Sensitivity: 0.1377  Best Layer: 26\n",
      "23. emotion_understanding               Sensitivity: 0.1373  Best Layer: 23\n",
      "24. self_questioning                    Sensitivity: 0.1364  Best Layer: 22\n",
      "25. emotion_receiving                   Sensitivity: 0.1269  Best Layer: 22\n",
      "26. cognition_awareness                 Sensitivity: 0.1218  Best Layer: 22\n",
      "27. perspective_taking                  Sensitivity: 0.1163  Best Layer: 21\n",
      "28. metacognitive_regulation            Sensitivity: 0.1105  Best Layer: 22\n",
      "29. convergent_thinking                 Sensitivity: 0.1093  Best Layer: 22\n",
      "30. reconsidering                       Sensitivity: 0.1092  Best Layer: 24\n",
      "31. connecting                          Sensitivity: 0.1087  Best Layer: 21\n",
      "32. distinguishing                      Sensitivity: 0.0977  Best Layer: 21\n",
      "33. response_modulation                 Sensitivity: 0.0884  Best Layer: 24\n",
      "34. pattern_recognition                 Sensitivity: 0.0838  Best Layer: 22\n",
      "35. emotion_perception                  Sensitivity: 0.0824  Best Layer: 22\n",
      "36. accepting                           Sensitivity: 0.0797  Best Layer: 22\n",
      "37. understanding                       Sensitivity: 0.0742  Best Layer: 22\n",
      "38. zooming_in                          Sensitivity: 0.0727  Best Layer: 23\n",
      "39. abstracting                         Sensitivity: 0.0704  Best Layer: 21\n",
      "40. divergent_thinking                  Sensitivity: 0.0672  Best Layer: 21\n",
      "41. emotion_characterizing              Sensitivity: 0.0636  Best Layer: 24\n",
      "42. emotion_valuing                     Sensitivity: 0.0508  Best Layer: 22\n",
      "43. counterfactual_reasoning            Sensitivity: 0.0415  Best Layer: 22\n",
      "44. zooming_out                         Sensitivity: 0.0365  Best Layer: 23\n",
      "45. suspending_judgment                 Sensitivity: 0.0236  Best Layer: 21\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "PROBES_BASE_DIR = Path('data/probes_binary')\n",
    "print_performance_summary(PROBES_BASE_DIR, top_n=45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Inference Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading probes...\n",
      "Initializing BestMultiProbeInferenceEngine...\n",
      "  Probes base dir: data/probes_binary\n",
      "  Model: google/gemma-3-4b-it\n",
      "  Device: cuda\n",
      "\n",
      "Loading best probes for 45 actions...\n",
      "Device: cuda\n",
      "\n",
      "Loaded probe from data/probes_binary/layer_21/probe_abstracting.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_accepting.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_analogical_thinking.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_analyzing.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_applying.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_attentional_deployment.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_cognition_awareness.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_concretizing.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_connecting.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_convergent_thinking.pth\n",
      "  Loaded 10 probes...\n",
      "Loaded probe from data/probes_binary/layer_22/probe_counterfactual_reasoning.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_creating.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_distinguishing.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_divergent_thinking.pth\n",
      "Loaded probe from data/probes_binary/layer_24/probe_emotion_characterizing.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_emotion_facilitation.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_emotion_management.pth\n",
      "Loaded probe from data/probes_binary/layer_26/probe_emotion_organizing.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_emotion_perception.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_emotion_receiving.pth\n",
      "  Loaded 20 probes...\n",
      "Loaded probe from data/probes_binary/layer_26/probe_emotion_responding.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_emotion_understanding.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_emotion_valuing.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_emotional_reappraisal.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_evaluating.pth\n",
      "Loaded probe from data/probes_binary/layer_25/probe_hypothesis_generation.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_meta_awareness.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_metacognitive_monitoring.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_metacognitive_regulation.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_noticing.pth\n",
      "  Loaded 30 probes...\n",
      "Loaded probe from data/probes_binary/layer_22/probe_pattern_recognition.pth\n",
      "Loaded probe from data/probes_binary/layer_21/probe_perspective_taking.pth\n",
      "Loaded probe from data/probes_binary/layer_26/probe_questioning.pth\n",
      "Loaded probe from data/probes_binary/layer_24/probe_reconsidering.pth\n",
      "Loaded probe from data/probes_binary/layer_24/probe_reframing.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_remembering.pth\n",
      "Loaded probe from data/probes_binary/layer_24/probe_response_modulation.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_self_questioning.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_situation_modification.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_situation_selection.pth\n",
      "  Loaded 40 probes...\n",
      "Loaded probe from data/probes_binary/layer_21/probe_suspending_judgment.pth\n",
      "Loaded probe from data/probes_binary/layer_22/probe_understanding.pth\n",
      "Loaded probe from data/probes_binary/layer_25/probe_updating_beliefs.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_zooming_in.pth\n",
      "Loaded probe from data/probes_binary/layer_23/probe_zooming_out.pth\n",
      "\n",
      "✓ Successfully loaded 45 probes\n",
      "\n",
      "Layer distribution:\n",
      "  Layer 21: 11 probes █████\n",
      "  Layer 22: 17 probes ████████\n",
      "  Layer 23:  8 probes ████\n",
      "  Layer 24:  4 probes ██\n",
      "  Layer 25:  2 probes █\n",
      "  Layer 26:  3 probes █\n",
      "\n",
      "✓ Will extract activations from 6 layers: [21, 22, 23, 24, 25, 26]\n",
      "\n",
      "Loading language model: google/gemma-3-4b-it...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/koalacrown/Desktop/Code/Projects/brije/src/probes/probe_models.py:419: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(load_path, map_location=device)\n",
      "/home/koalacrown/Desktop/Code/Projects/brije/.venv/lib/python3.12/site-packages/torch/_utils.py:89: UserWarning: expandable_segments not supported on this platform (Triggered internally at ../c10/hip/HIPAllocatorConfig.h:29.)\n",
      "  untyped_storage = torch.UntypedStorage(self.size(), device=device)\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected vision-language model. Loading text-only (skipping vision tower)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Initialized with 45 probes across 6 layers\n",
      "\n",
      "Ready!\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'google/gemma-3-4b-it'\n",
    "\n",
    "print('Loading probes...')\n",
    "engine = BestMultiProbeInferenceEngine(\n",
    "    probes_base_dir=PROBES_BASE_DIR,\n",
    "    model_name=MODEL_NAME\n",
    ")\n",
    "print('Ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test on Your Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected Cognitive Actions:\n",
      "======================================================================\n",
      "\n",
      "Sentence: After receiving feedback, I began reconsidering my approach.\n",
      "I realized I had been making assumptions without fully understanding the constraints.\n",
      "✓  1. updating_beliefs               100.0%\n",
      "      (Layer 25, AUC: 0.935)\n",
      "\n",
      "Sentence: After receiving feedback, I began reconsidering my approach.\n",
      "I realized I had been making assumptions without fully understanding the constraints.\n",
      "✓  2. metacognitive_regulation        35.5%\n",
      "      (Layer 22, AUC: 0.929)\n"
     ]
    }
   ],
   "source": [
    "text = '''After receiving feedback, I began reconsidering my approach.\n",
    "I realized I had been making assumptions without fully understanding the constraints.'''\n",
    "\n",
    "predictions = engine.predict(text, top_k=10, threshold=0.1)\n",
    "\n",
    "print('Detected Cognitive Actions:')\n",
    "print('='*70)\n",
    "for i, pred in enumerate(predictions, 1):\n",
    "    marker = '✓' if pred.is_active else '○'\n",
    "    print(f\"\\nSentence: {text}\")\n",
    "    print(f\"{marker} {i:2d}. {pred.action_name:30s} {pred.confidence:6.1%}\")\n",
    "    print(f\"      (Layer {pred.layer}, AUC: {pred.auc:.3f})\")\n",
    "    if hasattr(pred, 'beliefs') and pred.beliefs:\n",
    "        print(\"      Beliefs:\")\n",
    "        for belief in pred.beliefs:\n",
    "            print(f\"        - {belief}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Two Texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEXT 1: Analyzing the data to identify patterns and trends.\n",
      "\n",
      "Top actions:\n",
      "  - emotion_receiving              0.0%\n",
      "  - emotion_responding             0.0%\n",
      "  - abstracting                    0.0%\n",
      "  - applying                       0.0%\n",
      "  - evaluating                     0.0%\n",
      "\n",
      "======================================================================\n",
      "TEXT 2: Brainstorming creative solutions to the problem.\n",
      "\n",
      "Top actions:\n",
      "  - divergent_thinking             99.6%\n",
      "  - emotion_responding             0.0%\n",
      "  - applying                       0.0%\n",
      "  - creating                       0.0%\n",
      "  - metacognitive_regulation       0.0%\n"
     ]
    }
   ],
   "source": [
    "text1 = 'Analyzing the data to identify patterns and trends.'\n",
    "text2 = 'Brainstorming creative solutions to the problem.'\n",
    "\n",
    "comparison = engine.compare_texts(text1, text2, top_k=5)\n",
    "\n",
    "print('TEXT 1:', text1)\n",
    "print('\\nTop actions:')\n",
    "for action, conf in comparison['text1_top_actions'][:5]:\n",
    "    print(f'  - {action:30s} {conf:.1%}')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('TEXT 2:', text2)\n",
    "print('\\nTop actions:')\n",
    "for action, conf in comparison['text2_top_actions'][:5]:\n",
    "    print(f'  - {action:30s} {conf:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Batch Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. The quarterly numbers look... interesting. Revenue up 12%, but margins down 3%. Customer acquisition costs rising while retention rates plateau. Something doesn't add up here.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "   2. zooming_in                     0.31738281% (L23)\n",
      "\n",
      "2. What if we completely flipped the script? Instead of chasing the same customers everyone else wants, what about targeting the segment nobody's paying attention to?\n",
      "   1. divergent_thinking             100.00000000% (L21)\n",
      "   2. creating                       0.17013550% (L21)\n",
      "\n",
      "3. Last quarter's campaign... we spent $50K on social media ads, got 200 signups, but only 15 converted. That's a 7.5% conversion rate. Industry average is 12%. We're bleeding money.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "   2. evaluating                     8.64257812% (L23)\n",
      "\n",
      "4. That client meeting keeps replaying in my head. Sarah said 'the integration feels clunky' and I brushed it off. Now three clients have mentioned the same thing. I should have listened.\n",
      "   1. counterfactual_reasoning       3.85742188% (L22)\n",
      "   2. noticing                       1.09863281% (L23)\n",
      "\n",
      "5. My brain is scattered. Need to organize this mess: finish the Q4 budget review, prep for tomorrow's board meeting, and draft the hiring plan for next quarter. Otherwise I'll forget something crucial.\n",
      "   1. remembering                    0.43334961% (L22)\n",
      "\n",
      "6. Where are we on the Johnson account? Last I heard, legal was reviewing the contract. Marketing said they'd have the campaign ready by Friday. Finance needs the numbers by end of week. Everything's converging.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "\n",
      "7. Client feedback from Project Alpha, user research from Beta, and market analysis from Gamma. All pointing in different directions. There's a pattern here I'm not seeing yet.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "   2. zooming_out                    100.00000000% (L23)\n",
      "\n",
      "8. Option A: expand to Europe, higher risk but potentially 40% revenue growth. Option B: focus on domestic market, safer but maybe 15% growth. Both have merit. Both have downsides.\n",
      "   1. divergent_thinking             100.00000000% (L21)\n",
      "\n",
      "9. The website keeps crashing during peak hours. Server logs show increased traffic, but that shouldn't cause failures. There's something else going on.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "\n",
      "10. The interns are lost. I'm throwing terms like 'conversion funnel' and 'attribution modeling' at them. They need the basics first - what we're trying to achieve and why.\n",
      "   1. applying                       100.00000000% (L22)\n",
      "   2. understanding                  99.21875000% (L22)\n",
      "   3. abstracting                    4.61425781% (L21)\n",
      "   4. noticing                       0.42114258% (L23)\n",
      "\n",
      "11. This product launch strategy feels incomplete. Maybe I should bounce ideas off the team. Fresh perspectives could reveal blind spots I'm missing.\n",
      "   1. questioning                    99.60937500% (L26)\n",
      "\n",
      "12. I've been assuming our target demographic is 25-35 year olds. But what if that's wrong? What if I'm basing decisions on outdated assumptions?\n",
      "   1. questioning                    100.00000000% (L26)\n",
      "   2. self_questioning               100.00000000% (L22)\n",
      "\n",
      "13. This dashboard is overwhelming. Revenue charts, user engagement metrics, conversion rates, churn analysis. Too much noise. Need to focus on what actually matters.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "   2. zooming_in                     100.00000000% (L23)\n",
      "\n",
      "14. The current approach isn't working. Users aren't engaging with the new feature. Maybe we need to pivot. Try a different angle entirely.\n",
      "   1. questioning                    100.00000000% (L26)\n",
      "   2. reconsidering                  84.76562500% (L24)\n",
      "   3. metacognitive_regulation       0.18081665% (L22)\n",
      "\n",
      "15. These customer segments look similar on paper - both tech-savvy, both high income. But their behavior patterns are completely different. What am I missing?\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "\n",
      "16. If we launch in Q2 instead of Q1, we'd have more time for testing. But competitors might beat us to market. If we rush Q1, we risk bugs. If we wait, we risk irrelevance.\n",
      "   1. counterfactual_reasoning       100.00000000% (L22)\n",
      "\n",
      "17. The manager's email was vague: 'streamline the process.' What does that mean exactly? Reduce steps? Automate tasks? Cut costs? Need to clarify before I act.\n",
      "   1. questioning                    100.00000000% (L26)\n",
      "\n",
      "18. I'm recommending we increase the marketing budget by 30%. But why? Because last quarter's campaign worked? Because competitors are spending more? Need solid reasoning.\n",
      "   1. evaluating                     0.00000000% (L23)\n",
      "   2. applying                       0.00000000% (L22)\n",
      "   3. understanding                  0.00000000% (L22)\n",
      "   4. creating                       0.00000000% (L21)\n",
      "   5. noticing                       0.00000000% (L23)\n",
      "   6. abstracting                    0.00000000% (L21)\n",
      "   7. divergent_thinking             0.00000000% (L21)\n",
      "   8. convergent_thinking            0.00000000% (L22)\n",
      "   9. analyzing                      0.00000000% (L23)\n",
      "   10. cognition_awareness            0.00000000% (L22)\n",
      "\n",
      "19. Sally flagged that our pricing model doesn't account for seasonal fluctuations. She's right. Our revenue projections assume steady demand year-round. That's unrealistic.\n",
      "   1. questioning                    100.00000000% (L26)\n",
      "\n",
      "20. The project timeline is chaotic. Phase 1 should inform Phase 2, which should inform Phase 3. But everything's happening simultaneously. Need to map out dependencies.\n",
      "   1. creating                       0.00000000% (L21)\n",
      "   2. noticing                       0.00000000% (L23)\n",
      "   3. remembering                    0.00000000% (L22)\n",
      "   4. abstracting                    0.00000000% (L21)\n",
      "   5. understanding                  0.00000000% (L22)\n",
      "   6. analyzing                      0.00000000% (L23)\n",
      "   7. meta_awareness                 0.00000000% (L21)\n",
      "   8. cognition_awareness            0.00000000% (L22)\n",
      "   9. pattern_recognition            0.00000000% (L22)\n",
      "   10. metacognitive_regulation       0.00000000% (L22)\n",
      "\n",
      "21. This market research feels biased. The methodology seems sound, but the conclusions feel predetermined. Like they found what they were looking for.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "\n",
      "22. The correlation between social media engagement and sales is strong. But that doesn't mean social media causes sales. Could be reverse causation, or a third factor entirely.\n",
      "   1. distinguishing                 100.00000000% (L21)\n",
      "   2. abstracting                    2.19726562% (L21)\n",
      "\n",
      "23. Let's test this hypothesis: if our target users really want this feature, they'll use it within the first week. If not, we'll know it's not solving a real problem.\n",
      "   1. concretizing                   100.00000000% (L22)\n",
      "\n",
      "24. Both theories explain the data well. Theory A focuses on user behavior, Theory B on market conditions. They're not mutually exclusive, but they emphasize different factors.\n",
      "   1. noticing                       0.00014603% (L23)\n",
      "   2. distinguishing                 0.00000004% (L21)\n",
      "   3. applying                       0.00000000% (L22)\n",
      "   4. creating                       0.00000000% (L21)\n",
      "   5. hypothesis_generation          0.00000000% (L25)\n",
      "   6. evaluating                     0.00000000% (L23)\n",
      "   7. emotion_organizing             0.00000000% (L26)\n",
      "   8. abstracting                    0.00000000% (L21)\n",
      "   9. connecting                     0.00000000% (L21)\n",
      "   10. remembering                    0.00000000% (L22)\n",
      "\n",
      "25. This industry report cites impressive statistics, but I don't recognize the research firm. Need to verify their credibility before I base any decisions on their findings.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "\n",
      "26. Today's priorities are overwhelming. The client presentation, the budget review, the team meeting, the product demo. Can't do everything. Need to pick what's truly urgent.\n",
      "   1. convergent_thinking            0.01025200% (L22)\n",
      "\n",
      "27. The alternative approach might be better. Current method is familiar, but the new one could be more efficient. Should we compare them side by side before deciding?\n",
      "   1. counterfactual_reasoning       100.00000000% (L22)\n",
      "   2. questioning                    100.00000000% (L26)\n",
      "   3. metacognitive_regulation       1.63574219% (L22)\n",
      "   4. self_questioning               0.03795624% (L22)\n",
      "\n",
      "28. Let me explain this simply: we're not making money because we're spending more to acquire customers than we earn from them. Like buying a $10 item for $15.\n",
      "   1. analogical_thinking            100.00000000% (L22)\n",
      "   2. distinguishing                 59.37500000% (L21)\n",
      "   3. understanding                  24.21875000% (L22)\n",
      "   4. abstracting                    0.10299683% (L21)\n",
      "\n",
      "29. I remember being overwhelmed by all these metrics and KPIs when I started. Jamie looks lost in the same way. Maybe I can help them understand what actually matters.\n",
      "   1. applying                       24.80468750% (L22)\n",
      "\n",
      "30. Sitting here watching people interact with our app. Some scroll quickly, others pause and tap. Some get frustrated and leave. Others seem to find what they need. Patterns emerging.\n",
      "   1. noticing                       100.00000000% (L23)\n",
      "   2. meta_awareness                 0.02460480% (L21)\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"The quarterly numbers look... interesting. Revenue up 12%, but margins down 3%. Customer acquisition costs rising while retention rates plateau. Something doesn't add up here.\",\n",
    "    \"What if we completely flipped the script? Instead of chasing the same customers everyone else wants, what about targeting the segment nobody's paying attention to?\",\n",
    "    \"Last quarter's campaign... we spent $50K on social media ads, got 200 signups, but only 15 converted. That's a 7.5% conversion rate. Industry average is 12%. We're bleeding money.\",\n",
    "    \"That client meeting keeps replaying in my head. Sarah said 'the integration feels clunky' and I brushed it off. Now three clients have mentioned the same thing. I should have listened.\",\n",
    "    \"My brain is scattered. Need to organize this mess: finish the Q4 budget review, prep for tomorrow's board meeting, and draft the hiring plan for next quarter. Otherwise I'll forget something crucial.\",\n",
    "    \"Where are we on the Johnson account? Last I heard, legal was reviewing the contract. Marketing said they'd have the campaign ready by Friday. Finance needs the numbers by end of week. Everything's converging.\",\n",
    "    \"Client feedback from Project Alpha, user research from Beta, and market analysis from Gamma. All pointing in different directions. There's a pattern here I'm not seeing yet.\",\n",
    "    \"Option A: expand to Europe, higher risk but potentially 40% revenue growth. Option B: focus on domestic market, safer but maybe 15% growth. Both have merit. Both have downsides.\",\n",
    "    \"The website keeps crashing during peak hours. Server logs show increased traffic, but that shouldn't cause failures. There's something else going on.\",\n",
    "    \"The interns are lost. I'm throwing terms like 'conversion funnel' and 'attribution modeling' at them. They need the basics first - what we're trying to achieve and why.\",\n",
    "    \"This product launch strategy feels incomplete. Maybe I should bounce ideas off the team. Fresh perspectives could reveal blind spots I'm missing.\",\n",
    "    \"I've been assuming our target demographic is 25-35 year olds. But what if that's wrong? What if I'm basing decisions on outdated assumptions?\",\n",
    "    \"This dashboard is overwhelming. Revenue charts, user engagement metrics, conversion rates, churn analysis. Too much noise. Need to focus on what actually matters.\",\n",
    "    \"The current approach isn't working. Users aren't engaging with the new feature. Maybe we need to pivot. Try a different angle entirely.\",\n",
    "    \"These customer segments look similar on paper - both tech-savvy, both high income. But their behavior patterns are completely different. What am I missing?\",\n",
    "    \"If we launch in Q2 instead of Q1, we'd have more time for testing. But competitors might beat us to market. If we rush Q1, we risk bugs. If we wait, we risk irrelevance.\",\n",
    "    \"The manager's email was vague: 'streamline the process.' What does that mean exactly? Reduce steps? Automate tasks? Cut costs? Need to clarify before I act.\",\n",
    "    \"I'm recommending we increase the marketing budget by 30%. But why? Because last quarter's campaign worked? Because competitors are spending more? Need solid reasoning.\",\n",
    "    \"Sally flagged that our pricing model doesn't account for seasonal fluctuations. She's right. Our revenue projections assume steady demand year-round. That's unrealistic.\",\n",
    "    \"The project timeline is chaotic. Phase 1 should inform Phase 2, which should inform Phase 3. But everything's happening simultaneously. Need to map out dependencies.\",\n",
    "    \"This market research feels biased. The methodology seems sound, but the conclusions feel predetermined. Like they found what they were looking for.\",\n",
    "    \"The correlation between social media engagement and sales is strong. But that doesn't mean social media causes sales. Could be reverse causation, or a third factor entirely.\",\n",
    "    \"Let's test this hypothesis: if our target users really want this feature, they'll use it within the first week. If not, we'll know it's not solving a real problem.\",\n",
    "    \"Both theories explain the data well. Theory A focuses on user behavior, Theory B on market conditions. They're not mutually exclusive, but they emphasize different factors.\",\n",
    "    \"This industry report cites impressive statistics, but I don't recognize the research firm. Need to verify their credibility before I base any decisions on their findings.\",\n",
    "    \"Today's priorities are overwhelming. The client presentation, the budget review, the team meeting, the product demo. Can't do everything. Need to pick what's truly urgent.\",\n",
    "    \"The alternative approach might be better. Current method is familiar, but the new one could be more efficient. Should we compare them side by side before deciding?\",\n",
    "    \"Let me explain this simply: we're not making money because we're spending more to acquire customers than we earn from them. Like buying a $10 item for $15.\",\n",
    "    \"I remember being overwhelmed by all these metrics and KPIs when I started. Jamie looks lost in the same way. Maybe I can help them understand what actually matters.\",\n",
    "    \"Sitting here watching people interact with our app. Some scroll quickly, others pause and tap. Some get frustrated and leave. Others seem to find what they need. Patterns emerging.\"\n",
    "]\n",
    "\n",
    "batch_results = engine.predict_batch(texts, top_k=10, threshold=0.0001)\n",
    "\n",
    "for i, (text, preds) in enumerate(zip(texts, batch_results), 1):\n",
    "    print(f'\\n{i}. {text}')\n",
    "    for j, pred in enumerate(preds, 1):\n",
    "        print(f'   {j}. {pred.action_name:30s} {pred.confidence*100:.8f}% (L{pred.layer})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
